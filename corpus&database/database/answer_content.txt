1. massively parallel distributed process or made up of simple processing units 2. knowledge acquired from environment through a learning process 3. knowledge stored in the form of synaptic weights
1. biologically inspired 2. good learning properties 3. continuous, nonlinear 4. well adapted to certain tasks 5. fault tolerant 6. graceful degradation
1. 380BC Plato (Rationalism - innateness) 2. 330BC Aristotle (Empricism - experience) 3. 1641 Descartes (mind-body Dualism) 4. 1781 Kant (Critique of Pure Reason) 5. 1899 Sigmund Freud (Psychology) 6. 1953 B.F. Skinner (Behaviourism)
1. 1642 Blaise Pascal (mechanical adding machine) 2. 1694 Gottfried Leibniz (mechanical calculator) 3. 1769 Wolfgang von Kempelen (Mechanical Turk) 4. 1837 Charles Babbage & Ada Lovelace (Difference Engine) 5. 1848 George Boole (the Calculus of Logic) 6. 1879 Gottlob Frege (Predicate Logic) 7. 1950 Turing Test 8. 1956 Dartmouth conference
1. 1943 McCulloch & Pitts (neuron models) 2. 1948 Norbert Wiener (Cybernetics) 3. 1948 Alan Turing (B-Type Networks) 4. 1955 Oliver Selfridge (Pattern Recognition) 5. 1962 Hubel and Wiesel (visual cortex) 6. 1962 Frank Rosenblatt (Perceptron)
1. 1956 Newell & Simon (Logic Theorist) 2. 1959 John McCarthy (Lisp) 3. 1959 Arther Samuel (Checkers) 4. 1965 Joseph Weizenbaum (ELIZA) 5. 1967 Edward Feigenbaum (Dendral)
1. 1969 Minsky & Papert published Perceptrons, emphasizing the limitations of neural models, and lobbied agencies to cease funding neural network research. 2. from 1969 to 1985 there was very little work in neural networks or machine learning. 3. a few exceptions, e.g. Stephen Grossberg, Teuvo Kohonen (SOM), Paul Werbos.
1. 1970s and early 1980s, AI research focused on symbolic processing, Expert Systems 2. Some commercial success, but ran into difficulties:(1). combinatorial explosion in search spaces (2). difficulty of formalising everyday knowledge as well as expert knowledge
1. 1986 Rumelhart, Hinton & Williams (multi-layer, backprop) 2. 1989 Dean Pomerleau (ALVINN) 3. late 1980’s renewed enthusiasm, hype 4. 1990s more principled approaches 5. 2000’s SVM, Bayesian models became more popular 6. 2010’s deep learning networks, GPU’s 7. 2020’s spiking networks(?)
1. Image processing (1). classification (2). segmentation 2.Language processing (1). translation (2). semantic disambiguation (3). sentiment analysis 3. Combining images and tex (1). automatic captioning 4. Game playing (1). AlphaGo (2). Deep Q-Learning
Two perspectives on the history of Deep Learning Viewpoint 1: Focusing on recent work (after 2012) https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf Viewpoint 2: Focusing on earlier work (before 2012) http://people.idsia.ch/~juergen/deep-learning-overview.html
1. Central Nervous System (1). Brain (2).Spinal cord 2. Peripheral Nervous System (1). Somatic nervous system (2). Autonomic nervous system (3). Enteric nervous system
1. “cortex” from Latin word for “bark” (of tree) 2. cortex is a sheet of tissue making up outer layers of brain , 2-6cmthick 3. right and left sides connected by corpus callosum 4. functions: thought, voluntary movement, language, reasoning, perception
1. general term for area of brain between the thalamus and spinal cord 2. includes medulla, pons, tectum, reticular formation and tegmentum 3. functions: breathing, heart rate, blood pressure, and others
1. from Latin word for “little brain” 2. functions: movement, balance, posture
functions: vision, audition, eye movement, body movement
1. receives sensory information and relays it to the cerebral cortex 2. also relays information from the cerebral cortex to other areas of the brain, and the spinal cord 3. functions: sensory integration, motor integration
1. composed of several different areas at the base of the brain 2. the size of a pea (about 1/300 of the total brain weight) 3. functions: body temperature, emotions, hunger, thirst, circadian rhythms
1. group of structures including amygdala, hippocampus, mammillary bodies and cingulate gyrus 2. important for controlling the emotional response to a given situation 3. hippocampus also important for memory 4. functions: emotional behaviour
1. The body is made up of billions of cells. Cells of the nervous system, called neurons, are specialized to carry “messages” through an electrochemical process. 2. The human brain has about 100 billion neurons, and a similar number of support cells called “glia”. 3. Neurons are similar to other cells in the body in some ways, such as: (1). neurons are surrounded by a cell membrane (2). neurons have a nucleus that contains genes (DNA) (3). neurons carry out basic cellular processes like protein synthesis and energy production
1. Neurons have specialized extensions called dendrites and axons Dendrites bring information to the cell body, while axons take information away from the cell body. 2. The axon of one neuron can connect to the dendrite of another neuron through an electrochemical junction called a synapse. 3. Most neurons have only one axon, but the number of dendrites can vary widely: (1). Unipolar and Bipolar neurons have only one dendrite (2). Purkinje neurons can have up to 100,000 dendrites
1. Dendrites are typically less than a millimetre in length 2. Axons can vary in length from less than a millimetre to more than a metre (motor neurons) 3. Long axons are sometimes surrounded by a myelinated sheath, which prevents the electrical signal from dispersing, and allows it to travel faster (up to 100 m/s).
1. electrical pulse reaches the endbulb and causes the release of neurotransmitter molecules from little packets (vesicles) through the synaptic membrane 2. transmitter then diffuses through the synaptic cleft to the other side 3. when the neurotransmitter reaches the post-synaptic membrane, it causes a change in polarisation of the membrane 4. the change in potential can be excitatiory (moving the potential towards the threshold) or inhibitory (moving it away from the threshold)
1. human brain has 100 billion neurons with an average of 10,000 synapses each 2. latency is about 3-6 milliseconds 3. therefore, at most a few hundred “steps” in any mental computation, but massively parallel
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s
1. Suppose we want to classify an image as a bird, sunset, dog, cat, etc. 2. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. 2. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image.
1. can “unroll” a recurrent architecture into an equivalent feedforward architecture, with shared weights 2. useful for processing language or other temporal sequences
1. output is trained to reproduce the input as closely as possible 2. activations normally pass through a bottleneck, so the network is forced to compress the data in some way
1. biological neurons spike in different patterns (quiescent, persistent, sporadic) 2. spike timing might carry important information 3. most NN models ignore timing information, but some work has been done on spiking network models 4. in the future, special hardware might lead to a revolution for spiking networks, similar to what GPU’s provided for CNN’s
1. Neurons – Biological and Artificial 2. Perceptron Learning 3. Linear Separability 4. Multi-Layer Networks
1. The brain is made up of neurons (nerve cells) which have (1). a cell body (soma) (2). dendrites (inputs) (3). an axon (outputs) (4). synapses (connections between cells) 2. Synapses can be exitatory or inhibitory and may change over time. When the inputs reach some threshhold an action potential (electrical pulse) is sent along the axon to the outputs.
1.(Artificial) Neural Networks are made up of nodes which have (1). inputs edges, each with some weight (2). outputs edges (with weights) (3). an activation level (a function of the inputs) 2. Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: Some nodes are inputs (sensing), some are outputs (action)
w0=-th, s = w1x1 + w2x2−th = w1x1 + w2x2 + w0, x1, x2 are inputs, w1, w2 are synaptic weights, th is a threshold, w0 is a bias weight, g is transfer function
linearly separable functions
In 1969, Minsky and Papert published a book highlighting the limitations of Perceptrons, and lobbied various funding agencies to redirect funding away from neural network research, preferring instead logic-based methods such as expert systems. It was known as far back as the 1960’s that any given logical function could be implemented in a 2-layer neural network with step function activations. But, the the question of how to learn the weights of a multi-layer neural network based on training examples remained an open problem. The solution, which we describe in the next section, was found in 1976 by Paul Werbos, but did not become widely known until it was rediscovered in 1986 by Rumelhart, Hinton and Williams.
1. Supervised Learning 2. Ockham’s Razor (5.2) 3. Multi-Layer Networks 4.  Gradient Descent (4.3, 6.5.2)
1. Supervised Learning: agent is presented with examples of inputs and their target outputs 2. Reinforcement Learning: agent is not presented with target outputs, but is given a reward signal, which it aims to maximize 3. Unsupervised Learning: agent is only presented with the inputs themselves, and aims to find structure in these inputs
1. we have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specified. 2. the aim is to predict the target value, based on the input attributes. 3. agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set 4. various learning paradigms are available: (1). Neural Network (2). Decision Tree (3). Support Vector Machine, etc.
1. framework (decision tree, neural network, SVM, etc.) 2.  representation (of inputs and outputs) 3. pre-processing / post-processing 4. training method (perceptron learning, backpropagation, etc.) 5. generalization (avoid over-fitting) 6. evaluation (separate training and testing sets)
“The most likely hypothesis is the simplest one consistent with the data.” Since there can be noise in the measurements, in practice need to make a tradeoff between simplicity of the hypothesis and how well it fits the data.
x1 XOR x2 can be written as: (x1 AND x2) NOR (x1 NOR x2) Recall that AND, OR and NOR can be implemented by perceptrons.
Normally, the numbers of input and output units are fixed, but we can choose the number of hidden units.
for this toy problem, there is only a training set; there is no validation or test set, so we don’t worry about overfitting the XOR data cannot be learned with a perceptron, but can be achieved using a 2-layer network with two hidden units
We define an error function E to be (half) the sum over all input patterns of the square of the difference between actual output and desired output If we think of E as height, it defines an error landscape on the weight space. The aim is to find a set of weights for which E is very low.
because of the step function, the landscape will not be smooth but will instead consist almost entirely of flat local regions and “shoulders”, with occasional discontinuous jumps.
Recall that the error function E is (half) the sum over all input patterns of the square of the difference between actual output and desired output The aim is to find a set of weights for which E is very low. If the functions involved are smooth, we can use multi-variable calculus to adjust the weights in such a way as to take us in the steepest downhill direction. Parameter η is called the learning rate. 
This principle can be used to compute the partial derivatives in an efficient and localized manner. Note that the transfer function must be differentiable (usually sigmoid, or tanh).
1. Medical Dignosis 2. Autonomous Driving 3. Game Playing 4. Credit Card Fraud Detection 5. Handwriting Recognition 6. Financial Prediction
1. re-scale inputs and outputs to be in the range 0 to 1 or −1 to 1 2. replace missing values with mean value for that attribute 3. initialize weights to very small random values 4. on-line or batch learning 5. three different ways to prevent overfitting: (1). limit the number of hidden nodes or connections (2). limit the training time, using a validation set (3). weight decay 6. adjust learning rate (and momentum) to suit the particular task
1. Autonomous Land Vehicle In a Neural Network 2.  later version included a sonar range finder (1). 8×32 range finder input retina (2). 29 hidden units (3). 45 output units 3. Supervised Learning, from human actions (Behavioral Cloning) additional “transformed” training items to cover emergency situations 4. drove autonomously from coast to coast
1. Neural networks are biologically inspired 2. Multi-layer neural networks can learn nonlinearly separable functions 3. Backpropagation is effective and widely used
1. Probability (3.1-3.6, 3.9.3, 3.10) 2. Cross Entropy (5.5) 3. Bayes’ Rule (3.11) 4. Weight Decay (5.2.2) 5. Momentum (8.3)
Begin with a set Ω – the sample space (e.g. 6 possible rolls of a die) ω ∈ Ω is a sample point/possible world/atomic event A probability space or probability model is a sample space with an assignment P(ω) for every ω ∈ Ω s.t.
A random variable (r.v.) is a function from sample points to some range (e.g. the Reals or Booleans) For example, Odd(3) = true.
1. cross entropy (1). problem: least squares error function unsuitable for classification, where target = 0 or 1 (2). mathematical theory: maximum likelihood (3). solution: replace with cross entropy error function 2. Weight Decay (1). problem: weights “blow up”, and inhibit further learning (2). mathematical theory: Bayes’ rule (3). solution: add weight decay term to error function 3. Momentum (1). problem: weights oscillate in a “rain gutter” (2). solution: weighted average of gradient over time
For classification tasks, target t is either 0 or 1, so better to use E = −t log(z −(1−t)log(1−z) This can be justified mathematically, and works well in practice – especially when negative examples vastly outweigh positive ones. It also makes the backprop computations simpler.
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. logP(D|h) is called the likelihood. ML Principle: Choose h ∈ H which maximizes the likelihood, i.e. maximizes P(D|h) [or, maximizes logP(D|h)]
The formula for conditional probability can be manipulated to find a relationship when the two variables are swapped: P(a∧b) = P(a|b)P(b) = P(b a)P(a) This is often useful for assessing the probability of an underlying cause after an effect has been observed:
Question: Suppose we have a test for a type of cancer which occurs in 1% of patients. The test has a sensitivity of 98% and a specificity of 97%. If a patient tests positive, what is the probability that they have the cancer Answer: There are two random variables: Cancer (true or false) and Test (positive or negative). The probability is called a prior, because it represents our estimate of the probability before we have done the test (or made some other observation). The sensitivity and specificity are interpreted as follows: P(positive|cancer) = 0.98, and P(negative|¬cancer) = 0.97
Question: You work for a lighting company which manufactures 60% of its light bulbs in Factory A and 40% in Factory B. One percent of the light bulbs from Factory A are defective, while two percent of those from Factory B are defective. If a random light bulb turns out to be defective, what is the probability that it was manufactured in Factory A? Answer: There are two random variables: Factory (A or B) and Defect (Yes or No). In this case, the prior is: P(A) = 0.6, P(B) = 0.4 The conditional probabilities are: P(defect|A) = 0.01, and P(defect|B) = 0.02
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. P(h|D) = probability that h is correct, given that data D were observed. Bayes’ Theorem: P(h) is called the prior.
Assume that small weights are more likely to occur than large weights, i.e. where Z is a normalizing constant. Then the cost function becomes: This can prevent the weights from “saturating” to very high values. Problem: need to determine λ from experience, or empirically.
If landscape is shaped like a “rain gutter”, weights will tend to oscillate without much improvement. Solution: add a momentum factor
Compute matrix of second derivatives (called the Hessian). Approximate the landscape with a quadratic function (paraboloid). Jump to the minimum of this quadratic function.
Use methods from information geometry to find a “natural” re-scaling of the partial derivatives.
1. Geometry of Hidden Unit Activations 2. Limitations of 2-layer networks 3. Alternative transfer functions (6.3) 4. Dropout
1. swap any pair of hidden nodes, overall function will be the same 2. on any hidden node, reverse the sign of all incoming and outgoing weights (assuming symmetric transfer function) 3. hidden nodes with identical input-to hidden weights in theory would never separate; so, they all have to begin with different (small) random weights 4. in practice, all hidden nodes try to do similar job at first, then gradually specialize.
1. for small weights, each layer implements an approximately linear function, so multiple layers also implement an approximately linear function. 2. for large weights, transfer function approximates a step function, so computation becomes digital and learning becomes very slow. 3. with typical weight values, two-layer neural network implements a function which is close to linear, but takes advantage of a limited degree of nonlinearity.
Some functions cannot be learned with a 2-layer sigmoidal network. For example, this Twin Spirals problem cannot be learned with a 2-layer network, but it can be learned using a 3-layer network if we include shortcut connections between non-consecutive layers.
1. Twin Spirals can be learned by 3-layer network with shortcut connections 2. first hidden layer learns linearly separable features 3. second hidden layer learns “convex” features 4. output layer combines these to produce “concave” features 5. training the 3-layer network is delicate 6. learning rate and initial weight values must be very small 7. otherwise, the network will converge to a local optimum 
Training by backpropagation in networks with many layers is difficult. When the weights are small, the differentials become smaller and smaller as we backpropagate through the layers, and end up having no effect. When the weights are large, the activations in the higher layers will saturate to extreme values. As a result, the gradients at those layers will become very small, and will not be propagated to the earlier layers. When the weights have intermediate values, the differentials will sometimes get multiplied many times is places where the transfer function is steep, causing them to blow up to large values.
1. layerwise unsupervised pre-training 2. long short term memory (LSTM) 3. new activations functions LSTM is specifically for recurrent neural networks. We will discuss unsupervised pre-training and LSTM later in the course.
1. Sigmoid and hyperbolic tangent traditionally used for 2-layer networks, but suffer from vanishing gradient problem in deeper networks. 2. Rectified Linear Units (ReLUs) are popular for deep networks, including convolutional networks. Gradients don’t vanish. But, their highly linear nature may cause other problems. 3. Scaled Exponential Linear Units (SELUs) are a recent innovation which seems to work well for very deep networks.
Nodes are randomly chosen to not be used, with some fixed probability (usually, one half). When training is finished and the network is deployed, all nodes are used, but their activations are multiplied by the same probability that was used in the dropout. Thus, the activation received by each unit is the average value of what it would have received during training. Dropout forces the network to achieve redundancy because it must deal with situations where some features are missing. Another way to view dropout is that it implicitly (and efficiently) simulates an ensemble of different architectures.
Ensembling is a method where a number of different classifiers are trained on the same task, and the final class is decided by “voting” among them. In order to benefit from ensembling, we need to have diversity in the different classifiers. For example, we could train three neural networks with different architectures, three Support Vector Machines with different dimensions and kernels, as well as two other classifiers, and ensemble all of them to produce a final result. (Kaggle Competition entries are often done in this way)
Diversity can also be achieved by training on different subsets of data. Suppose we are given N training items. Each time we train a new classifier, we choose N items from the training set with replacement. This means that some items will not be chosen, while others are chosen two or three times. There will be diversity among the resulting classifiers because they have each been trained on a different subset of data. They can be ensembled to produce a more accurate result than a single classifier.
In the case of dropout, the same data are used each time but a different architecture is created by removing the nodes that are dropped. The trick of multiplying the output of each node by the probability of dropout implicitly averages the output over all of these different models.
Suppose we want to classify an image as a bird, sunset, dog, cat, etc. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image.
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s
1. convolution layers: extract shift-invariant features from the previous layer 2. subsampling or pooling layers: combine the activations of multiple units from the previous layer into one unit 3. fully connected layers: collect spatially diffuse information 4. output layer: choose between classes
There can be multiple steps of convolution followed by pooling, before reaching the fully connected layers. Note how pooling reduces the size of the feature map (usually, by half in each direction).
Consider a classification task with N classes, and assume zj is the output of the unit corresponding to class j. We assume the network’s estimate of the probability of each class j is proportional to exp(zj). Because the probabilites must add up to 1, we need to normalize by dividing by their sum: If the correct class is i, we can treat −logProb(i) as our cost function. The first term pushes up the correct class i, while the second term mainly pushes down the incorrect class j with the highest activation (if j ̸= i).
For example: Assume the original image is J × K , with L channels. We apply an M × N “filter” to these inputs to compute one hidden unit in the convolution layer. In this example J = 6,K = 7,L = 3,M = 3,N = 3. The same weights are applied to the next M × N block of inputs, to compute the next hidden unit in the convolution layer (“weight sharing”). If the original image size is J×K and the filter is size M×N, the convolution layer will be (J+1−M)×(K+1−N)
For example, in the first convolutional layer of LeNet, J = K = 32, M = N = 5. The width of the next layer is J + 1 − M = 32 + 1 − 5 = 28 Question: If there are 6 filters in this layer, compute the number of: weights per neuron? neurons? connections? independent parameters? 
The 5 × 5 window of the first convolution layer extracts from the original 32 × 32 image a 28 × 28 array of features. Subsampling then halves this size to 14 × 14. The second Convolution layer uses another 5 × 5 window to extract a 10 × 10 array of features, which the second subsampling layer reduces to 5 × 5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits ’0’ to ’9’.
Sometimes, we treat the off-edge inputs as zero (or some other value). This is known as “Zero-Padding”. With Zero Padding, the convolution layer is the same size as the original image (or the previous layer).
1. 5 convolutional layers + 3 fully connected layers 2. max pooling with overlapping stride 3. softmax with 1000 classes 4. 2 parallel GPUs which interact only at certain layers
Assume the original image is J × K , with L channels. We again apply an M × N filter, but this time with a “stride” of s > 1. In this example J = 7, K = 9, L = 3, M = 3, N = 3, s = 2. The same formula is used, but j and k are now incremented by s each time. The number of free parameters is 1+L×M×N
j takes on the values 0,s,2s,...,(J−M) k takes on the values 0,s,2s,…,(K−N) The next layer is (1+(J−M)/s) by (1+(K−N)/s)
When combined with zero padding of width P, j takes on the values 0,s,2s,…,(J+2P−M) k takes on the values 0,s,2s,…,(K+2P−N) The next layer is (1+(J+2P−M)/s) by (1+(K+2P−N)/s)
For example, in the first convolutional layer of AlexNet, J = K = 224, P = 2, M = N = 11, s = 4. The width of the next layer is 1+(J+2P−M)/s=1+(224+2×2−11)/4=55
If the previous layer is J × K, and max pooling is applied with width F and stride s, the size of the next layer will be (1+(J−F)/s)×(1+(K−F)/s)
1. Image Datasets and Tasks 2. Convolution in Detail 3. AlexNet 4. Weight Initialization 5. Batch Normalization 6. Residual Networks 7. Dense Networks 8. Style Transfer
1. black and white, resolution 28×28 2. 60,000 images 3. 10 classes (0,1,2,3,4,5,6,7,8,9)
1. color, resolution 32×32 2. 50,000 images 3. 10 classes
1. color, resolution 227×227 2. 1.2 million images 3. 1000 classes
1. image classification 2. object detection 3. object segmentation 4. style transfer 5. generating images 6. generating art 7. image captioning
The 5 × 5 window of the first convolution layer extracts from the original 32 × 32 image a 28 × 28 array of features. Subsampling then halves this size to 14 × 14. The second Convolution layer uses another 5 × 5 window to extract a 10 × 10 array of features, which the second subsampling layer reduces to 5 × 5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits ’0’ to ’9’.
1. AlexNet, 8 layers (2012) 2. VGG, 19 layers (2014) 3. GoogleNet, 22 layers (2014) 4. ResNets, 152 layers (2015)
1. 5 convolutional layers + 3 fully connected layers 2. max pooling with overlapping stride 3. softmax with 1000 classes 4. 2 parallel GPUs which interact only at certain layers
1. 650K neurons 2. 630M connections 3. 60M parameters 4. more parameters that images → danger of overfitting
1. Rectified Linear Units (ReLUs) 2. overlappingpooling(width=3,stride=2) 3. stochastic gradient descent with momentum and weight decay 4. data augmentation to reduce overfitting 5. 50% dropout in the fully connected layers
1. ten patches of size 224 × 224 are cropped from each of the original 227 × 277 images (using zero padding) 2. the horizontal reflection of each patch is also included. 3. at test time, average the predictions on the 10 patches. 4. also include changes in intensity to RGB channels
1. filters on GPU-1 (upper) are color agnostic 2. filters on GPU-2 (lower) are color specific 3. these resemble Gabor filters
1. > 10 layers: weight initialization and batch nomalization 2. > 30 layers: skip connections 3.  > 100 layers: identity skip connections
1. Example: Toss a coin once, and count the number of Heads Mean μ Variance Standard Deviation σ = 1 (0+1) = 0.5 2 Mean μ Variance Standard Deviation σ =100∗0.5 =50 = 100 ∗ 0.25 = 25 = √Variance = 5 = 1  (0 − 0.5)2 + (1 − 0.5)2)  = 0.25 2 = √Variance = 0.5 2. Example: Toss a coin 100 times, and count the number of Heads 3. Example: Toss a coin 10000 times, and count the number of Heads μ=5000, σ =√2500 =50
Idea: Take any two consecutive stacked layers in a deep network and add a “skip” connection which bipasses these layers and is added to their output. 1. the preceding layers attempt to do the “whole” job, making x as close as possible to the target output of the entire network 2. F(x) is a residual component which corrects the errors from previous layers, or provides additional details which the previous layers were not powerful enough to compute 3. with skip connections, both training and test error drop as you add more layers 4. with more than 100 layers, need to apply ReLU before adding the residual instead of afterwards. This is called an identity skip connection.
Recently, good results have been achieved using networks with densely connected blocks, within which each layer is connected by shortcut connections to all the preceding layers.
1. pretrain CNN on ImageNet (VGG-19) 2. pass input texture through CNN; compute feature map F l for ith filter at spatial location k in layer (depth) l 3. compute the Gram matrix for each pair of features 4. feed (initially random) image into CNN 5. compute L2 distance between Gram matrices of original and new image 6. backprop to get gradient on image pixels 7. update image and go to step 5.
1. “ImageNet Classification with Deep Convolutional Neural Networks”, Krizhevsky et al., 2015. 2. “Understanding the difficulty of training deep feedforward neural networks”, Glorot & Bengio, 2010. 3. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, Ioffe & Szegedy, ICML 2015. 4. “Deep Residual Learning for Image Recognition”, He et al., 2016. 5. “Densely Connected Convolutional Networks”, Huang et al., 2016. 6. “A Neural Algorithm of Artistic Style”, Gatys et al., 2015.
1. Processing Temporal Sequences 2. Sliding Window 3. Recurrent Network Architectures 4. Hidden Unit Dynamics 5. Long Short Term Memory
There are many tasks which require a sequence of inputs to be processed rather than a single input. 1. speech recognition 2. time series prediction 3. machine translation 4. handwriting recognition
The simplest way to feed temporal input to a neural network is the “sliding window” approach, first used in the NetTalk system (Sejnowski & Rosenberg, 1987).
Given a sequence of 7 characters, predict the phonetic pronunciation of the middle character. For this task, we need to know the characters on both sides. For example, how are the vowels in these words pronounced? pa pat pate paternal mo mod mode modern
1. NETtalk gained a lot of media attention at the time. 2. Hooking it up to a speech synthesizer was very cute. In the early stages of training, it sounded like a babbling baby. When fully trained, it pronounced the words mostly correctly (but sounded somewhat robotic). 3. Later studies on similar tasks have often found that a decision tree could produce equally good or better accuracy. 4. This kind of approach can only learn short term dependencies, not the medium or long term dependencies that are required for some tasks.
1. at each time step, hidden layer activations are copied to “context” layer 2. hidden layer receives connections from input and context layers 3. the inputs are fed one at a time to the network, it uses the context layer to “remember” whatever information is required for it to produce the correct output
1. we can “unroll” are current architecture into an equivalent feedforward architecture, with shared weights 2. applying backpropagation to the unrolled architecture is reffered to as “backpropagation through time” 3. we can backpropagate just one timestep, or a fixed number of timesteps, or all the way back to beginning of the sequence
1. it is sometimes beneficial to add “shortcut” connections directly from input to output 2. connections from output back to hidden have also been explored (sometimes called “Jordan Networks”)
Scan a sequence of characters one at a time, then classify the sequence as Accept or Reject.
1. gated network trained by BPTT 2. emulates exactly the behaviour of Finite State Automaton 3. trained network emulates the behaviour of Finite State Automaton 4. training set must include short, medium and long examples
Scan a sequence of characters one at a time, and try at each step to predict the next character in the sequence. In some cases, the prediction is probabilistic. For the anbn task, the first b is not predictable, but subsequent b’s and the initial a in the next subsequence are predictable.
1. for this task, sequence is accepted if the number of a’s and b’s are equal 2. network counts up by spiralling inwards, down by spiralling outwards
1. Simple Recurrent Networks (SRNs) can learn medium-range dependencies but have difficulty learning long range dependencies 2. Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) can learn long range dependencies better than SRN
Two excellent Web resources for LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php LSTM – context layer is modulated by three gating mechanisms: forget gate, input gate and output gate. http://colah.github.io/posts/2015-08-Understanding-LSTMs/
SRN – context layer is combined directly with the input to produce the next hidden layer. SRN can learn Reber Grammar, but not Embedded Reber Grammar.
1. statistical language processing 2. n-gram models 3. co-occurence matrix 4. word representations 5. word2vec 6. word relationships 7. neural machine translation 8. combining images and language
What is the meaning of meaning? 1. dictionary definitions 2. synonyms and antonyms 3. taxonomy (1). penguin is-a bird is-a mammal is-a vertebrate
Synonyms for “elegant” stylish, graceful, tasteful, discerning, refined, sophisticated, dignified, cultivated, distinguished, classic, smart, fashionable, modish, decorous, beautiful, artistic, aesthetic, lovely; charming, polished, suave, urbane, cultured, dashing, debonair; luxurious, sumptuous, opulent, grand, plush, high-class, exquisite Synonyms, antonyms and taxonomy require human effort, may be incomplete and require discrete choices. Nuances are lost. Words like “king”, “queen” can be similar in some attributes but opposite in others. Could we instead extract some statistical properties automatically, without human involvement?
1. some words occur frequently in all (or most) documents 2. some words occur frequently in a particular document, but not generally 3. this information can be useful for document classification
1. each column of the matrix becomes a vector representing the corresponding document 2. words like “cat”, “mouse”, “house” tend to occur in children’s books or rhymes 3. other groups of words may be characteristic of legal documents, political news, sporting results, etc. 4. words occurring many times in one document may skew the vector – might be better to just have a “1” or “0” indicating whether the word occurs at all
1. by normalizing each row (to sum to1) we can estimate the probability prob(wj|wi) of word wj occurring after wi 2. need to aggregrate over a large corpus, so that unusual words like “crooked” will not dominate 3. the model captures some common combinations like “there was”, “man who”, “and found”, “he bought”, “who caught”, “and they”, “they all”, “lived together”, etc. 4. this unigram model can be generalized to a bi-gram, tri-gram, ...,n-gram model by considering the n preceding words 5. if the vocabulary is large, we need some tricks to avoid exponential use of memory
“Rashly – Good night is very liberal – it is easily said there is – gyved to a sore distraction in wrath and with my king may choose but none of shapes and editing by this , and shows a sea And what this is miching malhecho ; And gins to me a pass , Transports his wit , Hamlet , my arms against the mind impatient , by the conditions that would fain know ; which , the wicked deed to get from a deed to your tutor .”
1. sometimes, we don’t necessarily predict the next word, but simply a “nearby word” (e.g. a word occurring within an n-word window centered on that word) 2. we can build a matrix in which each row represents a word, and each column a nearby word 3. each row of this matrix could be considered as a vector representation for the corresponding word, but the number of dimensions is equal to the size of the vocabulary, which could be very large (∼ 105) is there a way to reduce the dimensionality while still preserving the relationships between words? 4. by aggregating over many documents, pairs (or groups) of words emerge which tend to occur near each other (but not necessarily consecutively) 5. common words tend to dominate the matrix could we sample common words less often, in order to reveal the relationships of less common words?
“Words that are used and occur in the same contexts tend to purport similar meanings.” Z. Harris (1954) “You shall know a word by the company it keeps.” Aim of Word Embeddings: Find a vector representation of each word, such that words with nearby representations are likely to occur in similar contexts.
1. Structuralist Linguistics (Firth, 1957) 2. Recurrent Networks (Rumelhart, Hinton & Williams, 1986) 3. Latent Semantic Analysis (Deerwester et al., 1990) 4. Hyperspace Analogue to Language (Lund, Burgess & Atchley, 1995) 5. Neural Probabilistic Language Models (Bengio, 2000) 6. NLP (almost) from Scratch (Collobert et al., 2008) 7. word2vec (Mikolov et al., 2013) 8. GloVe (Pennington, Socher & Manning, 2014)
Typically, L is the number of words in the vocabulary (about 60,000) and M is either equal to L or, in the case of document classification, the number of documents in the collection. SVD is computationally expensive, proportional to L × M2 if L ≥ M. Can we generate word vectors in a similar way but with less computation, and incrementally? 1. word2vec: predictive model and maximize the probability of a word based on surrounding words 2. GloVe: count-based model and reconstruct a close approximation to the co-occurrence matrix X
1. if X is symmetric and positive semi-definite, eigenvalue and singular value decompositions are the same. 2. in general, eigenvalues can be negative or even complex, but singular values are always real and non-negative. 3. even if X is a square matrix, singular value decompositon treats the source and target as two entirely different spaces. 4. the word co-occurrence matrix is symmetric but not positive semi- definite; for example, if the text consisted entirely of two alternating letters ..ABABABABABABAB.. then A would be the context for B, and vice-versa.
1. word2vec is a linear model in the sense that there is no activation function at the hidden nodes 2. this 1-word prediction model can be extended to multi-word prediction in two different ways: Continuous Bag of Words and Skip-Gram 3. need a computationally efficient alternative to Softmax (Why?): Hierarchical Softmax and Negative Sampling 4. need to sample frequent words less often
1. If several context words are each used independently to predict the center word, the hidden activation becomes a sum (or average) over all the context words 2. Note the difference between this and NetTalk – in word2vec (CBOW) all context words share the same input-to-hidden weights
1. try to predict the context words, given the center word 2. this skip-gram model is similar to CBOW, except that in this case a single input word is used to predict multiple context words 3. all context words share the same hidden-to-output weights
1. target words are organized in a Huffman-coded Binary Tree 2. each output of the network corresponds to one branch point in the tree 3. only those nodes that are visited along the path to the target word are evaluated (which is log2(V) nodes on average)
The idea of negative sampling is that we train the network to increase its estimation of the target word j∗ and reduce its estimate not of all the words in the vocabulary but just a subset of them Wneg, drawn from an appropriate distribution. This is a simplified version of Noise Constrastive Estimation (NCE). It is not guaranteed to produce a well-defined probability distribution, but in practice it does produce high-quality word embeddings. The number of samples is 5-20 for small datasets, 2-5 for large datasets. Empirically, a good choice of the distribution from which to draw the negative samples is P(w) = U(w)3/4/Z where U(w) is the unigram distribution determined by the previous word, and Z is a normalizing constant.
The skip-gram model can be augmented using visual features from images labeled with words from the corpus. We first extract mean activations u j for each word from the highest (fully connected) layers of a CNN model like AlexNet. The objective function then becomes the formula. This encourages things that look similar to have closer representations.
T. Mikolov, K. Chen, G. Corrado & J. Dean, 2013. “Efficient estimation of word representations in vector space”, arXiv preprint arXiv:1301.3781. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado & J. Dean, 2013. “Dis- tributed representations of words and phrases and their compositionality”, NIPS 2013, 3111-19. Xin Rong, 2014. “word2vec parameter learning explained.”, arXiv:1411.2738. https://nlp.stanford.edu/projects/glove/ https://devblogs.nvidia.com/parallelforall/ introduction-neural-machine-translation-gpus-part-3/
1. Reinforcement Learning vs. Supervised Learning 2. Models of Optimality 3. Exploration vs. Exploitation 4. Value Function Learning (1). Q-Learning (2). TD-Learning 5. Policy Learning (1). Evolution Strategies (2). Policy Gradients 6. Actor-Critic
1. We have a training set and a test set, each consisting of a set of examples. For each example, a number of input attributes and a target attribute are specified. 2. The aim is to predict the target attribute, based on the input attributes. 3. Various learning paradigms are available: (1). Decision Trees (2). Neural Networks (3). SVM (4). .. others ..
Supervised Learning can also be used to learn Actions, if we construct a training set of situation-action pairs (called Behavioral Cloning). However, there are many applications for which it is difficult, inappropri- ate, or even impossible to provide a “training set” 1. optimal control: mobile robots, pole balancing, flying a helicopter 2. resource allocation: job shop scheduling, mobile phone channel allocation 3. mix of allocation and control: elevator control, backgammon
1. An agent interacts with its environment. 2. There is a set S of states and a set A of actions. 3. At each time step t, the agent is in some states t. It must choose an action at , whereupon it goes into state st+1 =δ(st,at) and receives reward rt =R(st,at) 4. Agent has a policyπ:S→A. We aim to find an optimal policy π∗ which maximizes the cumulative reward. 5. In general, δ, R and π can be multi-valued, with a random element, in which case we write them as probability distributions δ(st+1 = s|st,at) R (rt = r|st,at) π(at = a|st)
There are some environments in which any deterministic agent will perform very poorly, and the optimal (reactive) policy must be stochastic (i.e. randomized). In 2-player games like Rock-Paper-Scissors, a random strategy is also required in order to make agent choices unpredictable to the opponent.
1. Value Function Learning (1). TD-Learning (2). Q-Learning 2. Policy Learning (1). Hill Climbing (2). Policy Gradients (3). Evolutionary Strategy 3. Actor-Critic: combination of Value and Policy learning
The special case of an active, stochastic environment with only one state is called the K-armed Bandit Problem, because it is like being in a room with several (friendly) slot machines, for a limited time, and trying to collect as much money as possible. Each action (slot machine) provides a different average reward.
Most of the time we should choose what we think is the best action. However, in order to ensure convergence to the optimal strategy, we must occasionally choose something different from our preferred action, e.g. 1. choose a random action 5% of the time, or 2. use Softmax (Boltzmann distribution) to choose the next action
Theorem: Q-learning will eventually converge to the optimal policy, for any deterministic Markov decision process, assuming an appropriately randomized strategy. (Watkins & Dayan 1992) Theorem: TD-learning will also converge, with probability 1. (Sutton 1988, Dayan 1992, Dayan & Sejnowski 1994)
1. Delayed reinforcement: reward resulting from an action may not be received until several time steps later, which also slows down the learning 2. Search space must be finite: convergence is slow if the search space is large and relies on visiting every state infinitely often 3. For “real world” problems, we can’t rely on a lookup table: need to have some kind of generalisation (e.g. TD-Gammon)
Suppose we want a write a computer program to play a game like Backgammon, Chess, Checkers or Go. This can be done using a tree search algorithm (expectimax, MCTS, or minimax with alpha-beta pruning). But we need: (a) an appropriate way of encoding any board position as a set of numbers, and (b) a way to train a neural network or other learning system to compute a board evaluation, based on those numbers
The input s is the encoded board position (state), the output V(s) is the value of this position (probability of winning). At each move, roll the dice, find all possible “next board positions”, convert them to the appropriate input format, feed them to the network, and choose the one which produces the largest output.
In other words, how do we know what the value of the current position “should have been”? or, how do we find a better estimate for the value of the current position?
1. Behavioral Cloning (Supervised Learning): learn moves from human games (Expert Preferences) 2. Temporal Difference Learning: use subsequent positions to refine evaluation of current position and general method, does not rely on knowing the “world model” (rules of the game) 3. methods which combine learning with tree search (must know the “world model”): TD-Root, TD-Leaf, MCTS, TreeStrap
Backgammon is an example of an episodic task, in the sense that the agent receives just a single reward at the end of the game, which we can consider as the final value Vm+1 (typically, +1 for a win or −1 for a loss). We then have a sequence of game positions, each with its own (estimated) value: (current estimate) Vt→Vt+1→...→Vm→Vm+1 (final result) In this context, TD-Learning simplifies and becomes equivalent to using the value of the next state (Vt+1) as the training value for the current state (Vk) A fancier version, called TD(λ), uses Tk as the training value for Vk , where m Tt =(1−λ) ∑ λk−1−tVk+λm−tVm+1 k=t+1 Tt is a weighted average of future estimates, λ = discount factor (0 ≤ λ < 1)
1. Tesauro trained two networks: EP-network was trained on Expert Preferences (Supervised) and TD-network was trained by self play (TD-Learning) 2. TD-network outperformed the EP-network. 3. With modifications such as 3-step lookahead (expectimax) and additional hand-crafted input features, TD-Gammon became the best Backgammon player in the world (Tesauro, 1995).
There is another class of Reinforcement Learning algorithms which do not optimize a Value function but instead try to optimize the Policy itself, directly. Normally, we consider a family of policies πθ : S → A determined by parameters θ (for example, the weights of a neural network). For episodic domains like Backgammon, we do not need a discount factor, and the “fitness” of policy πθ can be taken as the Value function of the initial state s0 under this policy, which is the expected (or average) total reward received in each game by an agent using policy πθ
1. Initialize “champ” policy θchamp = 0 2. for each trial, generate “mutant” policy θmutant = θchamp + Gaussian noise (fixed σ) 3. champ and mutant are evaluated on the same task(s) 4. if mutant does “better” than champ, θchamp ← (1 − α)θchamp + α θmutant 5. in some cases, the size of the update is scaled according to the difference in fitness (and may be negative)
1. rectangular rink with rounded corners 2. near-frictionless playing surface 3. “spring” method of collision handling 4. frictionless puck (never acquires any spin)
a skate at each end of the vehicle with which it can push on the rink in two independent directions
1. 6 Braitenberg-style sensors equally spaced around the vehicle 2. each sensor has an angular range of 90◦ with an overlap of 30◦between neighbouring sensors
1. each of the 6 sensors responds to three different stimuli: ball / puck own goal and opponent goal 2. 3 additional inputs specify the current velocity of the vehicle 3. total of 3×6+3 = 21 inputs
1. single layer network with 21 inputs and 4 outputs 2. total of 4×(21+1) = 88 weights 3. our “genome” (for Evolutionary Computation) consists of a vector of these 88 parameters 4. mutation = add Gaussian random noise to each parameter, with standard deviation 0.05
1. each game begins with a random “game initial condition”: random position for puck and random position and orientation for player 2. each game ends with: +1 if puck → enemy goal -1 if puck → own goal 0 if time limit expires
1. mutant ← champ + Gaussian noise 2. champ and mutant play up to n games, with same game initial conditions 3. if mutant does “better” than champ, champ ← (1−α)∗champ+α∗mutant 3. “better” means the mutant must score higher than the champ in the first game, and at least as high as the champ in each subsequent game
1. HC-Gammon was trained to play Backgammon using this Evolution Strategy 2. same “game initial conditions” = same seed for generating dice rolls 3. weights were used to determine value function, but the learning optimizes performance of policy directly rather than aiming to make value function more accurate 4. performance was almost as good as TD-Gammon, but not quite: gradient information provides more precise updates, particularly for rarely used weights in the network
Policy Gradients are an alternative to Evolution Strategy, which use gradient ascent rather than random updates. Let’s first consider episodic games. The agent takes a sequence of actions a1 a2 ... at ... am At the end it receives a reward rtotal. We don’t know which actions contributed the most, so we just reward all of them equally. If rtotal is high (low), we change the parameters to make the agent more (less) likely to take the same actions in the same situations. In other words, we want to increase (decrease) If rtotal = +1 for a win and −1 for a loss, we can simply multiply the log probability by rtotal. Differentials can be calculated using the gradient The gradient of the log probability can be calculated nicely using Softmax. If rtotal takes some other range of values, we can replace it with (rtotal − b) where b is a fixed value, called the baseline.
We then get the following REINFORCE algorithm: for each trial run trial and collect states st , actions at , and reward rtotal for t = 1 to length(trial) end θ ← θ+η(rtotal −b)∇θ logπθ(at|st) end This algorithm has successfully been applied, for example, to learn to play the game of Pong from raw image pixels.
1. History of Reinforcement Learning 2. Deep Q-Learning for Atari Games 3. Actor-Critic 4. Asynchronous Advantage Actor Critic (A3C) 5. Evolutionary/Variational methods
1. model-free methods: 1961 MENACE tic-tac-toe (Donald Michie) 1986 TD(λ) (Rich Sutton) 1989 TD-Gammon (Gerald Tesauro) 2015 Deep Q Learning for Atari Games 2016 A3C (Mnih et al.) 2017 OpenAI Evolution Strategies (Salimans et al.) 2. methods relying on a world model: 1959 Checkers (Arthur Samuel) 1997 TD-leaf (Baxter et al.) 2009 TreeStrap (Veness et al.) 2016 Alpha Go (Silver et al.)
1. this BOXES algorithm was later adapted to learn more general tasks such as Pole Balancing, and helped lay the foundation for the modern field of Reinforcement Learning 2. for various reasons, interest in Reinforcement Learning faded in the late 70’s and early 80’s, but was revived in the late 1980’s, largely through the work of Richard Sutton 3. Gerald Tesauro applied Sutton’s TD-Learning algorithm to the game of Backgammon in 1989
1. end-to-end learning of values Q(s, a) from pixels s 2. input state s is stack of raw pixels from last 4 frames: 8-bit RGB images, 210×160pixels 3. output is Q(s,a) for 18 joystick /button positions 4. reward is change in score for that timestep
1. Prioritised Replay: weight experience according to surprise 2. Double Q-Learning: current Q-network w is used to select actions and older Q-network w is used to evaluate actions 3. Advantage Function: action-independent value function Vu(s) and action-dependent advantage function Aw(s,a) Q(s,a)=Vu(s)+Aw(s,a)
1. if the same weights w are used to select actions and evaluate actions, this can lead to a kind of confirmation bias 2. could maintain two sets of weights w and w, with one used for selection and the other for evaluation (then swap their roles) 3. in the context of Deep Q-Learning, a simpler approach is to use the current “online” version of w for selection, and an older “target” version w for evaluation; we therefore minimize 4. a new version of w is periodically calculated from the distributed values of w, and this w is broadcast to all processors.
Initialize “champ” policy θchamp = 0 for each trial, generate “mutant” policy θmutant = θchamp + Gaussian noise (fixed σ) champ and mutant are evaluated on the same task(s) if mutant does “better” than champ, θchamp ← (1 − α)θchamp + α θmutant in some cases, the size of the update is scaled according to the difference in fitness (and may be negative)
1. initialize mean μ = {μi}1≤i≤m and standard deviation σ = {σi}1≤i≤m 2. for each trial, collect k samples from a Gaussian distribution θi =μi+ηiσi where ηi ∼N(0,1) 3. sometimes include “mirrored” samples θi = μi − ηi σi 4. evaluate each sample θ to compute score or “fitness” F(θ) 5. update mean μ by μ ← μ+α(F(θ)−F)(θ−μ) α = learning rate, F = baseline α = learning rate, F = baseline 6. sometimes, σ is updated as well
1. Evolutionary Strategy with fixed σ 2. since only μ is updated, computation can be distributed across many processors 3. applied to Atari Pong, MuJoCo humanoid walking 4. competitive with Deep Q-Learning on these tasks
1. Evolutionary Strategy: select top 20% of samples and fit a new Gaussian distribution 2. Variational Inference: minimize Reverse KL-Divergence and backpropagate differentials through network, or differentiate with respect to μi, σi
1. KL-Divergence is used in some policy-based deep reinforcement learning algorithms such as Trust Region Policy Optimization (TPRO) (but we will not cover these in detail). 2. KL-Divergence is also important in other areas of Deep Learning, such as Variational Autoencoders.
1. augment A3C with unsupervised auxiliary tasks 2. encourage exploration, increased entropy 3. encourage actions for which the rewards are less predictable 4. concentrate on state features from which the preceding action is more predictable 5. transfer learning (between tasks) 6. inverse reinforcement learning (infer rewards from policy) 7. hierarchical RL 8. multi-agent RL
1. David Silver, Deep Reinforcement Learning Tutorial, http://icml.cc/2016/tutorials/deep rl tutorial.pdf 2. A Brief Survey of Deep Reinforcement Learning, https://arxiv.org/abs/1708.05866 3. Asynchronous Methods for Deep Reinforcement Learning, https://arxiv.org/abs/1602.01783 4. Evolution Strategies as a Scalable Alternative to Reinforcement Learning, https://arxiv.org/abs/1703.03864 5. Eric Jang, Beginner’s Guide to Variational Methods, http://blog.evjang.com/2016/08/variational-bayes.html
1. Content Addressable Memory 2. Hopfield Network 3. Generative Models 4. Boltzmann Machine 5. Restricted Boltzmann Machine 6. Deep Boltzmann Machine 7. Greedy Layerwise Pretraining 
Humans have the ability to retrieve something from memory when presented with only part of it.
We want to store a set of images in a neural network in such a way that, starting with a corrupted or occluded version, we can recon- struct the original image.
We can try to define an energy function E(x) in configuration space, in such a way that the local minima of this energy function correspond to the stored items.
Example: Place n queens on an n-by-n chessboard in such a way that no two queens are attacking each other. We assume there is exactly one queen on each column, so we just need to assign a row to each queen, in such a way that there are no “conflicts”.
Some algorithms for solving Constraint Satisfaction Problems work by “Iterative Improvement” or “Local Search”. These algorithms assign all variables randomly in the beginning (thus violating several constraints), and then change one variable at a time, trying to reduce the number of violations at each step.
1. Variable selection: randomly select any conflicted variable 2. Value selection by min-conflicts heuristic: choose value that violates the fewest constraints
The term Hill-climbing suggests climbing up to regions of greater “fitness”.
When we are minimizing violated constraints, it makes sense to think of starting at the top of a ridge and climbing down into the valleys.
1. The number of patterns p that can be reliably stored in a Hopfield network is proportional to the number of neurons d in the network. 2. A careful mathematical analysis shows that if p/d < 0.138, we can expect the patterns to be stored and retrieved successfully. 3. If we try to store more patterns than these, additional, “spurious” stable states may emerge.
The Hopfield Network is used to store specific items and retrieve them. What if, instead, we want to generate new items, which are somehow “similar” to the stored items, but not quite the same. This is known as a generative model. The first attempt to do this using neural networks was the Boltzmann Machine.
The Boltzmann Machine uses exactly the same energy function as the Hopfield network The Boltzmann Machine is very similar to the Hopfield Network, except that 1. components (neurons) xi take on the values 0, 1 instead of −1, +1 2. used to generate new states rather than retrieving stored states 3. update is not deterministic but stochastic, using the sigmoid
The Boltzmann Distribution is a probability distribution over a state space, given by 1. E(x) is an energy function 2. T is a temperature parameter 3. Z is the partition function which ensures that ∑ p(x) = 1 In most cases, it is too complicated to compute the partition function directly. But, we can sample from the distribution by an iterative process using the relative probability of neighboring states.
The Boltzmann Machine is limited in that the probability of each unit must be a linearly separable function of the surrounding units. It becomes more powerful if we make a division between “visible” units v and “hidden” units h. The visible and hidden units roughly correspond to input and hidden units in a feedforward network. The aim is that the hidden units should learn some hidden features or “latent variables” which help the system to model the distribution of the inputs.
If we allow visible-to-visible and hidden-to-hidden connections, the network takes too long to train. So we normally restrict the model by allowing only visible-to-hidden connections. This is known as a Restricted Boltzmann Machine. 1. inputs are binary vectors 2. two-layer bi-directional neural network: visible layer v and hidden layer h 3. no vis-to-vis or hidden-to-hidden connections 4. all visible units connected to all hidden units 5. trained to maximize the expected log probability of the data
With the Restricted Boltzmann Machine, we can sample from the Boltzmann distribution as follows: choose v0 randomly then sample h0 from p(h|v0) then sample v1 from p(v|h0) then sample h1 from p(h|v1) etc.
It was noticed in the early 2000’s that the process can be sped up by taking just one additional sample instead of running for many iterations. 1. v0,h0 are used as positive sample, and v1,h1 as negative sample 2. this can be compared to the Negative Sampling that was used with word2vec – it is not guaranteed to approximate the true gradient, but it works well in practice
The same approach can be applied iteratively to a multi-layer network. The weights from the input to the first hidden layer are trained first. Keeping those fixed, the weights from the first to the second hidden layer are trained, and so on.
One application for the deep Bolzmann machine is greedy unsupervised layerwise pretraining. Each pair of layers in succession is trained as an RBM. The resulting values are then used as the initial weights and biases for a feedforward neural network, which is then trained by backpropagation for some other task, such as classification. For the sigmoid or tanh activation function, this kind of pre-training leads to a much better result than training directly by backpropagation from random initial weights.
