1. Image Processing  Convolutional Networks 2. Language Processing  Recurrent Networks  Long Short Term Memory  Word Embeddings 3. Deep Reinforcement Learning  Deep Q-Learning  Policy Gradients  Asynchronous Advantage Actor Critic c(cid:13)Alan Blair, 2017-8
1. image classication 2. object detection 3. object segmentation 4. style transfer 5. generating images 6. generating art c(cid:13)Alan Blair, 2017-8
Some functions cannot be learned with a 2-layer sigmoidal network. 6 4 2 0 2 4 6 6 4 2 0 2 4 6 For example, this Twin Spirals problem cannot be learned with a 2-layer network, but it can be learned using a 3-layer network if we include shortcut connections between non-consecutive layers. c(cid:13)Alan Blair, 2017-8
1. black and white, resolution 28  28 2. 60,000 images 3. 10 classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) c(cid:13)Alan Blair, 2017-8
1. color, resolution 32  32 2. 50,000 images 3. 10 classes c(cid:13)Alan Blair, 2017-8
1. color, resolution 227  227 2. 1.2 million images 3. 1000 classes c(cid:13)Alan Blair, 2017-8
Training by backpropagation in networks with many layers is difcult. When the weights are small, the differentials become smaller and smaller as we backpropagate through the layers, and end up having no effect. When the weights are large, the activations in the higher layers will saturate to extreme values. As a result, the gradients at those layers will become very small, and will not be propagated to the earlier layers. When the weights have intermediate values, the differentials will sometimes get multiplied many times is places where the transfer function is steep, causing them to blow up to large values. c(cid:13)Alan Blair, 2017-8
4 3 2 1 0 -1 -2 -4 4 3 2 1 0 -1 -2 -4 4 3 2 1 0 -1 -2 -4 -2 0 2 4 -2 0 2 4 Sigmoid Rectied Linear Unit (ReLU) 4 3 2 1 0 -1 -2 -4 -2 0 2 4 -2 0 2 4 Hyperbolic Tangent Scaled Exponential Linear Unit (SELU) c(cid:13)Alan Blair, 2017-8
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNNs can now be simulated with massive parallelism, using GPUs c(cid:13)Alan Blair, 2017-8
Suppose we want to classify an image as a bird, sunset, dog, cat, etc. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image. c(cid:13)Alan Blair, 2017-8
j j+m k k+n l j, k = g(cid:0)bi +  Z i l M1 m=0 N1 n=0 l , m, nV l K i j+m, k+n(cid:1) The same weights are applied to the next M  N block of inputs, to compute the next hidden unit in the convolution layer (weight sharing). c(cid:13)Alan Blair, 2017-8
First Layer Second Layer Third Layer c(cid:13)Alan Blair, 2017-8
The 5  5 window of the rst convolution layer extracts from the original 32  32 image a 28  28 array of features. Subsampling then halves this size to 14  14. The second Convolution layer uses another 5  5 window to extract a 10  10 array of features, which the second subsampling layer reduces to 5  5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits 0 to 9. c(cid:13)Alan Blair, 2017-8
1. LeNet, 5 layers (1998) 2. AlexNet, 8 layers (2012) 3. VGG, 19 layers (2014) 4. GoogleNet, 22 layers (2014) 5. ResNets, 152 layers (2015) 6. DenseNets, 160 layers (2017) c(cid:13)Alan Blair, 2017-8
1. 650K neurons 2. 630M connections 3. 60M parameters 4. more parameters that images  danger of overtting c(cid:13)Alan Blair, 2017-8
1. Rectied Linear Units (ReLUs) 2. overlapping pooling (width = 3, stride = 2) 3. stochastic gradient descent with momentum and weight decay 4. data augmentation to reduce overtting 5. 50% dropout in the fully connected layers c(cid:13)Alan Blair, 2017-8
Nodes are randomly chosen to not be used, with some xed probability (usually, one half). c(cid:13)Alan Blair, 2017-8
Idea: Take any two consecutive stacked layers in a deep network and add a skip connection which bipasses these layers and is added to their output. c(cid:13)Alan Blair, 2017-8
Recently, good results have been achieved using networks with densely connected blocks, within which each layer is connected by shortcut connections to all the preceding layers. c(cid:13)Alan Blair, 2017-8
content + style  new image c(cid:13)Alan Blair, 2017-8
There are many tasks which require a sequence of inputs to be processed rather than a single input. 1. speech recognition 2. time series prediction 3. machine translation 4. handwriting recognition 5. image captioning How can neural network models be adapted for these tasks? c(cid:13)Alan Blair, 2017-8
1. at each time step, hidden layer activations are copied to context layer 2. hidden layer receives connections from input and context layers 3. the inputs are fed one at a time to the network, it uses the context layer to remember whatever information is required for it to produce the correct output c(cid:13)Alan Blair, 2017-8
1. we can unroll a recurrent architecture into an equivalent feedforward architecture, with shared weights 2. applying backpropagation to the unrolled architecture is reffered to as backpropagation through time 3. we can backpropagate just one timestep, or a xed number of timesteps, or all the way back to beginning of the sequence c(cid:13)Alan Blair, 2017-8
1. SRN with 3 hidden units can learn to predict anbncn by counting up and down simultaneously in different directions, thus producing a star shape. c(cid:13)Alan Blair, 2017-8
1. Simple Recurrent Networks (SRNs) can learn medium-range dependencies but have difculty learning long range dependencies 2. Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) can learn long range dependencies better than SRN c(cid:13)Alan Blair, 2017-8
LSTM  context layer is modulated by three gating mechanisms: forget gate, input gate and output gate. http://colah.github.io/posts/2015-08-Understanding-LSTMs/ c(cid:13)Alan Blair, 2017-8
Synonyms for elegant stylish, graceful, tasteful, discerning, rened, sophisticated, dignied, cultivated, distinguished, classic, smart, fashionable, modish, decorous, beautiful, artistic, aesthetic, lovely; charming, polished, suave, urbane, cultured, dashing, debonair; luxurious, sumptuous, opulent, grand, plush, high-class, exquisite Synonyms, antonyms and taxonomy require human effort, may be incomplete and require discrete choices. Nuances are lost. Words like king, queen can be similar in some attributes but opposite in others. Could we instead extract some statistical properties automatically, without human involvement? c(cid:13)Alan Blair, 2017-8
The kth row vk of W is a representation of word k. j of W is an (alternative) representation of word j. The jth column v If the (1-hot) input is k, the linear sum at each output will be u j = v j Tvk c(cid:13)Alan Blair, 2017-8
King + Woman - Man  Queen More generally, A is to B as C is to ?? d = argmaxx (vc + vb  va)Tvx ||vc + vb  va|| c(cid:13)Alan Blair, 2017-8
1. An agent interacts with its environment. 2. There is a set S of states and a set A of actions. 3. At each time step t, the agent is in some state st . It must choose an action at , whereupon it goes into state st+1 = (st , at) and receives reward rt = R (st , at) 4. Agent has a policy  : S  A. We aim to nd an optimal policy  which maximizes the cumulative reward. 5. In general, , R and  can be multi-valued, with a random element, in which case we write them as probability distributions (st+1 = s | st , at ) R (rt = r | st , at ) (at = a | st ) c(cid:13)Alan Blair, 2017-8
For a deterministic environment, , Q and V  are related by (s) = argmaxa Q(s, a) Q(s, a) = R (s, a) + V ((s, a)) V (s) = max b Q(s, b) Q(s, a) = R (s, a) +  max b Q((s, a), b) So This allows us to iteratively approximate Q by Q(st , at )  rt +  max b Q(st+1 , b) If the environment is stochastic, we instead write Q(st , at)  Q(st , at ) +  [ rt +  max b Q(st+1 , b)  Q(st , at)] c(cid:13)Alan Blair, 2017-8
1. end-to-end learning of values Q(s, a) from pixels s 2. input state s is stack of raw pixels from last 4 frames  8-bit RGB images, 210  160 pixels 3. output is Q(s, a) for 18 joystick/button positions 4. reward is change in score for that timestep c(cid:13)Alan Blair, 2017-8
1. use policy network to choose actions 2. learn a parameterized Value function Vu(s) by TD-Learning 3. estimate Q-value by n-step sample Q(st , at ) = rt+1 +  rt+2 + . . . +  n1rt+n +  nVu(st+n) 4. update policy by    +  [Q(st , at ) Vu(st )] log (at | st ) 5. update Value function my minimizing [Q(st , at) Vu(st)]2 c(cid:13)Alan Blair, 2017-8
1. Hopeld Networks 2. Restricted Boltzmann Machines 3. Autoencoders 4. Generative Adversarial Networks c(cid:13)Alan Blair, 2017-8
