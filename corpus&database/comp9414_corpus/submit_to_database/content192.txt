1. Reinforcement Learning vs. Supervised Learning 2. Models of Optimality 3. Exploration vs. Exploitation 4. Temporal Difference learning 5. Q-Learning
Agent World Model Planning Bayesian Learning Inference Learning Perception Action Statistical Learning Reinforcement Learning Environment
Recall: Supervised Learning 1. We have a training set and a test set, each consisting of a set of examples. For each example, a number of input attributes and a target attribute are specied. 2. The aim is to predict the target attribute, based on the input attributes. 3. Various learning paradigms are available:  Decision Trees  Neural Networks  .. others ..
Patrons? None Some Full F T Hungry?  Yes No Type? F French Italian Thai Burger T F Fri/Sat? T No  Yes F T
Output units Hidden units Input units ai Wj,i aj Wk,j ak
Supervised Learning can also be used to learn Actions, if we construct a training set of situation-action pairs (called Behavioral Cloning). However, there are many applications for which it is difcult, inappropri- ate, or even impossible to provide a training set 1. optimal control  mobile robots, pole balancing, ying a helicopter 2. resource allocation  job shop scheduling, mobile phone channel allocation 3. mix of allocation and control  elevator control, backgammon
Agent World Model Planning Perception Action Reinforcement Learning Environment
1. An agent interacts with its environment. 2. There is a set S of states and a set A of actions. 3. At each time step t, the agent is in some state st . It must choose an action at , whereupon it goes into state st+1 = (st, at ) and receives reward r(st, at ). 4. In general, r() and () can be multi-valued, with a random element 5. The aim is to nd an optimal policy  : S  A which will maximize the cumulative reward.
Is a fast nickel worth a slow dime? Finite horizon reward Average reward Innite discounted reward h  i=0 rt+i 1 h h1  i=0  irt+i, lim h   i=0 rt+i 0   < 1 1. Finite horizon reward is simple computationally 2. Innite discounted reward is easier for proving theorems 3. Average reward is hard to deal with, because cant sensibly choose between small reward soon and large reward very far in the future.
For each state s  S, let V (s) be the maximum discounted reward obtainable from s. 3 2 1 START + 1  1 1 2 3 4 (a) .5 .5 .5 .33 .33 1.0 +1 .5 .5 .33 .33 .33 1.0 3 0.0380  0.0886  0.2152 + 1 1 2 0.1646 0.4430  1 .5 .5 .33 .33 .5 .5 .5 .5 .33 (b) .33 .5 1 0.2911 0.0380 0.5443 0.7722 1 2 3 4 (c) Learning this Value Function can help to determine the optimal strategy.
Environments can be: 1. passive and stochastic (as in previous slide) 2. active and deterministic (chess) 3. active and stochastic (backgammon)
The special case of an active, stochastic environment with only one state is called the K-armed Bandit Problem, because it is like being in a room with several (friendly) slot machines, for a limited time, and trying to collect as much money as possible. Each action (slot machine) provides a different average reward.
Most of the time we should choose what we think is the best action. However, in order to ensure convergence to the optimal strategy, we must occasionally choose something different from our preferred action, e.g. 1. choose a random action 5% of the time, or 2. use a Bolzmann distribution to choose the next action: P(a) = V (a)/T e e V (b)/T  bA
I was born to try... But youve got to make choices Be wrong or right Sometimes youve got to sacrice the things you like. - Delta Goodrem
TD(0) [also called AHC, or Widrow-Hoff Rule] V (s)  V (s) +  [ r(s, a) +  V ((s, a))  V (s) ] ( = learning rate) The (discounted) value of the next state, plus the immediate reward, is used as the target value for the current state. A more sophisticated version, called TD(), uses a weighted average of future states.
For each s  S, let V (s) be the maximum discounted reward obtainable from s, and let Q(s, a) be the discounted reward available by rst doing action a and then acting optimally. Then the optimal policy is where then so (s) = argmaxaQ(s, a) Q(s, a) = r(s, a) + V ((s, a)) V (s) = max a Q(s, a), Q(s, a) = r(s, a) +  max b Q((s, a), b) which allows us to iteratively approximate Q by Q(s, a)  r +  max b Q((s, a), b)
Theorem: Q-learning will eventually converge to the optimal policy, for any deterministic Markov decision process, assuming an appropriately randomized strategy. (Watkins & Dayan 1992) Theorem: TD-learning will also converge, with probability 1. (Sutton 1988, Dayan 1992, Dayan & Sejnowski 1994)
1. Delayed reinforcement  reward resulting from an action may not be received until several time steps later, which also slows down the learning 2. Search space must be nite  convergence is slow if the search space is large  relies on visiting every state innitely often 3. For real world problems, we cant rely on a lookup table  need to have some kind of generalisation (e.g. TD-Gammon)
