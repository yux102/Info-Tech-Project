1. Linear Separability 2. Multi-Layer Networks 3. Backpropagation 4. Application - ALVINN 5. Training Tips
(Articial) Neural Networks are made up of nodes which have 1. inputs edges, each with some weight 2. outputs edges (with weights) 3. an activation level (a function of the inputs) Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: activationi = g(si) = g( j wi jx j) Some nodes are inputs (sensing), some are outputs (action)
w1 s  g  g(s)  x1 x2   w2   w0=-th   s = w1x1 + w2x2th x1, x2 are inputs w1, w2 are synaptic weights 1 = w1x1 + w2x2 + w0 th is a threshold w0 is a bias weight g is transfer function
Originally, a (discontinuous) step function was used for the transfer function: g(s) = n 1, 0, if if s  0 s < 0
Q: what kind of functions can a perceptron compute? A: linearly separable functions Examples include: AND OR NOR w1 = w2 = 1.0, w0 = 1.5 w1 = w2 = 1.0, w0 = 0.5 w1 = w2 = 1.0, w0 = 0.5 Q: How can we train it to learn a new function?
Adjust the weights as each input is presented. recall: s = w1x1 + w2x2 + w0 if g(s) = 0 but should be 1, if g(s) = 1 but should be 0, wk  wk +  xk w0  w0 +  wk  wk   xk w0  w0   so s  s +  (1 +  x2 k ) so s  s   (1 +  x2 k ) k k otherwise, weights are unchanged. ( > 0 is called the learning rate) Theorem: This will eventually learn to classify the data correctly, as long as they are linearly separable.
Problem: many useful functions are not linearly separable (e.g. XOR) I 1 1 I 1 1 0 0 1 I 2 0 0 1 I 2 I 1 1 0 0 ? 1 I 2 (a) I 1 and  I 2 (b) I 1 or I 2 (c) I 1 xor I 2 Possible solution: x1 XOR x2 can be written as: (x1 AND x2) NOR (x1 NOR x2) Recall that AND, OR and NOR can be implemented by perceptrons.
XOR NOR +0.5 1 1 +1 1 AND NOR 1.5 +1 1 +0.5 Given an explicit logical function, we can design a multi-layer neural network by hand to compute that function. But, if we are just given a set of training data, can we train a multi-layer network to t these data?
In 1969, Minsky and Papert published a book highlighting the limitations of Perceptrons, and lobbied various funding agencies to redirect funding away from neural network research, preferring instead logic-based methods such as expert systems. It was known as far back as the 1960s that any given logical function could be implemented in a 2-layer neural network with step function activations. But, the the question of how to learn the weights of a multi-layer neural network based on training examples remained an open problem. The solution, which we describe in the next section, was found in 1976 by Paul Werbos, but did not become widely known until it was rediscovered in 1986 by Rumelhart, Hinton and Williams.
We dene an error function E to be (half) the sum over all input patterns of the square of the difference between actual output and desired output E = 1 2 (z  t)2 If we think of E as height, it denes an error landscape on the weight space. The aim is to nd a set of weights for which E is very low.
Problem: because of the step function, the landscape will not be smooth but will instead consist almost entirely of at local regions and shoulders, with occasional discontinuous jumps.
a i +1 a i +1 a i +1 t ini ini ini 1 (a) Step function (b) Sign function (c) Sigmoid function Replace the (discontinuous) step function with a differentiable function, such as the sigmoid: or hyperbolic tangent g(s) = 1 1 + es g(s) = tanh(s) = es  es es + es = 2(cid:0) 1 1 + e2s(cid:1)  1
Recall that the error function E is (half) the sum over all input patterns of the square of the difference between actual output and desired output E = 1 2 (z  t)2 The aim is to nd a set of weights for which E is very low. If the functions involved are smooth, we can use multi-variable calculus to adjust the weights in such a way as to take us in the steepest downhill direction. w  w   E w Parameter  is called the learning rate.
If, say Then y = y(u) u = u(x) y x = y u u x This principle can be used to compute the partial derivatives in an efcient and localized manner. Note that the transfer function must be differentiable (usually sigmoid, or tanh). Note: if if 1 , z(s) = 1 + es z(s) = tanh(s), z(s) = z(1  z). z(s) = 1  z2 .
z s v 1 v 2 c 1y u1 b1 11w w21 1x b2 w12 2y u2 w22 2x u1 = b1 + w11x1 + w12x2 y1 = g(u1) s = c + v1y1 + v2y2 z = g(s) E = 1 2 (z  t)2
Partial Derivatives = z  t E z dz ds s y1 dy1 du1 Useful notation E s 1 = E u1 2 = E u2 out = Then = g(s) = z(1  z) out = (z  t) z (1  z) = v1 = y1(1  y1) = out y1 E v1 1 = out v1 y1 (1  y1) E w11 = 1 x1 Partial derivatives can be calculated efciently by packpropagating deltas through the network.
1. Autonomous Driving 2. Game Playing 3. Credit Card Fraud Detection 4. Handwriting Recognition 5. Financial Prediction
1. Autonomous Land Vehicle In a Neural Network 2. later version included a sonar range nder  8  32 range nder input retina  29 hidden units  45 output units 3. Supervised Learning, from human actions (Behavioral Cloning)  additional transformed training items to cover emergency situations 4. drove autonomously from coast to coast
1. re-scale inputs and outputs to be in the range 0 to 1 or 1 to 1 2. initialize weights to very small random values 3. on-line or batch learning 4. three different ways to prevent overtting:  limit the number of hidden nodes or connections  limit the training time, using a validation set  weight decay 5. adjust learning rate (and momentum) to suit the particular task
