Instructions on how to access the course materials are given here: 1. http://www.cse.unsw.edu.au/~cs3411/ 2. http://www.cse.unsw.edu.au/~cs9414/ 
1. Alan Blair 2. blair@cse.unsw.edu.au 3. K17-412C 4. 9385-7131
1. Lecture Friday 10-1 in John Clancy Auditorium 2. Friday tutorials begin in Week 1 3. Mon, Tue, Wed, Thu tutorials begin in Week 2 4. Prolog Labs (optional) start Friday of Week 1 and continue until Week 5
1. AI, Tasks, Agents & Prolog: What is AI?, Classifying Tasks, Agent Types, Prolog Programming 2. Solving Problems by Search: Path Search, Heuristic Path Search, Games, Constraint Satisfaction 3. Logic & Uncertainty: Logical Agents, First Order Logic, Uncertainty 4. Learning: Learning and Decision Trees, Perceptrons & Neural Networks, Reinforcement Learning
1. sign up to OpenLearning (through Moodle) 2. work through this weeks Learning Activities 3. set up and log into your CSE account 4. start working through the Prolog Exercises
Prolog Labs are not compulsory. You can attend at any time(s) that suit you. Lab Consultants will be there to help you if you have questions.
1. very useful for AI and search 2. good for you to see an example of a non-imperative language 3. logic programming languages like Prolog have recently had a resurgence of popularity in the computing industry
Recommended Text: 1. Stuart Russell and Peter Norvig, Artificial Intelligence: a Modern Approach, 3rd Edition, Prentice Hall, 2009. 2. Ivan Bratko, Programming in Prolog for Artificial Intelligence, 4th Edition, Pearson, 2013. Reference Text: 1. Nils J. Nilsson, Artificial Intelligence: a New Synthesis, Morgan Kaufmann, 1998. 2. Valentino Braitenberg, Vehicles: Experiments in Synthetic Psychology, MIT Press, 1984.
Assessment will consist of: Assignments 40% Written Exam 60% In order to pass the course, you must score 1. at least 16/40 for the assignments 2. at least 24/60 for the exam 3. a combined mark of at least 50/100
The assignments may, for example, involve writing a program to: 1. enable an agent to act in a simulated environment 2. solve a problem using search techniques 3. play a game 4. apply a machine learning algorithm
1. ALL work submitted for assessment must be your own work 2. for an individual assignment, collaborative work in the form of “think tanking” is encouraged, but students are not allowed to derive code together as a group during such discussions 3. in the case of a group assignment, code must not be obtained from outside the group 4. plagiarism detection software may be used on submitted work 5. UNSW Plagiarism Policy: https://student.unsw.edu.au/plagiarism
1. COMP9417 Machine Learning and Data Mining 2. COMP4418 Knowledge Representation and Reasoning 3. COMP3431 Robotic Software Architecture 4. COMP9517 Machine Vision 5. COMP9444 Neural Networks and Deep Learning 6. 4th Year Thesis topics
1. Philosophy (428 B.C  present) 2. Mathematics (c. 800  present) 3. Psychology (1879  present) 4. Linguistics (1957  present) 5. Computer engineering (1940 present) 6. Biocybernetics (1940’s  present) 7. Neurology (1950’s  present)
1. Philosophy / Arts - what is mind ? ~ mind is like a machine - it operates on knowledge encoded in an internal language - thought and reasoning can be used to arrive at the right actions - what is consciousness ?
1. Philosophy 2. Mathematics / Physics / Statistics / Logic - tools to manipulate logical statements - tools to manipulate probabilistic statements - algorithms and their analysis - complexity issues - dynamical systems / RNNs - statistical physics / Hopfieled nets - methods for pattern recognition - models using differential equations, statistics, etc.
1. Philosophy 2. Mathematics 3. Psychology / Cognitive Science - humans and animals are information processing machines - introspection - experiments - what is intelligence ? (http://www.iqtest.com/) - what is learning and memory ?
1. Philosophy 2. Mathematics 3. Psychology 4. Linguistics / Computational Linguistics / Formal Languages - language use fits into the information processing machine’ model - Chomsky hierachy - natural language processing
1. Philosophy 2. Mathematics 3. Psychology 4. Linguistics 5. Computer Engineering - build computers and robots fast enough to make AI applications and simulations possible - links to mechanical engineering
1. Philosophy 2. Mathematics 3. Psychology 4. Linguistics 5. Computer Engineering 6. Biocybernetics and Neurobiology - molecular level - single cell recordings - cell circuit level - information processing in biological systems
1. Philosophy 2. Mathematics 3. Psychology 4. Linguistics 5. Computer Engineering 6. Biocybernetics / Neurobiology 7. Neurology / Psychiatry - drugs - learning from disorders - brain scans (EEG/MEG/PET/MRI)
1. Philosophy 2. Mathematics 3. Psychology 4. Linguistics 5. Computer Engineering 6. Biocybernetics / Neurobiology 7. Neurology / Psychiatry AI is a central topic of current interdisciplinary scientific investigation.
1. 380BC Plato (Rationalism - innateness) 2. 330BC Aristotle (Empricism - experience) 3. 1641 Descartes (mind-body Dualism) 4. 1781 Kant (Critique of Pure Reason) 5. 1899 Sigmund Freud (Psychology) 6. 1953 B.F. Skinner (Behaviourism)
1. Greek Mythology (Pygmalion, Talos) 2. 1580 Rabbi Loew (Golem, a clay man brought to life) 3. 1818 Mary Shelley (Frankenstein) 4. 1883 Carlo Collodi (Pinocchio) 5. 1920 Karel Capek (Rossum’s Universal Robots) 6. 1950 Isaac Asimov (Three Laws of Robotics) 7. 1951 Osamu Tezuka (Astro Boy)
1. 1642 Blaise Pascal (mechanical adding machine) 2. 1694 Gottfried Leibniz (mechanical calculator) 3. 1769 Wolfgang von Kempelen (Mechanical Turk) 4. 1837 Charles Babbage & Ada Lovelace (Difference Engine) 5. 1848 George Boole (the Calculus of Logic) 6. 1879 Gottlob Frege (Predicate Logic) 7. 1950 Turing Test 8. 1956 Dartmouth conference
1. proposed by Alan Turing in 1950 2. a human interrogates/converses with the computer via a teletype 3. the aim is for the computer to imitate a human well enough to fool the human interrogator
Turing thought a computer would pass this test by end of the 20th century. The Loebner Prize (Turing test) competition is held each year. The Total Turing Test allows physical objects to be passed to the machines as well as characters via the teletype.
YouTube: Building Watson - A Brief Overview of the DeepQA Project
1. Misplaced emphasis on abstract reasoning rather than low-level perception and behaviour  Intelligence Without Reason (Brooks 1991) 2. General Intelligence vs. Specific Modules  How the Mind Works (Pinker, 1997) 3. Philosophical Objections to AI  Godels Theorem, Undecidability (Lucas 1961, Penrose 1989)  Chinese Room (Searle 1980)  What Computers (Still) Cant Do (Dreyfus 1972,1993)
Which of the following can be done at present? 1. Play a decent game of table tennis (ping-pong) 2. Drive in the center of Cairo, Egypt 3. Drive along a curving mountain road 4. Play games like Chess, Go, Bridge, Poker 5. Discover and prove a new mathematical theorem 6. Write an intentionally funny story 7. Give competent legal advice in a specialized area of law 8. Translate spoken English into spoken Swedish (or Chinese) in real time
1. Artificial Intelligence has a long history in diverse areas of science as well as philosophy and literature 2. Debates continue over the definition of Intelligence 3. Significant progress has been made, but many challenges remain.
1. Module 2: Wumpus World, Robocup Soccer 2. Module 3: Path Planning (mazes, graph search) 3. Module 4: Path Search Puzzles (8-puzzle, Rubik’s cube) 4. Module 5: Games (board games, dice games, card games) 5. Module 6: Constraint Satisfaction (N-queens, Sudoku)
We want a unified framework that can be used to specify, characterize, compare and contrast different AI tasks.
Agents can be evaluated empirically, sometimes analysed mathematically Agent is a function from percept sequences to actions Ideal rational agent would pick actions which are expected to maximise the performance measure.
1. Performance measure 2. Environment 3. Actuators 4. Sensors
Performance measure: +1 for a Win, + 1/2 for a Draw, 0 for a Loss. Environment: board, pieces Actuators: move piece to new square Sensors: which piece is on which square
Performance measure: safety, reach destination, maximize profits, obey laws, passenger comfort, . . . Environment: city streets, freeways, traffic, pedestrians, weather, customers, . . . Actuators: steer, accelerate, brake, horn, speak/display, . . . Sensors: video, accelerometers, gauges, engine sensors, keyboard, GPS, . . .
1. Environment: Squares adjacent to Wumpus are Smelly, Squares adjacent to Pit are Breezy, Glitter iff Gold is in the same square, Shoot:(1) kills Wumpus if you are facing it,(2) uses up the only arrow Grab:(1) picks up Gold if in same square 
1. Performance measure:  Return with Gold +1000, death -1000,  -1 per step, -10 for using the arrow 2. Actuators:  Left, Right, Forward, Grab, Shoot 3. Sensors:  Breeze, Glitter, Stench
Simulated vs. Situated or Embodied, Static vs. Dynamic, Discrete vs. Continuous, Fully Observable vs. Partially Observable, Deterministic vs. Stochastic, Episodic vs. Sequential, Known vs. Unknown, Single-Agent vs. Multi-Agent
Simulated: a separate program is used to simulate an environment, feed percepts to agents, evaluate performance, etc. Static: environment doesn’t change while the agent is deliberating Discrete: finite (or countable) number of possible percepts/actions Fully Observable: percept contains all relevant information about the world Deterministic: current state of world uniquely determines the next Episodic: every action by the agent is evaluated independently Known: the rules of the game, or physics/dynamics of the environment are known to the agent Single-Agent: only one agent acting in the environment
1. Chess is Simulated, Robocup is Situated and Embodied Simulated: a separate program is used to simulate an environment, feed percepts to agents, evaluate performance, etc. Situated: the agent acts directly on the actual environment Embodied: the agent has a physical body in the world Question: If Chess is played on a physical board with actual pieces, would it become embodied?
1. Chess is Simulated, Robocup is Situated and Embodied Simulated: a separate program is used to simulate an environment, feed percepts to agents, evaluate performance, etc. Situated: the agent acts directly on the actual environment Embodied: the agent has a physical body in the world Question: If Chess is played on a physical board with actual pieces, would it become embodied? Answer: Yes it would; however, we normally abstract away the game itself (choice of moves), and treat the movement of the pieces as a separate task.
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic Static: the environment does not change while the agent is thinking Dynamic: the environment may change while the agent is thinking e.g. if the ball is in front of you but you take too long to act, another player may come in and kick it away Notes: 1. In a multi-player game, Static environment will obviously change when the opponent moves, but cannot change once it is our turn. 2. In tournament Chess, the clock will tick down while the player is thinking (thus making it slightly non-static).
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous Discrete: only a finite (or countable) number of discrete percepts / actions Continuous: states, percepts or actions can vary continuously e.g. each piece must be on one square or the other, not half way in between.
1. Chess is simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous 4. Chess is Fully Observable, Robocup (Legged) is Partially Observable Fully Observable: agent percept contains all relevant information about the world Partially Observable: some relevant information is hidden from the agent [watch Dog’s Eye View video] [watch Melbourne vs. Cornell video] Note: The Robocup F180 League is close to fully observable, because the robots have access to an external computer connected to an overhead camera.
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous 4. Chess is fully observable, Robocup (Legged) is Partially Observable 5. Chess is Deterministic, Robocup is Stochastic Deterministic: the current state uniquely determines the next state Stochastic: there is some random element involved Note: The non-determinisim partly arises because the physics can only be modeled with limited precision. But, even if it could be modeled perfectly, there would still be randomness due to quantum mechanical effects.
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous 4. Chess is Fully Observable, Robocup (Legged) is Partially Observable 5. Chess is Deterministic, Robocup is Stochastic 6. Both Chess and Robocup are Sequential Episodic: every action by the agent is evaluated independently Sequential: the agent is evaluated based on a long sequence of actions Both Chess and Robocup are considered Sequential, because evaluation only happens at the end of a game, and it is necessary to plan several steps ahead in order to play the game well.
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous 4. Chess is Fully Observable, Robocup (Legged) is Partially Observable 5. Chess is Deterministic, Robocup is Stochastic 6. Both Chess and Robocup are Sequential 7. Both Chess and Robocup are Known Known: the rules of the game, or physics / dynamics of the environment, are known to the agent. Note: Video Games like Infinite Mario are sometimes set up in such a way that the dynamics of the environment are Unknown to the agent.
1. Chess is Simulated, Robocup is Situated and Embodied 2. Chess is Static, Robocup is Dynamic 3. Chess is Discrete, Robocup is Continuous 4. Chess is Fully Observable, Robocup (Legged) is Partially Observable 5. Chess is Deterministic, Robocup is Stochastic 6. Both Chess and Robocup are Sequential 7. Both Chess and Robocup are Known 8. Both Chess and Robocup are Multi-Agent Examples of Single-Agent tasks include: 9. solving puzzles like Sudoku, or Rubik’s cube 10. Solitaire card games
1. Like Chess, Wumpus World is Simulated, Static, Discrete, Sequential and Known. 2. Wumpus World is Partially Observable  for example, you don’t know where the Wumpus is. 3. Wumpus World is normally considered Deterministic, because the location of the Wumpus, Gold and Pits are determined at the beginning and don’t change after that. 4. Wumpus World is Single-Agent. We consider the Wumpus as a natural feature, because it doesn’t move and can’t make any choices.
1. Like Chess, Backgammon is Simulated, Static, Discrete, Sequential and Known and Multi-Agent. 2. Normally, we consider Backgammon to be Fully Observable and Stochastic. The dice rolls are random, but all players can see them. 3. If instead the dice rolls are generated by a computer using a pseudo- random number generator, with a specified seed, the game could be considered Deterministic but Partially Observable. In this case, the sequence of dice rolls is fully determined by the seed, but future dice rolls are not observable by the players.
1. Card Games like Poker, Rummy or Mahjong are Simulated, Static, Discrete, Sequential, Known and Multi-Agent. 2. Card Games are Stochastic if the cards are shuffled during the game, but can be considered Deterministic if the cards are shuffled only once, before the game begins. 3. Card Games are Partially Observable and involve Asymmetric Information in the sense that each player can see their own cards but not those of other players.
Rodney Brooks 1991:  Situatedness: The robots are situated in the world  they do not deal with abstract descriptions, but with the here and now of the environment which directly influences the behaviour of the system.  Embodiment: The robots have bodies and experience the world directly  their actions are part of a dynamics with the world, and actions have immediate feedback on the robot’s own sensations.
1. Situated but not Embodied: High frequency stock trading system: (1) it deals with thousands of buy/sell bids per second and its responses vary as its database changes. (2) but it interacts with the world only through sending and receiving messages. 2. Embodied but not Situated: an industrial spray painting robot: (1) does not perceive any aspects of the shape of an object presented to it for painting; simply goes through a pre-programmed series of actions (2) but it has physical extent and its servo routines must correct for its interactions with gravity and noise present in the system.
1. AI tasks or environments can be classified in terms of whether they are simulated, static, discrete, fully observable, deterministic, episodic, known, single- or multi- agent. 2. The environment type strongly influences the agent design (discussed in the next section..)
In this course we will consider five different types of agent: 1. Reactive Agent 2. Model-Based Agent 3. Planning Agent 4. Game Playing Agent 5. Learning Agent
1. Choose the next action based only on what they currently perceive, using a “policy” or set of rules which are simple to apply 2. Sometimes pajoratively called “simple reflex agents” - but they can do surprisingly sophisticated things! (1) Swiss robots (2) simulated hockey
1. Reactive Agents have no memory or “state": (1) unable to base decision on previous observations (2) may repeat the same sequence of actions over and over 2. This phenomenon can also be observed in nature:  wasp dragging stung grasshopper into its nest
An agent with a world model but no planning can look into the past, but not into the future; it will perform poorly when the task requires any of the following: 1. searching several moves ahead:  Chess, Rubik’s cube 2. complex tasks requiring many individual steps:  cooking a meal, assembling a watch 3. logical reasoning to achieve goals:  travel to New York
1. Sometimes an agent may appear to be planning ahead but is actually just applying reative rules. {if( Glitter ) then Grab} {else if( Stench ) then Shoot} {else randomly Left, Right or Forward} 2. These rules can be hand-coded, or learned from experience. 3. Agent may appear intelligent, but is not flexible in adapting to new situations.
1. Learning is not a separate module, but rather a set of techniques for improving the existing modules 2. Learning is necessary because: (1) may be difficult or even impossible for a human to design all aspects of the system by hand (2) the agent may need to adapt to new situations without being re-programmed by a human
We must distinguish complexity of learning from complexity of application. The policy for the simulated hockey player took several days of computation to derive (in this case, by evolutionary computation) but, once derived, it can be applied in real time.
1. History of Reactive Agents 2. Braitenberg Vehicles 3. Chemotaxis 4. Behavior-Based Robotics
1. choose the next action based only on what they currently perceive, using a “policy” or set of rules which are simple to apply 2. unable to remember, plan or logically reason 3. interesting behaviors can “emerge” from these simple rules
1. 1948 Alan Turing (importance of embodiment) 2. 1969 Herbert Simon (parable of ant on beach) 3. 1984 Valentino Braitenberg (Vehicles) 4. 1991 Rodney Brooks (Intelligence without Reason) 5. 1995 Lego MindStorms
1. Braitenberg showed how simple arrangements of sensors and motors can lead to surprisingly sophisticated behavior 2. simplest vehicles have two wheels and two sensors 3. sensors respond to a light source 4. response is inversely proportional to distance
1. connections can be: (1) straight or crossed (2) excitatory (+) or inhibitory (-) 2. leads to four behaviors:  hate,  fear,  love,  curiosity   
1. Many single- and multi-cell organisms can direct their movement to swim to areas with higher (or lower) chemical concentration 2. bacteria use flagella to propel themselves: (1) anti-clockwise → rotation  linear motion (2) clockwise rotation → tumbling motion
1. normally, bacterium switches between linear and tumbling motion, producing a random walk 2. if it senses that it is heading in the “right” direction, it will lengthen the current period of linear motion 3. in this way, it can successfully move toward food sources and away from toxins
The rules used by the Didabots: 1. normally, move forward 2. if you detect an obstacle to the left or right, turn away from it 3. if you detect an obstacle directly in front, move forward
Introduced by Rodney Brooks in the late 1980’s as a challenge to “Good Old Fashioned AI” (GOFAI) 1. robots should be based on insects rather than humans 2. tasks like walking and avoiding obstacles rather than playing Chess 3. abandon traditional horizontal decompositon  Sense →  Plan →  Act 4. replace with vertical decompositon or “subsumption architecture”:  each layer can connect sensing right through to action
1. Each layer in the vertical decomposition is a behavior (1) low-level behaviors like avoid hitting things are reactive, connecting sensors directly to actuators (2) mid-level behaviors like build maps make use of a world model (3) high-level behaviors make use of world model and planning 2. higher level behavior may take control from lower-level behavior:  e.g. if the low-level behavior has gotten “stuck” 3. lower level behavior may take control from higher-level behavior:  e.g. to avoid getting burned, or falling down a staircase
1. Valentino Braitenberg, Vehicles: Experiments in Synthetic Psychology, MIT Press, 1984. 2. Rolf Pfeifer & Christian Sheier, “Understanding Intelligence", MIT Press, 1999.:  http://www.ifi.unizh.ch/ailab 3. Rodney Brooks, “Cambrian Intelligence: the Early History of the New AI”, MIT Press, 1999.:  http://www.csail.mit.edu
1. Reactive and Model-Based Agents choose their actions based only on what they currently perceive, or have perceived in the past. 2. a Planning Agent can use Search techniques to plan several steps ahead in order to achieve its goal(s). 3. two classes of search strategies: (1) Uninformed search strategies can only distinguish goal states from non-goal states (2) Informed search strategies use heuristics to try to get closer to the goal
On touring holiday in Romania; currently in Arad. Flight leaves tomorrow from Bucharest; non-refundable ticket. 1. Step 1 Formulate goal: be in Bucharest on time 2. Step 2 Specify task: (1) states: various cities (2) operators or actions (= transitions between states): drive between cities 3. Step 3 Find solution (= action sequences): sequence of cities, e.g. Arad, Sibiu, Fagaras, Bucharest 4. Step 4 Execute: drive through all the cities given by the solution.
A task is specified by states and actions: 1. state space e.g. other cities 2. initial state e.g. “at Arad” 3. actions or operators (or successor function S(x)) etc. e.g. Arad →  Zerind Arad →  Sibiu 4. goal test, check if a state is goal state In this case, there is only one goal specified (“at Bucharest”) 5. path cost e.g. sum of distances, number of actions etc.
1. Real world is absurdly complex ⇒  state space must be abstracted for problem solving 2. (abstract) state = set of real states 3. (abstract) action = complex combination of real actions (1) e.g. “Arad →  Zerind” represents a complex set of possible routes, detours, rest stops, etc. (2) for guaranteed realizability, any real state “in Arad” must get to some real state “in Zerind" 4. (abstract) solution = set of real paths that are solutions in the real world
1. Toy problems: concise exact description 2. Real world problems: don’t have a single agreed desription
1. states: integer locations of tiles (ignore intermediate positions) 2. operators: move blank left, right, up, down (ignore unjamming etc.) 3. goal test: = goal state (given) 4. path cost: 1 per move
Search: Finding state-action sequences that lead to desirable states. Search is a function: solution search(task) Basic idea: Offline, simulated exploration of state space by generating successors of already-explored states (i.e. “expanding” them)
1. Start with a priority queue consisting of just the initial state. 2. Choose a state from the queue of states which have been generated but not yet expanded. 3. Check if the selected state is a Goal State. If it is, STOP (solution has been found). 4. Otherwise, expand the chosen state by applying all possible transitions and generating all its children. 5. If the queue is empty, Stop (no solution exists). 6. Otherwise, go back to Step 2.
1. Search tree: superimposed over the state space. 2. Root: search node corresponding to the initial state. 3. Leaf nodes: correspond to states that have no successors in the tree because they were not expanded or generated no new nodes. 4. state space is not the same as search tree (1) there are 20 states = 20 cities in the route finding example (2) but there are infinitely many paths!
One possibility is to have a node data structure with five components: 1. Corresponding state 2. Parent node: the node which generated the current node. 3. Operator that was applied to generate the current node. 4. Depth: number of nodes from the root to the current node. 5. Path cost.
a state is (a representation of) a physical configuration a node is a data structure constituting part of a search tree includes parent, children, depth, path cost g(x) States do not have parents, children, depth, or path cost!
Frontier: collection of nodes waiting to be expanded It can be implemented as a priority queue with the following operations: 1. MAKE-QUEUE(ITEMS) creates queue with given items. 2. Boolean EMPTY(QUEUE) returns TRUE if no items in queue. 3. REMOVE-FRONT(QUEUE) removes the item at the front of the queue and returns it. 4. QUEUEING-FUNCTION(ITEMS, QUEUE) inserts new items into the queue.
1. A strategy is defined by picking the order of node expansion 2. Strategies are evaluated along the following dimensions: (1) completeness  does it always find a solution if one exists? (2) time complexity  number of nodes generated/expanded (3) space complexity  maximum number of nodes in memory  optimality (4) does it always find a least-cost solution? 3. Time and space complexity are measured in terms of (1) b - maximum branching factor of the search tree (2) d - depth of the least-cost solution (3) m - maximum depth of the state space (may be ∞ )
How to compare algorithms ? Two approaches: 1. Benchmarking: run both algorithms on a computer and measure speed 2. Analysis of algorithms: mathematical analysis of the algorithm
1. Run two algorithms on a computer and measure speed. 2. Depends on implementation, compiler, computer, data, network ... 3. Measuring time 4. Processor cycles 5. Counting operations 6. Statistical comparison, confidence intervals
Please find in the lecture notes “3a_PathSearch.pdf” P21.
Uninformed (or “blind”) search strategies use only the information available in the problem definition (can only distinguish a goal from a non-goal state): 1. Breadth First Search 2. Uniform Cost Search 3. Depth First Search 4. Depth Limited Search 5. Iterative Deepening Search Strategies are distinguished by the order in which the nodes are expanded.
Informed (or “heuristic”) search strategies use task-specific knowledge. 1. Example of task-specific knowledge: distance between cities on the map. 2. Informed search is more efficient than Uninformed search. 3. Uninformed search systematically generates new states and tests them against the goal.
1. All nodes are expanded at a given depth in the tree before any nodes at the next level are expanded 2. Expand root first, then all nodes generated by root, then All nodes generated by those nodes, etc. 3. Expand shallowest unexpanded node 4. implementation: QUEUEING-FUNCTION = put newly generated successors at end of queue 5. Very systematic 6. Finds the shallowest goal first
1. Complete? Yes (if b is finite the shallowest goal is at a fixed depth d and will be found before any deeper nodes are generated) 2. Time: 1 + b + b^2 + b^3 + . . . + b^d = b^(d+11)/ b-1 = O(b^d) 3. Space: O(b^d) (keeps every node in memory; generate all nodes up to level d) 4. Optimal? Yes, but only if all actions have the same cost Space is the big problem for Breadth-First Search; it grows exponentially with depth!
1. Expand root first, then expand least-cost unexpanded node 2. Implementation: QUEUEINGFUNCTION = insert nodes in order of increasing path cost. 3. Reduces to Breadth First Search when all actions have same cost 4. Finds the cheapest goal provided path cost is monotonically increasing along each path (i.e. no negative-cost steps)
Please find in the lecture notes “3a_PathSearch.pdf” P30.
1. Expands one of the nodes at the deepest level of the tree 2. Implementation: (1) QUEUEINGFUNCTION = insert newly generated states at the front of the queue (thus making it a stack) (2) can alternatively be implemented by recursive function calls
1. Complete? No! fails in infinite-depth spaces, spaces with loops; modify to avoid repeated states along path  complete in nite spaces 2. Time: O(b^m) (terrible if m is much larger than d but if solutions are dense, may be much faster than breadth-first) 3. Space: O(bm), i.e. linear space! 4. Optimal? No, can find suboptimal solutions first.
Expands nodes like Depth First Search but imposes a cutoff on the maximum depth of path. 1. Complete? Yes (no infinite loops anymore) 2. Time: O(b^k), where k is the depth limit 3. Space: O(bk), i.e. linear space similar to DFS 4. Optimal? No, can find suboptimal solutions first Problem: How to pick a good limit ?
1. Tries to combine the benefits of depth-first (low memory) and breadth-first (optimal and complete) by doing a series of depth- limited searches to depth 1, 2, 3, etc. 2. Early states will be expanded multiple times, but that might not matter too much because most of the nodes are near the leaves.
1. Complete? Yes. 2. Time: nodes at the bottom level are expanded once, nodes at the next level twice, and so on:  depth-limited: 1 + b^1 + b^2 + . . . + b^(d-1) + b^d = O(b^d)  iterative deepening: (d + 1)b^0 + db^1 + (d - 1)b^2 + . . . + 2 * b^(d-1) + 1 * b^d = O(b^d) (We assume b > 1)
1. Complete? Yes. 2. Time: nodes at the bottom level are expanded once, nodes at the next level twice, and so on:  depth-limited: 1 + b^1 + b^2 + . . . + b^(d-1) + b^d = O(b^d)  iterative deepening: (d + 1)b^0 + db^1 + (d - 1)b^2 + . . . + 2 * b^(d-1) + 1 * b^d = O(b^d)  example b = 10, d = 5 :  depth-limited: 1 + 10 + 100 + 1, 000 + 10, 000 + 100, 000 = 111,111  iterative-deepening: 6 + 50 + 400 + 3, 000 + 20, 000 + 100, 000 = 123,456  only about 11% more nodes (for b = 10).
1. Complete? Yes 2. Time: O(b^d) 3. Space: O(bd) 4. Optimal? Yes, if step costs are identical.
1. Idea: Search both forward from the initial state and backward from the goal, and stop when the two searches meet in the middle. 2. We need an efcient way to check if a new node already appears in the other half of the search. The complexity analysis assumes this can be done in constant time, using a Hash Table. 3. Assume branching factor = b in both directions and that there is a solution at depth = d. Then bidirectional search nds a solution in O(2b^(d/2)) = O(b^(d/2)) time steps.
1. searching backwards means generating predecessors starting from the goal, which may be difficult 2. there can be several goals  e.g. chekmate positions in chess 3. space complexity: O(b^(d/2)) because the nodes of at least one half must be kept in memory.
1. problem formulation usually requires abstracting away real-world details to dene a state space that can feasibly be explored. 2. variety of Uninformed search strategies 3. Iterative Deepening Search uses only linear space and not much more time than other Uninformed algorithms.
Please find this part in lecture notes “3a_PathSearch.pdf” P45.
General Search algorithm: 1. add initial state to queue 2. repeat: (1) take node from front of queue (2) test if it is a goal state; if so, terminate (3) “expand" it, i.e. generate successor nodes and add them to the queue Search strategies are distinguished by the order in which new nodes are added to the queue of nodes awaiting expansion.
1. BFS and DFS treat all new nodes the same way: (1) BFS add all new nodes to the back of the queue (2) DFS add all new nodes to the front of the queue 2. (Seemingly) Best First Search uses an evaluation function f () to order the nodes in the queue; we have seen one example of this:  UCS f (n) = cost g(n) of path from root to node n 3. Informed or Heuristic search strategies incorporate into f () an estimate of distance to goal (1) Greedy Search f (n) = estimate h(n) of cost from node n to goal (2) A*Search f (n) = g(n) + h(n)
There is a whole family of Best First Search algorithms with different evaluation functions f (). A key component of these algorithms is a heuristic function: 1. Heuristic function h: {Set of nodes} −→ R : (1) h(n) = estimated cost of the cheapest path from current node n to goal node. (2) in the area of search, heuristic functions are problem specific functions that provide an estimate of solution cost.
1. Greedy Best-First Search: Best-First Search that selects the next node for expansion using the heuristic function for its evaluation function, i.e. f (n) = h(n) 2. h(n) = 0  n is a goal state 3. i.e. greedy search minimises the estimated cost to the goal; it expands whichever node n is estimated to be closest to the goal. 4. Greedy: tries to “bite off” as big a chunk of the solution as possible, without worrying about long-term consequences.
1. hSLD(n) = straight-line distance between n and the goal location (Bucharest). 2. Assume that roads typically tend to approximate the direct connection between two cities. 3. Need to know the map coordinates of the cities: sqrt( p(Sibiux - Bucharestx)^2 + (Sibiuy - Bucharesty)^2)
Try 1. Iasi to Fagaras 2. Fagaras to Iasi 3. Rimnicu Vilcea to Lugoj
1. Complete: No! can get stuck in loops, e.g., Iasi → Neamt → Iasi → Neamt  ... Complete in nite space with repeated-state checking 2. Time: O(b^m), where m is the maximum depth in search space. 3. Space: O(b^m) (retains all nodes in memory) 4. Optimal: No! e.g., the path Sibiu → Fagaras → Bucharest is 32 km longer than Sibiu → Rimnicu Vilcea → Pitesti → Bucharest. Therefore Greedy Search has the same deficits as Depth-First Search. However, a good heuristic can reduce time and memory costs substantially.
1. Expand root first, then expand least-cost unexpanded node 2. Implementation: QUEUEINGFN = insert nodes in order of increasing path cost. 3. Reduces to breadth-first search when all actions have same cost 4. Finds the cheapest goal provided path cost is monotonically increasing along each path (i.e. no negative-cost steps)
1. Complete? Yes, if b is nite and step costs   with ε  > 0. 2. Optimal? Yes. 3. Guaranteed to find optimal solution, but does so by exhaustively expanding all nodes closer to the initial state than the goal. Q: can we still guarantee optimality but search more efficiently, by giving priority to more “promising” nodes?
1. A* Search uses evaluation function f (n) = g(n) + h(n) (1) g(n) = cost from initial node to node n (2) h(n) = estimated cost of cheapest path from n to goal (3) f (n) = estimated total cost of cheapest solution through node n 2. Greedy Search minimizes h(n)  efficient but not optimal or complete 3. Uniform Cost Search minimizes g(n)  optimal and complete but not efficient
1. A* Search minimizes f (n) = g(n) + h(n)  idea: preserve efficiency of Greedy Search but avoid expanding paths that are already expensive 2. Q: is A Search optimal and complete ? 3. A: Yes! provided h() is admissible in the sense that it never overestimates the cost to reach the goal.
Please find this part in lecture notes “Heuristic” P23.
Suppose a suboptimal goal node G2 has been generated and is in the queue. Let n be the last unexpanded node on a shortest path to an optimal goal node G
Since f (G2) > f (n), A* will never select G2 for expansion. Note: suboptimal goal node G2 may be generated, but it will never be expanded. In other words, even after a goal node has been generated, A* will keep searching so long as there is a possibility of finding a shorter solution. Once a goal node is selected for expansion, we know it must be optimal, so we can terminate the search.
1. Complete: Yes, unless there are infinitely many nodes with f  cost of solution. 2. Time: Exponential in [relative error in h length of solution] 3. Space: Keeps all nodes is memory 4. Optimal: Yes (assuming h() is admissible).
1. Iterative Deepening A* is a low-memory variant of A* which performs a series of depth-first searches, but cuts off each search when the sum f () = g() + h() exceeds some pre-defined threshold. 2. The threshold is steadily increased with each successive search. 3. IDA* is asymptotically as efficient as A* for domains where the number of states grows exponentially.
What sort of search will greedy search emulate if we run it with: 1. h(n) = -g(n) ? 2. h(n) = g(n) ? 3. h(n) = number of steps from initial state to node n ?
Please find this part in lecture notes “Heuristic” P29.
1. if h2(n) > h1(n) for all n (both admissible) then h2 dominates h1 and is better for search. So the aim is to make the heuristic h() as large as possible, but without exceeding h(). 2. typical search costs:Please find this part in the lecture notes “Heuristic” P31.
1. Admissible heuristics can often be derived from the exact solution cost of a simplified or relaxed version of the problem. (i.e. with some of the constraints weakened or removed) (1) If the rules of the 8-puzzle are relaxed so that a tile can move anywhere, then h1(n) gives the shortest solution. (2) If the rules are relaxed so that a tile can move to any adjacent square, then h2(n) gives the shortest solution.
1. Let h1, h2, ..., hm be admissible heuristics for a given task. 2. Dene the composite heuristic h(n) = max(h1(n), h2(n), ..., hm(n)) 3. h is admissible 4. h dominates h1, h2, ..., hm
1. 3D Manhattan distance, but to be admissible need to divide by 8. 2. better to take 3D Manhattan distance for edges only, divided by 4. 3. alternatively, max of 3D Manhattan distance for edges and corners, divided by 4 (but the corners slow down the computation without much additional benefit). 4. best approach is to pre-compute Pattern Databases which store the minimum number of moves for every combination of the 8 corners, and for two sets of 6 edges. 5. to save memory, use IDA*. “Finding Optimal Solutions to Rubiks Cube using Pattern Databases (Korf, 1997)”
1. Heuristics can be applied to reduce search cost. 2. Greedy Search tries to minimize cost from current node n to the goal. 3. A* combines the advantages of Uniform-Cost Search and Greedy Search. 4. A* is complete, optimal and optimally efficient among all optimal search algorithms. 5. Memory usage is still a concern for A*. IDA* is a low-memory variant.
1. origins 2. motivation 3. minimax search 4. resource limits and heuristic evaluation 5. - pruning 6. stochastic games 7. partially observable games 8. continuous, embodied games
1. 1769 Wolfgang von Kempelen (Mechanical Turk) 2. 1846 Charles Babbage & Ada Lovelace (tic-tac-toe) 3. 1952 Alan Turing (Chess algorithm) 4. 1959 Arthur Samuel (Checkers) 5. 1961 Donald Michie (MENACE machine learner)
“What shall we do to get rid of Mr. Babbage and his calculating machine?” (Prime Minister Robert Peel, 1842)
“For the machine is not a thinking being, but simply an automation which acts according to the laws imposed upon it.” (Ada Lovelace, 1843)
1. Discrete Games (1) fully observable, deterministic (chess, checkers, go, othello) (2) fully observable, stochastic (backgammon, monopoly) (3) partially observable (bridge, poker, scrabble) 2. Continuous, embodied games : robocup soccer, pool (snooker)
1. Computer considers possible lines of play (Babbage, 1846) 2. Algorithm for perfect play (Zermelo, 1912; Von Neumann, 1944) 3. Finite horizon, approximate evaluation (Zuse, 1945; Wiener, 1948; Shannon, 1950) 4. First chess program (Turing, 1951) 5. Machine learning to improve evaluation accuracy (Samuel, 1952-57) 6. Pruning to allow deeper search (McCarthy, 1956)
1. “Unpredictable” opponent ⇒  solution is a strategy : must respond to every possible opponent reply 2. Time limits ⇒  must rely on approximation : tradeoff between speed and accuracy 3. Games have been a key driver of new techniques in CS and AI
“Elaborate table-lookup procedures, fast sorting and searching procedures, and a variety of new programming tricks were developed…" Samuel’s 1959 paper contains groundbreaking ideas in these areas: 1. hash tables 2. data compression 3. parameter tuning via machine learning
Perfect play for deterministic, perfect-information games Idea: choose move to position with highest minimax value = best achievable payoff against best play
Please find this part in the lecture notes “Games” P15.
The above formulation of Minimax assumes that all nodes are evaluated with respect to a fixed player (e.g. White in Chess). If we instead assume that each node is evaluated with respect to the player whose turn it is to move, we get a simpler formulation known as Negamax.
Please find this part in the lecture notes “Games” P17
1. Complete? 2. Optimal? 3. Time complexity? 4. Space complexity?
For chess, b ≈  35, m ≈  100 for “reasonable” games  exact solution completely infeasible Two ways to make the search feasible: 1. don’t search to final position; use heuristic evaluation at the leaves 2. a-b pruning
1. material  Queen = 9, Rook = 5, Knight = Bishop = 3, Pawn = 1 2. position  some (fractional) score for a particular piece on a particular square 3. interaction  some (fractional) score for one piece attacking another piece, etc. 4. KnightCap used 2000 different features, but evaluation is rapid because very few features are non-zero for any particular board state (e.g. Queen can only be on one of the 64 squares at a time) 5. the value of individual features can be determined by reinforcement learning
Q1: Why would Queen to G5 be a bad move for Black? Q2: How many White replies did you need to consider in answering? Once we have seen one reply scary enough to convince us the move is really bad, we can abandon this move and continue searching elsewhere.
Please find this part in the lecture notes “Games” P22.
Please find this part in the lecture notes “Games” P26.
Please find this part in the lecture notes “Games” P27.
Please find this part in the lecture notes “Games” P28.
Please find this part in the lecture notes “Games” P29.
Deep Blue defeated human world champion Gary Kasparov in a six-game match in 1997. Traditionally, computers played well in the opening (using a database) and in the endgame (by deep search) but humans could beat them in the middle game by opening up the board to increase the branching factor. Kasparov tried this, but because of its speed Deep Blue remained strong. Some experts believe Kasparov should have been able to defeat Deep Blue in 1997 if he hadn’t “lost his nerve”. However, chess programs stronger than Deep Blue are now running on standard PCs and could definitely defeat the strongest humans. Modern chess programs rely on quiescent search, transposition tables and pruning heuristics.
Chinook failed to defeat human world champion Marion Tinsley prior to his death in 1994, but has beaten all subsequent human champions. Chinook used an endgame database defining perfect play for all positions involving 8 or fewer pieces on the board - a total of 443,748,401,247 positions. This database has since been expanded to include all positions with 10 or fewer pieces (38 trillion positions). In 2007, Jonathan Shaeffer released a new version of Chinook and published a proof that it will never lose. His proof method lls out the game tree incrementally, ignoring branches which are likely to be pruned. After many months of computation, it eventually converges to a skeleton of the real (pruned) tree which is comprehensive enough to complete the proof.
The branching factor for Go is greater than 300, and static board evaluation is difficult. Traditional Go programs broke the board into regions and used pattern knowledge to explore each region. Since 2006, new “Monte Carlo” players have been developed using UCB search. A tree is built up stochastically. After a small number of moves, the rest of the game is played out randomly, using fast pattern matching to give preference to urgent moves. In March 2016, AlphaGo defeated the human Go champion Lee Sedol in a 4-1 match. AlphaGo uses MCTS, with deep learning neural networks for move selection and board evaluation. The networks are trained initially on a database of thousands of human championship Go games, and then refined with millions of games of self-play.
In stochastic games, chance introduced by dice, card-shuffling, etc. Expectimax is an adaptation of Minimax which also handles chance nodes. ... ... if node is a chance node return average of values of successor nodes Adaptations of a-b pruning are possible, provided the evaluation is bounded.
Please find this part in the lecture notes “Games” P36.
Move choice is preserved under any monotonic transformation of EVAL. Only the order matters: payoff in deterministic games acts as an ordinal utility function.
Move choice only preserved by positive linear transformation of EVAL Hence EVAL should be proportional to the expected payoff.
Card games are partially observable, because (some of) the opponents’ cards are unknown. This makes the problem very difficult, because some information is known to one player but not to another. Typically we can calculate a probability for each possible deal. Idea: compute the minimax value of each action in each deal, then choose the action with highest expected value over all deals. GIB, a strong and well-known bridge program, approximates this idea by 1) generating 100 deals consistent with bidding information 2) picking the action that wins most tricks on average
Currently best solution uses A*Search, after reverse engineering the world model.
Combines path planning, low-level control, reasoning under uncertainty and (for ghosts) multi-agent coordination.
Low level technical issues 1. undistortion of overhead camera image 2. ball appears “egg-shaped”, need to find centre accurately High level strategy 1. easy to sink current ball 2. more complicated to “set up” for the next ball 3. competition using physical simulator
Machine Educable Noughts And Crosses Engine Donald Michie, 1961
Please find this part in the lecture notes “Games” P47.
1. games are fun to work on! 2. games continue to be a driver of new technology 3. tradeoff between speed and accuracy 4. probabilistic reasoning 5. force us to build "whole systems” - chain is as strong as its weakest link
Tom Standage, 2002. The Mechanical Turk, Penguin Books. Arthur Samuel, 1959. Some studies in machine learning using the game of checkers, IBM Journal on Research and Development, pages 210-229. Chinook: www.cs.ualberta.ca/~ chinook Robocup: www.robocup.org [look for Infinite Mario and Deep Green on youtube]
1. Constraint Satisfaction Problems 2. CSP examples 3. backtracking search and heuristics 4. forward checking and arc consistency 5. local search (1) hill climbing (2) simulated annealing 
Constraint Satisfaction Problems are defined by a set of variables Xi, each with a domain Di of possible values, and a set of constraints C. The aim is to find an assignment of the variables Xi from the domains Di in such a way that none of the constraints C are violated. 
Please find this part in lecture notes “CSP” P3.
Put n queens on an n-by-n chess board so that no two queens are attacking each other. 
Assume one queen in each column. Which row does each one go in? Variables: Q1, Q2, Q3, Q4 Domains: Di = {1, 2, 3, 4} For extra, please find this part in lecture notes “CSP” P6
Please find this part in lecture notes “CSP” P7. 
We can add hidden variables to simplify the constraints. Please find this part in lecture notes “CSP” P9
1. Assignment problems (e.g. who teaches what class) 2. Timetabling problems (e.g. which class is offered when and where?) 3. Hardware configuration 4. Transport scheduling 5. Factory scheduling 
Please find this part in lecture notes “CSP” P12. 
CSPs can be solved by assigning values to variables one by one, in different combinations. Whenever a constraint is violated, we go back to the most recently assigned variable and assign it a new value. This can be achieved by a Depth First Search on a special kind of state space, where states are defined by the values assigned so far: 1. Initial state: the empty assignment. 2. Successor function: assign a value to an unassigned variable that does not conflict with previously assigned values of other variables. (If no legal values remain, the successor function fails.) 3. Goal test: all variables have been assigned a value, and no constraints have been violated. 
Important difference between Path Search Problems and CSP's: 1. Constraint Satisfaction Problems (e.g. n-Queens)  difficult part is knowing the final state  how to get there is easy 2. Path Search Problems (e.g. Rubik's Cube)  knowing the final state is easy  difficult part is how to get there 
The search space for this Depth First Search has certain very specific properties: 1. if there are n variables, every solution will occur at exactly depth n. 2. variable assignments are commutative [WA = red then NT = green] same as [NT = green then WA = red] Backtracking search can solve n-Queens for n = 25 
General-purpose heuristics can give huge gains in speed: 1. which variable should be assigned next? 2. in what order should its values be tried? 3. can we detect inevitable failure early? 
Minimum Remaining Values (MRV): Choose the variable with the fewest legal values. 
Tie-breaker among MRV variables Degree heuristic: Choose the variable with the most constraints on remaining variables. 
Given a variable, choose the least constraining value: the one that rules out the fewest values in the remaining variables (More generally, 3 allowed values would be better than 2, etc.) Combining these heuristics makes 1000 queens feasible. 
Idea: Keep track of remaining legal values for unassigned variables
Idea: Keep track of remaining legal values for unassigned variables Terminate search when any variable has no legal values
Forward checking propagates information from assigned to unassigned variables, but doesn’t provide early detection for all failures: NT and SA cannot both be blue! Constraint propagation repeatedly enforces constraints locally. 
Simplest form of constraint propagation makes each arc consistent X → Y is consistent if for every value x of X there is some allowed y
X →  Y is consistent if for every value x of X there is some allowed y If X loses a value, neighbors of X need to be rechecked. 
X →  Y is consistent if for every value x of X there is some allowed y Arc consistency detects failure earlier than forward checking. For some problems, it can speed up the search enormously. For others, it may slow the search due to computational overheads. 
There is another class of algorithms for solving CSP’s, called “Iterative Improvement” or “Local Search”. These algorithms assign all variables randomly in the beginning (thus violating several constraints), and then change one variable at a time, trying to reduce the number of violations at each step. 
Variable selection: randomly select any conflicted variable 2. Value selection by min-conflicts heuristic  choose value that violates the fewest constraints 
Sometimes, have to go sideways or even backwards in order to make progress towards the actual solution. 
When we are minimizing violated constraints, it makes sense to think of starting at the top of a ridge and climbing down into the valleys. 
1. stochastic hill climbing based on difference between evaluation of previous state (h0) and new state (h1). 2. if h1 < h0, definitely make the change 3. otherwise, make the change with probability e^((h1-h0)/T) where T is a “temperature” parameter. 4. reduces to ordinary hill climbing when T = 0 5. becomes totally random search as T   6. sometimes, we gradually decrease the value of T during the search 
Given random initial state, hill climbing by min-conflicts with random restarts can solve n-queens in almost constant time for arbitrary n with high probability (e.g., n = 10,000,000). In general, randomly-generated CSP's tend to be easy if there are very few or very many constraints. They become extra hard in a narrow range of the ratio R = number of constraints/ number of variables
1. Much interest in CSP’s for real-world applications 2. Backtracking = depth-first search with one variable assigned per node 3. Variable and Value ordering heuristics help significantly 4. Forward Checking helps by detecting inevitable failure early 5. Hill Climbing by min-conflicts often effective in practice 6. Simulated Annealing can help to escape from local optima 7. Which method(s) are best? It varies from one task to another! 
1. Darwinian Evolution 2. Evolutionary Computation 3. Simulated Hockey 4. Evolutionary Robotics
1. Darwin’s theory of Natural Selection was largely inspired by what he observed on a visit to the Galapagos Islands (1) different species of finches from different islands (2) unusual adaptations such as the marine iguana (3) breeding habits of turtles 2. Darwin was influenced by: (1) Charles Lyell’s “Principles of Geology” (2) Thomas Malthus’s “Essay on Population” (3) his grandfather Erasmus Darwin (4) his other grandfather, Josiah Wedgwood
1. human genome consists of 3 billion DNA base pairs 2. each base pair can be one of four nucleotides (1) A (Adenine) (2) G (Guanine) (3) C (Cytosine) (4) T (Thymine) 3. approximately 30,000 “genes”, each coding for a specific protein 4. 97% of genome does not code for proteins (1) once thought to be useless “junk” DNA (2) now thought to serve some other function(s)
1. use principles of natural selection to evolve a computational mechanism which performs well at a specified task. 2. start with randomly initialized population 3. repeated cycles of:  evaluation  selection  reproduction + mutation 4. any computational paradigm can be used, with appropriately dened reproduction and mutation operators
1. Bit Strings (Holland  “Genetic Algorithm”) 2. S-expression trees (Koza - “Genetic Programming”) 3. set of continuous parameters (Swefel - “Evolutionary Strategy”) 4. Lindenmeyer system (e.g. Sims - “Evolving Virtual Creatures”)
1. reproduction = just copying 2. mutation = add random noise to each weight (or parameter), from a Gaussian distribution with specified standard deviation : sometimes, the standard deviation evolves as well
1. rectangular rink with rounded corners 2. near-frictionless playing surface 3. “spring” method of collision handling 4. frictionless puck (never acquires any spin)
a skate at each end of the vehicle with which it can push on the rink in two independent directions
1. 6 Braitenberg-style sensors equally spaced around the vehicle 2. each sensor has an angular range of 90 with an overlap of 30 between neighbouring sensors
1. each of the 6 sensors responds to three different stimuli (1) ball / puck (2) own goal (3) opponent goal 2. 3 additional inputs specify the current velocity of the vehicle 3. total of 3 * 6 + 3 = 21 inputs
1. Perceptron with 21 inputs and 4 outputs 2. total of 4 * (21 + 1) = 88 weights 3. our “genome” (for Evolutionary Computation) consists of a vector of these 88 parameters 4. mutation = add Gaussian random noise to each parameter, with standard deviation 0.05
1. each game begins with a random “game initial condition” (1) random position for puck (2) random position and orientation for player 2. each game ends with (1) +1 if puck - enemy goal (2) -1 if puck - own goal (3) 0 if time limit expires
1. mutant - champ + Gaussian noise 2. champ and mutant play up to 5 games with same game initial conditions 3. if mutant does better than 4. “better” means the mutant must score higher than the champ in the first game, and at least as high as the champ in each subsequent game
Please find this part in lecture notes “Evolution” P18.
1. Evolutionary Strategy with fixed  2. since only  is updated, computation can be distributed across many processors 3. applied to Atari Pong, MuJoCo humanoid walking 4. competitive with Deep Q-Learning on these tasks
1. Aibo walk learning 2. Humanoid walk learning 3. Evolving body as well as controller 4. Simulation to Reality
1. Learning done on actual robot.
1. Learning done in simulator(s), then tested on actual robot.
1. Body evolves as a Lindenmeyer system 2. Controller evolves as a neural network
1. Evolved in simulation, tested in reality.
One example of the use of Evolutionary Algorithms for a real world application is the antenna that was evolved by Hornby et al in 2006 for NASA’s Space Technology 5 (ST5) mission.
1. Logic in general - models and entailment 2. Propositional Logic 3. Equivalence, Validity, Satisfiability 4. Inference Rules and Theorem Proving 5. Resolution and Conjunctive Normal Form 6. Forward and Backward Chaining 7. First Order Logic 8. Universal and Existential Quantifiers 9. Situation Calculus
We previously used a transition table for our World Model, with Planning done by State-Based Search (BFS, DFS, UCS, IDS, Greedy, A*, etc.)
Some environments instead require a Knowledge Base of facts and a set of Logical Inference Rules to reason about those facts.
Facts: “Breeze in Square (1,2)”, “Stench in Square (2,1)” 2. Inference Rules: “If there is a Breeze in Square (1,2) then there is a Wumpus in Square (1,1), (2,2) or (1,3)”. Then try to deduce, for example, whether it is safe to move into Square (2,2).
A Knowledge Base is a set of sentences in a formal language. It takes a Declarative approach to building an agent (or other system): 1. Tell the system what it needs to know, then it can Ask itself what it needs to do 2. Answers should follow from the KB.
1. If there is a Breeze in (1,2) and (2,1), then there are no safe actions. 2. Assuming that pits are uniformly distributed, a pit is more likely in (2,2) than in (3,1). How much more likely?
If there is a Smell in (1,1), there is no safe square to move into. However, we can use logic to reason about future states. 1. Shoot straight ahead 2. Wumpus was there - dead - safe 3. Wumpus wasn’t there - safe
The agent must be able to: 1. represent states, actions, etc. 2. incorporate new percepts 3. update internal representations of the world 4. deduce hidden properties of the world 5. determine appropriate actions
Logics are formal languages for representing information such that conclusions can be drawn. Syntax defines the sentences in the language. Semantics dene the meaning of sentences; i.e. dene truth of a sentence in a world. For example, the language of arithmetic: please find in the lecture notes.
Entailment means that one thing follows from another: KB |=  Knowledge base KB entails sentence  if and only if  is true in all worlds where KB is true. e.g. the KB containing “the Moon is full” and “the tide is high” entails “Either the Moon is full or the tide is high”. e.g. x + y = 4 entails 4 = x + y Entailment is a relationship between sentences (i.e. syntax) that is based on semantics.
Logicians typically think in terms of models, which are formally structured worlds with respect to which truth can be evaluated. We say m is a model of a sentence  if  is true in m M() is the set of all models of, for more, please find in the lecture notes P11.
Situation after detecting nothing in [1,1], moving right, Breeze in [2,1] Consider possible combinations for ?s assuming only pits. 3 Boolean choices - 8 possible combinations.
Please find this part in lecture notes “Logic” P13.
Please find this part in lecture notes “Logic” P17.
Each model species TRUE / FALSE for each proposition symbol. For example, if there are Pits in (1,2) and (2,2) but not (3,1) we would have the following assignments: E.g. P1,2 TRUE P2,2 TRUE P3,1 FALSE (With these symbols, 8 possible models, can be enumerated automatically.)
Please find this part in lecture notes “Logic” P19.
Please find this part in lecture notes “Logic” P20.P- “Fred is served alcohol” Q-“Fred is over 18 years old” P - Q “If Fred is served alcohol, then he must be over 18” Implication is not a causal relationship, but a rule that needs to be checked.
Please find this part in lecture notes “Logic” P21.
Please find this part in lecture notes “Logic” P23.
Please find this part in lecture notes “Logic” P24.
Please find this part in lecture notes “Logic” P25.
Please find this part in lecture notes “Logic” P26.
Please find this part in lecture notes “Logic” P27.
In order to apply Resolution, we must rst convert the KB into Conjunctive Normal Form (CNF). This means that the KB is a conjunction of clauses, and each clause is a disjunction of (possibly negated) literals
Please find this part in lecture notes “Logic” P29.
Please find this part in lecture notes “Logic” P30.
Please find this part in lecture notes “Logic” P31.
Resolution provides us an alternative proof method which is generally somewhat faster than Truth Table Enumeration: 1. convert the into Conjunctive Normal Form, 2. add the negative of the clause you are trying to prove, 3. continually apply a series of resolutions until either (a) you derive the empty clause, or (b) no more pairs of clauses to which resolution can be applied
Model Checking can be done more efficiently if the clauses in the all happen to be in a special form for example they may all be Horn Clauses. Each Horn Clause is an implication involving only positive literals, in the form: Please find this part in lecture notes “Logic” P33. Efficient Proof Methods, using Horn Clauses, can generally be divided into Forward Chaining and Backward Chaining.
Look for a rule such that all the clauses on the left hand side are already in the KB. Apply this rule, and add q to the KB. Repeat this process until the goal clause  has been derived (or we run out of rules to apply).
Backward Chaining instead maintains a list of subgoals that it is trying to prove. Initially, this list consists of the ultimate goal . Choose a clause q from the list of subgoals. 1. check if q is known already 2. otherwise, find a rule with q on the right side and add clauses from the left side of this rule as new subgoals 3. check to make sure each new subgoal is not on the list already, and has not already been proved, or failed
Forward Chaining is data-driven automatic, unconscious processing e.g. object recognition, routine decisions 1. May do lots of work that is irrelevant to the goal Backward Chaining is goal-driven, appropriate for problem-solving 2. e.g. Where are my keys? How do I get into a PhD program?
Suppose you are given a KB written in 3-CNF. (This means Conjunctive Normal Form, with at most three literals in each clause.) Does there exist any assignment of truth values to the symbols which will make all of the clauses in the KB TRUE? For example, is there an assignment of truth values to A, B,C, D, E which will make the following TRUE? Please find this part in lecture notes “Logic” P37. This provides a classic example of a Constraint Satisfaction Problem, to which methods such as Hill Climbing or Simulated Annealing can be applied.
Difficulty of finding a solution depends on the ratio (m/n) where m is the number of clauses and n is the number of distinct symbols. Suppose n = 50 and the KB is in 3-CNF. Please find this part in lecture notes “Logic” P38. Other CSPs like n-Queens are sometimes converted to 3-CNF, as a way of measuring whether they are under- or over-constrained.
Logical agents apply inference to a knowledge base to derive new information and make decisions. Basic concepts of logic: 1. syntax: formal structure of sentences 2. semantics: truth of sentences wrt models 3. entailment: necessary truth of one sentence given another 4. inference: deriving sentences from other sentences 5. soundness: derivations produce only entailed sentences 6. completeness: derivations can produce all entailed sentences
Please find this part in lecture notes “Logic” P40.
1. Objects: people, houses, numbers, theories, colors, football games, wars, centuries . . . 2. Predicates: red, round, bogus, prime, multistoried, . . . brother of, bigger than, inside, part of, has color, occurred after, owns, comes between, . . . 3. Functions: father of, best friend, third inning of, one more than, . . .
Please find this part in lecture notes “Logic” P42. 
Please find this part in lecture notes “Logic” P43.
Please find this part in lecture notes “Logic” P44.
Please find this part in lecture notes “Logic” P45.
Please find this part in lecture notes “Logic” P46.
Please find this part in lecture notes “Logic” P47.
Please find this part in lecture notes “Logic” P48.
Please find this part in lecture notes “Logic” P49.
Brothers are siblings “Sibling” is symmetric One’s mother is one’s female parent A first cousin is a child of a parent's sibling
Please find this part in lecture notes “Logic” P51.
Please find this part in lecture notes “Logic” P52.
Facts hold only in certain situations, not universally. e.g. Holding(Gold, Now) rather than just Holding(Gold) Situation calculus is one way to represent change: 1. Adds a situation argument to each non-eternal predicate 2. e.g. Now denotes a situation in Holding(Gold, Now) Situations are connected by the Result function Result(a, s) is the situation that results from doing action a is state s
We can plan a series of actions in a logical domain in a manner analogous to the Path Search algorithms discussed in Weeks 3 & 4. But, instead of the successor state being explicitly specified, we instead need to deduce what will be true and false in the state resulting from the previous state and action: Effect axiom describe changes due to action 
Frame problem: Some facts will change as a result of an action, but many more will stay as they were. However, adding too many of these frame axioms can make the process unmanageable. For example, if a cup is red, and you turn it upside down, it is still red. But, if a cup is full of water, and you turn it upside down, it is no longer full of water. Large-scale expert systems of the 1980’s often failed because of their inability to encode this kind of “commonsense” reasoning in explicit rules.
Qualification problem: Normally, we expect actions to have a certain effect. But, in the real world there could be endless caveats. What happens if the gold is slippery, or nailed down, or too heavy, or you cant reach it, etc. Ramification problem: Real actions have many secondary consequences  what about the dust on the gold, wear and tear on gloves, shoes, etc.. In general, we assume that a fact is true if a rule tells us that an action made it true, or if it was true before and no action made it false.
Initial condition in KB (knowledge base): At(Agent, [1, 1], S0) At(Gold, [1, 2], S0) Query: Ask(KB, s Holding(Gold, s)) i.e., in what situation will I be holding the gold? Answer: s = Result(Grab, Result(Forward, S0)) i.e., go forward and then grab the gold This assumes that the agent is interested in plans starting at S0 and that S0 is the only situation described in the KB.
Please find this part in lecture notes “Logic” P58.
1. First Order Logic:  objects and relations are semantic primitives  syntax: constants, functions, predicates, equality, quantifiers 2. Increased expressive power: sufficient to dene Wumpus World 3. Situation calculus: (1) conventions for describing actions and change (2) can formulate planning as inference on a knowledge base
Please find this part in lecture notes “Learning” P1.
1. Supervised Learning : agent is presented with examples of inputs and their target outputs 2. Reinforcement Learning : agent is not presented with target outputs, but is given a reward signal, which it aims to maximize 3. Unsupervised Learning : agent is only presented with the inputs themselves, and aims to find structure in these inputs 
1. we have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specified. 2. the aim is to predict the target value, based on the input attributes. 3. agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set 4. various learning paradigms are available: (1) Decision Tree (2) Neural Network (3) Support Vector Machine, etc. 
1. framework (decision tree, neural network, SVM, etc.) 2. representation (of inputs and outputs) 3. pre-processing / post-processing 4. training method (perceptron learning, backpropagation, etc.) 5. generalization (avoid over-fitting) 6. evaluation (separate training and testing sets) 
Please find this part in lecture notes “Learning” P5.
“The most likely hypothesis is the simplest one consistent with the data.” Since there can be noise in the measurements, in practice need to make a tradeoff between simplicity of the hypothesis and how well it fits the data. 
Please find this part in lecture notes “Learning” P11.
Please find this part in lecture notes “Learning” P13.
Please find this part in lecture notes “Learning” P14.
1. Provided the training data are not inconsistent, we can split the attributes in any order and still produce a tree that correctly classifies all examples in the training set. 2. However, we really want a tree which is likely to generalize to correctly classify the (unseen) examples in the test set. 3. In view of Ochkams Razor, we prefer a simpler hypothesis, i.e. a smaller tree. 4. But how can we choose attributes in order to produce a small tree? 
Patrons is a “more informative” attribute than Type, because it splits the examples more nearly into sets that are “all positive” or “all negative”. This notion of “informativeness” can be quantified using the mathematical concept of “entropy”. A parsimonious tree can be built by minimizing the entropy at each step. 
Entropy is a measure of how much information we gain when the target attribute is revealed to us. In other words, it is not a measure of how much we know, but of how much we don’t know. If the prior probabilities of the n target attribute values are p1, . . . , pn then the entropy is Please find this part in lecture notes “Learning” P17.
Entropy is the number of bits per symbol achieved by a (block) Huffman Coding scheme. Example 1: H(<0.5, 0.5>) = 1 bit. Suppose we want to encode, in zeroes and ones, a long message composed of the two letters A and B, which occur with equal frequency. This can be done efficiently by assigning A=0, B=1. In other words, one bit is needed to encode each letter. 
Example 2: H(<0.5, 0.25, 0.25>) = 1.5 bits. Suppose we need to encode a message consisting of the letters A, B and C, and that B and C occur equally often but A occurs twice as often as the other two letters. In this case, the most efficient code would be A=0, B=10, C=11. The average number of bits needed to encode each letter is 1.5 . If the letters occur in some other proportion, we would need to “block” them together in order to encode them efficiently. But, the average number of bits required by the most efficient coding scheme is given by Please find this part in lecture notes “Learning” P19.
Please find this part in lecture notes “Learning” P20.
Please find this part in lecture notes “Learning” P21.
Please find this part in lecture notes “Learning” P22.
According to Ockham’s Razor, we may wish to prune off branches that do not provide much benefit in classifying the items. When a node becomes a leaf, all items will be assigned to the majority class at that node. We can estimate the error rate on the (unseen) test items using the Laplace error: E = 1 -( n + 1)/( N + k ) N = total number of (training) items at the node n = number of (training) items in the majority class k = number of classes If the average Laplace error of the children exceeds that of the parent node, we prune off the children. 
Should the children of this node be pruned or not? Left child has class frequencies [7,3] E = 1 -( n + 1)/( N + k )= 1 - (7 + 1)/( 10 + 2 )= 0.333 Right child has E = 0.429 Parent node has E = 0.412 Average for Left and Right child is E = 10/ 15 (0.333) + 5/ 15 (0.429) = 0.365 Since 0.365 < 0.412, children should NOT be pruned. 
Should the children of this node be pruned or not? Left child has class frequencies [3,2] E = 1 - (n + 1)/( N + k )= 1 -( 3 + 1 )/(5 + 2 )= 0.429 Right child has E = 0.333 Parent node has E = 0.375 Average for Left and Right child is E = 5 /6 (0.429) + 1/ 6 (0.333) = 0.413 Since 0.413 > 0.375, children should be pruned. 
Should the children of this node be pruned or not? Left and Middle child have class frequencies [15,1] E = 1 - (n + 1)/ (N + k )= 1 - (15 + 1)/( 16 + 2) = 0.111 Right child has E = 0.333 Parent node has E = 4 Average for Left, Middle and Right child is 35 = 0.114 E = 16/ 33 (0.111)+ 16/ 33 (0.111)+ 1/ 33 (0.333) = 0.118 Since 0.118 > 0.114, children should be pruned. 
1. Supervised Learning (1) training set and test set (2) try to predict target value, based on input attributes 2. Ockham's Razor  tradeoff between simplicity and accuracy 3. Decision Trees (1) improve generalisation by building a smaller tree (using entropy) (2) prune nodes based on Laplace error 
1. Neurons - Biological and Artificial 2. Perceptron Learning 3. Linear Separability 4. Multi-Layer Networks
The brain is made up of neurons (nerve cells) which have 1. a cell body (soma) 2. dendrites (inputs) 3. an axon (outputs) 4. synapses (connections between cells) Synapses can be exitatory or inhibitory and may change over time. When the inputs reach some threshhold an action potential (electrical pulse) is sent along the axon to the outputs.
1. human brain has 100 billion neurons with an average of 10, 000 synapses each 2. latency is about 3-6 milliseconds 3. therefore, at most a few hundred “steps” in any mental computation, but massively parallel
(Artificial) Neural Networks are made up of nodes which have 1. inputs edges, each with some weight 2. outputs edges (with weights) 3. an activation level (a function of the inputs) Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: Some nodes are inputs (sensing), some are outputs (action)
Please find this part in lecture notes “Perceptrons” P10.
Originally, a (discontinuous) step function was used for the transfer function:(Later, other transfer functions were introduced, which are continuous and smooth)
Q: what kind of functions can a perceptron compute? A: linearly separable functions Examples include: AND w1 = w2 = 1.0, w0 = -1.5 OR w1 = w2 = 1.0, w0 = -0.5 NOR w1 = w2 = 1.0, w0 = 0.5 Q: How can we train it to learn a new function?
Please find this part in lecture notes “Perceptrons” P13.
Please find this part in lecture notes “Perceptrons” P14.
Please find this part in lecture notes “Perceptrons” P15.
Please find this part in lecture notes “Perceptrons” P16.
Please find this part in lecture notes “Perceptrons” P17.
Please find this part in lecture notes “Perceptrons” P18.
Please find this part in lecture notes “Perceptrons” P19.
Please find this part in lecture notes “Perceptrons” P20.
1. Linear Separability 2. Multi-Layer Networks 3. Backpropagation 4. Application - ALVINN 5. Training Tips
(Artificial) Neural Networks are made up of nodes which have 1. inputs edges, each with some weight 2. outputs edges (with weights) 3. an activation level (a function of the inputs) Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: Some nodes are inputs (sensing), some are outputs (action)
Please find this part in lecture notes “NeuralNets” P3.
Originally, a (discontinuous) step function was used for the transfer function: Please find this part in lecture notes “NeuralNets” P4.
Q: what kind of functions can a perceptron compute? A: linearly separable functions Examples include: AND w1 = w2 = 1.0, w0 = -1.5 OR w1 = w2 = 1.0, w0 = -0.5 NOR w1 = w2 = 1.0, w0 = 0.5 Q: How can we train it to learn a new function?
Please find this part in lecture notes “NeuralNets” P6.
Please find this part in lecture notes “NeuralNets” P7.
Please find this part in lecture notes “NeuralNets” P8.
In 1969, Minsky and Papert published a book highlighting the limitations of Perceptrons, and lobbied various funding agencies to redirect funding away from neural network research, preferring instead logic-based methods such as expert systems. It was known as far back as the 1960’s that any given logical function could be implemented in a 2-layer neural network with step function activations. But, the the question of how to learn the weights of a multi-layer neural network based on training examples remained an open problem. The solution, which we describe in the next section, was found in 1976 by Paul Werbos, but did not become widely known until it was rediscovered in 1986 by Rumelhart, Hinton and Williams.
We define an error function E to be (half) the sum over all input patterns of the square of the difference between actual output and desired output E = 1 2 (z - t)^2 If we think of E as height, it defines an error landscape on the weight space. The aim is to find a set of weights for which E is very low.
Problem: because of the step function, the landscape will not be smooth but will instead consist almost entirely of at local regions and “shoulders”, with occasional discontinuous jumps.
Please find this part in lecture notes “NeuralNets” P12.
Recall that the error function E is (half) the sum over all input patterns of the square of the difference between actual output and desired output E = 1 2 (z - t)^2 The aim is to find a set of weights for which E is very low. If the functions involved are smooth, we can use multi-variable calculus to adjust the weights in such a way as to take us in the steepest downhill direction. Parameter  is called the learning rate.
Please find this part in lecture notes “NeuralNets” P14.
Please find this part in lecture notes “NeuralNets” P15.
Please find this part in lecture notes “NeuralNets” P16.
1. Autonomous Driving 2. Game Playing 3. Credit Card Fraud Detection 4. Handwriting Recognition 5. Financial Prediction
1. Autonomous Land Vehicle In a Neural Network 2. later version included a sonar range finder (1) 8 * 32 range finder input retina (2) 29 hidden units (3) 45 output units 3. Supervised Learning, from human actions (Behavioral Cloning)  additional “transformed” training items to cover emergency situations 4. drove autonomously from coast to coast
1. re-scale inputs and outputs to be in the range 0 to 1 or -1 to 1 2. initialize weights to very small random values 3. on-line or batch learning 4. three different ways to prevent overfitting: (1) limit the number of hidden nodes or connections (2) limit the training time, using a validation set (3) weight decay 5. adjust learning rate (and momentum) to suit the particular task
1. Reinforcement Learning vs. Supervised Learning 2. Models of Optimality 3. Exploration vs. Exploitation 4. Temporal Difference learning 5. Q-Learning
Recall: Supervised Learning 1. We have a training set and a test set, each consisting of a set of examples. For each example, a number of input attributes and a target attribute are specified. 2. The aim is to predict the target attribute, based on the input attributes. 3. Various learning paradigms are available: (1) Decision Trees (2) Neural Networks (3) .. others ..
Please find this part in lecture notes “Reinforcement” P4.
Please find this part in lecture notes “Reinforcement” P5.
Supervised Learning can also be used to learn Actions, if we construct a training set of situation-action pairs (called Behavioral Cloning). However, there are many applications for which it is difficult, inappropriate, or even impossible to provide a “training set” 1. optimal control  mobile robots, pole balancing, flying a helicopter 2. resource allocation  job shop scheduling, mobile phone channel allocation 3. mix of allocation and control  elevator control, backgammon
Please find this part in lecture notes “Reinforcement” P7.
Please find this part in lecture notes “Reinforcement” P8.
Please find this part in lecture notes “Reinforcement” P9.
For each state s  S, let V (s) be the maximum discounted reward obtainable from s. Learning this Value Function can help to determine the optimal strategy.
Environments can be: 1. passive and stochastic (as in previous slide) 2. active and deterministic (chess) 3. active and stochastic (backgammon)
The special case of an active, stochastic environment with only one state is called the K-armed Bandit Problem, because it is like being in a room with several (friendly) slot machines, for a limited time, and trying to collect as much money as possible. Each action (slot machine) provides a different average reward.
Most of the time we should choose what we think is the best action. However, in order to ensure convergence to the optimal strategy, we must occasionally choose something different from our preferred action, e.g. 1. choose a random action 5% of the time, or 2. use a Bolzmann distribution to choose the next action: Please find this part in lecture notes “Reinforcement” P14.
I was born to try... But you’ve got to make choices Be wrong or right Sometimes you’ve got to sacrifice the things you like. - Delta Goodrem
Please find this part in lecture notes “Reinforcement” P17.
Please find this part in lecture notes “Reinforcement” P18.
Theorem: Q-learning will eventually converge to the optimal policy, for any deterministic Markov decision process, assuming an appropriately randomized strategy. (Watkins & Dayan 1992) Theorem: TD-learning will also converge, with probability 1. (Sutton 1988, Dayan 1992, Dayan & Sejnowski 1994)
1. Delayed reinforcement  reward resulting from an action may not be received until several time steps later, which also slows down the learning 2. Search space must be nite (1) convergence is slow if the search space is large (2) relies on visiting every state infinitely often 3. For “real world” problems, we can’t rely on a lookup table : need to have some kind of generalisation (e.g. TD-Gammon)
1. Image Processing : Convolutional Networks 2. Language Processing :(1) Recurrent Networks (2) Long Short Term Memory (3) Word Embeddings 3. Deep Reinforcement Learning (1) Deep Q-Learning (2) Policy Gradients (3) Asynchronous Advantage Actor Critic 
1. image classification 2. object detection 3. object segmentation 4. style transfer 5. generating images 6. generating art 
Some functions cannot be learned with a 2-layer sigmoidal network. For example, this Twin Spirals problem cannot be learned with a 2-layer network, but it can be learned using a 3-layer network if we include shortcut connections between non-consecutive layers. 
1. black and white, resolution 28  28 2. 60,000 images 3. 10 classes (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) 
1. color, resolution 32 * 32 2. 50,000 images 3. 10 classes 
1. color, resolution 227 * 227 2. 1.2 million images 3. 1000 classes 
Training by backpropagation in networks with many layers is difficult. When the weights are small, the differentials become smaller and smaller as we backpropagate through the layers, and end up having no effect. When the weights are large, the activations in the higher layers will saturate to extreme values. As a result, the gradients at those layers will become very small, and will not be propagated to the earlier layers. When the weights have intermediate values, the differentials will sometimes get multiplied many times is places where the transfer function is steep, causing them to blow up to large values. 
Please find this part in lecture notes “DeepLearning” P9.
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s 
Suppose we want to classify an image as a bird, sunset, dog, cat, etc. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image. 
Please find this part in lecture notes “DeepLearning” P12.
Please find this part in lecture notes “DeepLearning” P13.
The 5 * 5 window of the first convolution layer extracts from the original 32 * 32 image a 28 * 28 array of features. Subsampling then halves this size to 14  *14. The second Convolution layer uses another 5 * 5 window to extract a 10  *10 array of features, which the second subsampling layer reduces to 5 * 5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits ‘0’ to ‘9’. 
1. LeNet, 5 layers (1998) 2. AlexNet, 8 layers (2012) 3. VGG, 19 layers (2014) 4. GoogleNet, 22 layers (2014) 5. ResNets, 152 layers (2015) 6. DenseNets, 160 layers (2017) 
1. 650K neurons 2. 630M connections 3. 60M parameters 4. more parameters that images - danger of overfitting 
1. Rectified Linear Units (ReLU’s) 2. overlapping pooling (width = 3, stride = 2) 3. stochastic gradient descent with momentum and weight decay 4. data augmentation to reduce overfitting 5. 50% dropout in the fully connected layers 
Nodes are randomly chosen to not be used, with some fixed probability (usually, one half). 
Idea: Take any two consecutive stacked layers in a deep network and add a skip connection which bipasses these layers and is added to their output. 
Recently, good results have been achieved using networks with densely connected blocks, within which each layer is connected by shortcut connections to all the preceding layers. 
There are many tasks which require a sequence of inputs to be processed rather than a single input. 1. speech recognition 2. time series prediction 3. machine translation 4. handwriting recognition 5. image captioning How can neural network models be adapted for these tasks? 
1. at each time step, hidden layer activations are copied to “context” layer 2. hidden layer receives connections from input and context layers 3. the inputs are fed one at a time to the network, it uses the context layer to “remember” whatever information is required for it to produce the correct output 
1. we can “unroll” a recurrent architecture into an equivalent feedforward architecture, with shared weights 2. applying backpropagation to the unrolled architecture is reffered to as “backpropagation through time” 3. we can backpropagate just one timestep, or a fixed number of timesteps, or all the way back to beginning of the sequence 
1. SRN with 3 hidden units can learn to predict a^nb^nc^n by counting up and down simultaneously in different directions, thus producing a star shape. 
1. Simple Recurrent Networks (SRN’s) can learn medium-range dependencies but have difficulty learning long range dependencies 2. Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) can learn long range dependencies better than SRN 
LSTM - context layer is modulated by three gating mechanisms: forget gate, input gate and output gate. http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 
Synonyms for “elegant” stylish, graceful, tasteful, discerning, refined, sophisticated, dignified, cultivated, distinguished, classic, smart, fashionable, modish, decorous, beautiful, artistic, aesthetic, lovely; charming, polished, suave, urbane, cultured, dashing, debonair; luxurious, sumptuous, opulent, grand, plush, high-class, exquisite Synonyms, antonyms and taxonomy require human effort, may be incomplete and require discrete choices. Nuances are lost. Words like “king”, “queen” can be similar in some attributes but opposite in others. Could we instead extract some statistical properties automatically, without human involvement? 
Please find this part in lecture notes “DeepLearning” P32.
Please find this part in lecture notes “DeepLearning” P33.
1. An agent interacts with its environment. 2. There is a set S of states and a set A of actions. 3. At each time step t, the agent is in some state st . It must choose an action at , whereupon it goes into state st+1 = (st , at) and receives reward rt = R (st , at) 4. Agent has a policy  : S  A. We aim to find an optimal policy  which maximizes the cumulative reward. 5. In general, , R and  can be multi-valued, with a random element, in which case we write them as probability distributions (st+1 = s | st , at ) R (rt = r | st , at ) (at = a | st ) 
Please find this part in lecture notes “DeepLearning” P39.
1. end-to-end learning of values Q(s, a) from pixels s 2. input state s is stack of raw pixels from last 4 frames : 8-bit RGB images, 210 * 160 pixels 3. output is Q(s, a) for 18 joystick/button positions 4. reward is change in score for that timestep 
Please find this part in lecture notes “DeepLearning” P42.
1. Hopfield Networks 2. Restricted Boltzmann Machines 3. Autoencoders 4. Generative Adversarial Networks 
1. Uncertainty 2. Probability 3. Syntax and Semantics 4. Inference 5. Conditional Independence 6. Bayes’ Rule 
In many situations, an AI agent has to choose an action based on incomplete information. 1. stochastic environments (e.g. dice rolls in Backgammon) 2. partial observability (1) some aspects of environment hidden from agent (2) robots can have noisy sensors, reporting quantities which differ from the “true” values 
In this situation no action is completely safe, because the agent does not know the location of the Pit(s). 
Let action At = leave for airport t minutes before flight Will At get me there on time? Problems: 1. partial observability, noisy sensors 2. uncertainty in action outcomes (at tyre, etc.) 3. immense complexity of modelling and predicting traffic Hence a purely logical approach either 1) risks falsehood: “A30 will get me there on time”, or 2) leads to conclusions that are too weak for decision making: “A30 will get me there on time if theres no accident on the bridge and it doesn’t rain and my tires remain intact etc etc”. (A1440 might be safe but I’d have to stay overnight in the airport . . .) 
Default or nonmonotonic logic: Assume my car does not have a at tire, etc. Assume A30 works unless contradicted by evidence Issues: What assumptions are reasonable? How to handle contradiction? Probability Given the available evidence, A30 will get me there on time with probability 0.04 Mahaviracarya (9th C.), Cardamo (1565) theory of gambling 
Probabilistic assertions summarize effects of Laziness: failure to enumerate exceptions, qualifications, etc. Ignorance: lack of relevant facts, initial conditions, etc. Subjective or Bayesian probability: Probabilities relate propositions to one’s own state of knowledge e.g. P(A30|no reported accidents) = 0.06 These are not claims of a “probabilistic tendency” in the current situation (but might be learned from past experience of similar situations) Probabilities of propositions change with new evidence: e.g. P(A30|no reported accidents, 5 a.m.) = 0.15
Suppose I believe the following: P(A30 gets me there on time| . . .) P(A90 gets me there on time| . . .) P(A120 gets me there on time| . . .) P(A1440 gets me there on time| . . .) Which action to choose? = = = = 0.04 0.70 0.95 0.9999 Depends on my preferences for missing flight vs. airport cuisine, etc. Utility theory is used to represent and infer preferences Decision theory = utility theory + probability theory 
Please find this part in lecture notes “Uncertainty” P8.
A random variable (r.v.) is a function from sample points to some range (e.g. the Reals or Booleans) For example, Odd(3) = true. P induces a probability distribution for any r.v. X
Please find this part in lecture notes “Uncertainty” P10.
The definitions imply that certain logically related events must have related probabilities de Finetti (1931): an agent who bets according to probabilities that violate these axioms can be forced to bet so as to lose money regardless of outcome. 
Propositional or Boolean random variables e.g., Cavity (do I have a cavity?) Cavity = true is a proposition, also written Cavity Discrete random variables (finite or infinite) e.g., Weather is one of (sunny, rain, cloudy, snow) Weather = rain is a proposition Values must be exhaustive and mutually exclusive Continuous random variables (bounded or unbounded) e.g. Temp = 21.6; also allow, e.g. Temp < 22.0 Arbitrary Boolean combinations of basic propositions. 
Prior or unconditional probabilities of propositions e.g. P(Cavity = true) = 0.1 and P(Weather = sunny) = 0.72 correspond to belief prior to arrival of any (new) evidence. Probability distribution gives values for all possible assignments: P(Weather) = <0.72, 0.1, 0.08, 0.1> (normalized, i.e., sums to 1) 
Joint probability distribution for a set of r.v.’s gives the probability of every atomic event on those r.vs (i.e., every sample point) P(Weather, Cavity) is a 4 2 matrix of values: Please find this part in lecture notes “Uncertainty” P14. Every question about a domain can be answered by the joint distribution because every event is a sum of sample points. 
Express distribution as a parameterized function. e.g. P(X = x) = U[18, 26](x) = uniform density between 18 and 26 Here P is a density; integrates to 1. P(X = 20.5) = 0.125 really means Please find this part in lecture notes “Uncertainty” P15.
Please find this part in lecture notes “Uncertainty” P16.
We consider an Agent whose World Model consists not of a set of facts, but rather a set of probabilities of certain facts being true, or certain random variables taking particular values. When the Agent makes an observation, it may update its World Model by adjusting these probabilities, based on what it has observed. 
Assume you live in a community where, at any given time, 20% of people have a cavity in one of their teeth which needs a filling from the dentist. P(cavity) = 0.2 If you have a toothache, suddenly you will think it is much more likely that you have a cavity, perhaps as high as 60%. We say that the conditional probability of cavity, given toothache, is 0.6, written as follows: P(cavity| toothache) = 0.6 If you go to the dentist, they will use a small hook-shaped instrument called a probe, and check whether this probe can catch on the back of your tooth. If it does catch, this information will increase the probability that you have a cavity. 
We assume there is some underlying joint probability distribution over the three random variables Toothache, Cavity and Catch, which we can write in the form of a table: Please find this part in lecture notes “Uncertainty” P19.
Please find this part in lecture notes “Uncertainty” P20. 
Please find this part in lecture notes “Uncertainty” P21.
Please find this part in lecture notes “Uncertainty” P22.
Please find this part in lecture notes “Uncertainty” P23.
Please find this part in lecture notes “Uncertainty” P24. In other words, learning that the Weather is sunny has no effect on the probability of having a cavity (and the same for rain, cloudy and snow). We say that Cavity and Weather are independent variables. 
If variables not independent, would need 32 items in probability table. Because Weather is independent of the other variables, only need two smaller tables, with a total of 8+4=12 items. P(Toothache, Catch, Cavity, Weather) = P(Toothache, Catch, Cavity)P(Weather) (Note: the number of free parameters is slightly less, because the values in each table must sum to 1). 
The variables Toothache, Cavity and Catch are not independent. But, they do exhibit conditional independence. If you have a cavity, the probability that the probe will catch is 0.9, no matter whether you have a toothache or not. If you don’t have a cavity, the probability that the probe will catch is 0.2, regardless of whether you have a toothache. In other words, P(Catch| Toothache, Cavity) = P(Catch| Cavity) We say that Catch is conditionally independent of Toothache given Cavity. 
This conditional independence reduces the number of free parameters from 7 down to 5. For larger problems with many variables, deducing this kind of conditional independence among the variables can reduce the number of free parameters substantially, and allow the Agent to maintain a simpler World Model. Equivalent statements: P(Toothache|Catch, Cavity) = P(Toothache|Cavity) P(Toothache, Catch|Cavity) = P(Toothache|Cavity)P(Catch|Cavity) 
The formula for conditional probability can be manipulated to find a relationship when the two variables are swapped: P(a b) = P(a| b)P(b) = P(b| a)P(a)  Baye’s rule P(a| b) = P(b| a)P(a) /P(b) This is often useful for assessing the probability of an underlying cause after an effect has been observed: P(Cause|Effect) = P(Effect|Cause)P(Cause)/ P(Effect) 
Question: Suppose we have a 98% accurate test for a type of cancer which occurs in 1% of patients. If a patient tests positive, what is the probability that they have the cancer? Answer: There are two random variables: Cancer (true or false) and Test (positive or negative). The probability is called a prior, because it represents our estimate of the probability before we have done the test (or made some other observation). We interpret the statement that the test is 98% accurate to mean: P(positive| cancer) = 0.98, and P(negative|cancer) = 0.98 
Please find this part in lecture notes “Uncertainty” P30.
Please find this part in lecture notes “Uncertainty” P31.
What is the probability of a Pit in (1,3) ? What about (2,2) ? To answer this, we need a prior assumption about the placement of Pits. We will assume a 20% chance of a Pit in each square at the beginning of the game (independent of what Pits are in the other squares). 
We will use Bi, j to indicate a Breeze in square (i, j), and Piti, j to indicate a Pit in square (i, j). We use known to represent what we know, i.e. B1,2  B2,1 B1,1 Pit1,2 Pit2,1 Pit1,1 We use Unknown to represent the joint probability of Pits in all the other squares, i.e. P(Unknown) = P(Pit1,4, . . . , Pit4,1) We divide Unknown into Fringe and Other, where P(Fringe) = P(Pit1,3, Pit2,2, Pit3,1) and Other is all the other variables. 
Please find this part in lecture notes “Uncertainty” P34.
Please find this part in lecture notes “Uncertainty” P35.
Please find this part in lecture notes “Uncertainty” P36.
Probability is a rigorous formalism for uncertain knowledge Joint probability distribution species probability of every atomic event Queries can be answered by summing over atomic events For nontrivial domains, we must find a way to reduce the joint size Independence and conditional independence provide the tools 
1. 1959 Checkers (Arthur Samuel) 2. 1961 MENACE tic-tac-toe (Donald Michie) 3. 1989 TD-Gammon (Gerald Tesauro) 4. 1997 TD-leaf (Baxter et al.) 5. 2006 MoGo using MCTS (Gelly & Wang) 6. 2009 TreeStrap (Veness et al.) 7. 2016 AlphaGo 8. 2018 Alpha Zero 
Please find this part in lecture notes “Learning Games” P4.
This BOXES algorithm was later adapted to learn more general tasks such as Pole Balancing, and helped lay the foundation for the modern eld of Reinforcement Learning. 1. BOXES: An Experiment in Adaptive Control, D.Michie and R.Chambers, Machine Intelligence, Oliver and Boyd, Edinburgh, UK, (1968). 
Artificial Intelligence in general, and Machine Learning in particular, came under heavy criticism in the early 1970s. Donald Michies Reinforcement Learning research was deliberately excluded from the 1973 Lighthill report because Lighthill wanted to focus attention on other areas which could most easily be criticised. The eld became largely dormant, until it was revived in the late 1980s, largely through the work of Richard Sutton. Gerald Tesauro applied Sutton’s TD-Learning algorithm to the game of Backgammon in 1989. 
Suppose we want a write a computer program to play a game like Backgammon, Chess, Checkers or Go. This can be done using a tree search algorithm (expectimax, MCTS, or minimax with alpha-beta pruning). But we need: (a) an appropriate way of encoding any board position as a set of numbers, and (b) a way to train a neural network or other learning system to compute a board evaluation, based on those numbers 
Please find this part in lecture notes “Learning Games” P10.
Board encoding: 1. 4 units * 2 players * 24 points 2. 2 units for the bar 3. 2 units for off the board Two layer neural network: 1. 196 input units 2. 20 hidden units 3. 1 output unit The input s is the encoded board position (state), the output V(s) is the value of this position (probability of winning). At each move, roll the dice, find all possible “next board positions”, convert them to the appropriate input format, feed them to the network, and choose the one which produces the largest output. 
Q: How do we choose the target value T ? In other words, how do we know what the value of the current position "should have been”? or, how do we find a better estimate for the value of the current position? 
1. Behavioral Cloning (Supervised Learning)  learn moves from human games (Expert Preferences) 2. Temporal Difference Learning (1) use subsequent positions to refine evaluation of current position (2) general method, does not rely on knowing the world model (rules of the game) 3. methods which combine learning with tree search (must know the “world model”) (1) TD-Root (Samuel, 1959) (2) TD-Leaf (Baxter et al., 1998) (3) TreeStrap (Veness et al., 2009) 
Please find this part in lecture notes “Learning Games” P14. 
1. Tesauro trained two networks: (1) EP-network was trained on Expert Preferences (Supervised) (2) TD-network was trained by self play (TD-Learning) 2. TD-network outperformed the EP-network. 3. With modifications such as 3-step lookahead (expectimax) and additional hand-crafted input features, TD-Gammon became the best Backgammon player in the world (Tesauro, 1995). 
1. Random dice rolls in Backgammon force self-play to explore a much larger part of the search space than in a deterministic game. 2. Humans are good at reasoning about a small set of probabilistic outcomes. But, playing Backgammon well requires aggregating a large set of possibilities, each with a small likelihood, and balancing them against each other. Neural Networks might be better at this than humans. 3. For deterministic games like Chess, direct TD-Learning performs poorly. Methods which combine learning with tree search are more effective. 
Move selection is by alpha-beta search, using some function V(s) to evaluate the leaves. 
1. Material weights  For example, Queen = 9, Rook = 5, Knight = Bishop = 3, Pawn = 1 2. Piece-Square weights  some (fractional) score for a particular piece on a particular square 3. Attack/Defend weights  some (fractional) score for one piece attacking or defending another piece. 4. Other features, such as Pawn structure, Mobility, etc. 5. There are no hidden nodes. V(s) is a linear combination of input features, composed with a sigmoid function, to produce a value between 0 and 1 (probability of winning). 
1. TD-Learning can be applied even if we do not know the world model. But, in this case we do know the world model (rules of the game) 2. Can we make use of the valuable information in the search tree? 
Please find this part in lecture notes “Learning Games” P21.
Please find this part in lecture notes “Learning Games” P22. 
Please find this part in lecture notes “Learning Games” P23.
1. all non-leaf positions are updated (including moves not selected) 2. when alpha-beta causes a cutoff, we can still train towards the upper or lower bound 
1. showed for the first time that a Chess player could be trained to Master level entirely by self-play, from random initial weights 2. learning sometimes became unstable (1) learning rate had to be carefully chosen (2) had to put a limit on the size of individual weight updates (3) we have since found that scaling the learning rate by depth of the node makes learning more stable 
Please find this part in lecture notes “Learning Games” P29.
Please find this part in lecture notes “Learning Games” P30. 
Please find this part in lecture notes “Learning Games” P31.
Please find this part in lecture notes “Learning Games” P32.
Please find this part in lecture notes “Learning Games” P33.
1. Games can be learned from human expert preferences, or from self-play (or a combination) 2. TD-Learning is a general method, which does not rely on knowing the world model 3. TreeStrap is more powerful, because it also refines the value of moves which were not chosen; but it relies on knowing the world model 4. Monte Carlo tree search good for games with large branching factor 5. Deep Learning for Go, Atari Games 6. Starcraft? 
