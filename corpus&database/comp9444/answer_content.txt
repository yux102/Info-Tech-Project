1. massively parallel distributed process or made up of simple processing units 2. knowledge acquired from environment through a learning process 3. knowledge stored in the form of synaptic weights
1. biologically inspired 2. good learning properties 3. continuous, nonlinear 4. well adapted to certain tasks 5. fault tolerant 6. graceful degradation
1. 380BC Plato (Rationalism - innateness) 2. 330BC Aristotle (Empricism - experience) 3. 1641 Descartes (mind-body Dualism) 4. 1781 Kant (Critique of Pure Reason) 5. 1899 Sigmund Freud (Psychology) 6. 1953 B.F. Skinner (Behaviourism)
1. 1642 Blaise Pascal (mechanical adding machine) 2. 1694 Gottfried Leibniz (mechanical calculator) 3. 1769 Wolfgang von Kempelen (Mechanical Turk) 4. 1837 Charles Babbage & Ada Lovelace (Difference Engine) 5. 1848 George Boole (the Calculus of Logic) 6. 1879 Gottlob Frege (Predicate Logic) 7. 1950 Turing Test 8. 1956 Dartmouth conference
1. 1943 McCulloch & Pitts (neuron models) 2. 1948 Norbert Wiener (Cybernetics) 3. 1948 Alan Turing (B-Type Networks) 4. 1955 Oliver Selfridge (Pattern Recognition) 5. 1962 Hubel and Wiesel (visual cortex) 6. 1962 Frank Rosenblatt (Perceptron)
1. 1956 Newell & Simon (Logic Theorist) 2. 1959 John McCarthy (Lisp) 3. 1959 Arther Samuel (Checkers) 4. 1965 Joseph Weizenbaum (ELIZA) 5. 1967 Edward Feigenbaum (Dendral)
1. 1969 Minsky & Papert published Perceptrons, emphasizing the limitations of neural models, and lobbied agencies to cease funding neural network research. 2. from 1969 to 1985 there was very little work in neural networks or machine learning. 3. a few exceptions, e.g. Stephen Grossberg, Teuvo Kohonen (SOM), Paul Werbos.
1. 1970s and early 1980s, AI research focused on symbolic processing, Expert Systems 2. Some commercial success, but ran into difficulties:(1). combinatorial explosion in search spaces (2). difficulty of formalising everyday knowledge as well as expert knowledge
1. 1986 Rumelhart, Hinton & Williams (multi-layer, backprop) 2. 1989 Dean Pomerleau (ALVINN) 3. late 1980’s renewed enthusiasm, hype 4. 1990s more principled approaches 5. 2000’s SVM, Bayesian models became more popular 6. 2010’s deep learning networks, GPU’s 7. 2020’s spiking networks(?)
1. Image processing (1). classification (2). segmentation 2.Language processing (1). translation (2). semantic disambiguation (3). sentiment analysis 3. Combining images and tex (1). automatic captioning 4. Game playing (1). AlphaGo (2). Deep Q-Learning
Two perspectives on the history of Deep Learning Viewpoint 1: Focusing on recent work (after 2012) https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf Viewpoint 2: Focusing on earlier work (before 2012) http://people.idsia.ch/~juergen/deep-learning-overview.html
1. Central Nervous System (1). Brain (2).Spinal cord 2. Peripheral Nervous System (1). Somatic nervous system (2). Autonomic nervous system (3). Enteric nervous system
1. “cortex” from Latin word for “bark” (of tree) 2. cortex is a sheet of tissue making up outer layers of brain , 2-6cmthick 3. right and left sides connected by corpus callosum 4. functions: thought, voluntary movement, language, reasoning, perception
1. general term for area of brain between the thalamus and spinal cord 2. includes medulla, pons, tectum, reticular formation and tegmentum 3. functions: breathing, heart rate, blood pressure, and others
1. from Latin word for “little brain” 2. functions: movement, balance, posture
functions: vision, audition, eye movement, body movement
1. receives sensory information and relays it to the cerebral cortex 2. also relays information from the cerebral cortex to other areas of the brain, and the spinal cord 3. functions: sensory integration, motor integration
1. composed of several different areas at the base of the brain 2. the size of a pea (about 1/300 of the total brain weight) 3. functions: body temperature, emotions, hunger, thirst, circadian rhythms
1. group of structures including amygdala, hippocampus, mammillary bodies and cingulate gyrus 2. important for controlling the emotional response to a given situation 3. hippocampus also important for memory 4. functions: emotional behaviour
1. The body is made up of billions of cells. Cells of the nervous system, called neurons, are specialized to carry “messages” through an electrochemical process. 2. The human brain has about 100 billion neurons, and a similar number of support cells called “glia”. 3. Neurons are similar to other cells in the body in some ways, such as: (1). neurons are surrounded by a cell membrane (2). neurons have a nucleus that contains genes (DNA) (3). neurons carry out basic cellular processes like protein synthesis and energy production
1. Neurons have specialized extensions called dendrites and axons Dendrites bring information to the cell body, while axons take information away from the cell body. 2. The axon of one neuron can connect to the dendrite of another neuron through an electrochemical junction called a synapse. 3. Most neurons have only one axon, but the number of dendrites can vary widely: (1). Unipolar and Bipolar neurons have only one dendrite (2). Purkinje neurons can have up to 100,000 dendrites
1. Dendrites are typically less than a millimetre in length 2. Axons can vary in length from less than a millimetre to more than a metre (motor neurons) 3. Long axons are sometimes surrounded by a myelinated sheath, which prevents the electrical signal from dispersing, and allows it to travel faster (up to 100 m/s).
1. electrical pulse reaches the endbulb and causes the release of neurotransmitter molecules from little packets (vesicles) through the synaptic membrane 2. transmitter then diffuses through the synaptic cleft to the other side 3. when the neurotransmitter reaches the post-synaptic membrane, it causes a change in polarisation of the membrane 4. the change in potential can be excitatiory (moving the potential towards the threshold) or inhibitory (moving it away from the threshold)
1. human brain has 100 billion neurons with an average of 10,000 synapses each 2. latency is about 3-6 milliseconds 3. therefore, at most a few hundred “steps” in any mental computation, but massively parallel
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s
1. Suppose we want to classify an image as a bird, sunset, dog, cat, etc. 2. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. 2. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image.
1. can “unroll” a recurrent architecture into an equivalent feedforward architecture, with shared weights 2. useful for processing language or other temporal sequences
1. output is trained to reproduce the input as closely as possible 2. activations normally pass through a bottleneck, so the network is forced to compress the data in some way
1. biological neurons spike in different patterns (quiescent, persistent, sporadic) 2. spike timing might carry important information 3. most NN models ignore timing information, but some work has been done on spiking network models 4. in the future, special hardware might lead to a revolution for spiking networks, similar to what GPU’s provided for CNN’s
1. Neurons – Biological and Artificial 2. Perceptron Learning 3. Linear Separability 4. Multi-Layer Networks
1. The brain is made up of neurons (nerve cells) which have (1). a cell body (soma) (2). dendrites (inputs) (3). an axon (outputs) (4). synapses (connections between cells) 2. Synapses can be exitatory or inhibitory and may change over time. When the inputs reach some threshhold an action potential (electrical pulse) is sent along the axon to the outputs.
1.(Artificial) Neural Networks are made up of nodes which have (1). inputs edges, each with some weight (2). outputs edges (with weights) (3). an activation level (a function of the inputs) 2. Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: Some nodes are inputs (sensing), some are outputs (action)
w0=-th, s = w1x1 + w2x2−th = w1x1 + w2x2 + w0, x1, x2 are inputs, w1, w2 are synaptic weights, th is a threshold, w0 is a bias weight, g is transfer function
linearly separable functions
In 1969, Minsky and Papert published a book highlighting the limitations of Perceptrons, and lobbied various funding agencies to redirect funding away from neural network research, preferring instead logic-based methods such as expert systems. It was known as far back as the 1960’s that any given logical function could be implemented in a 2-layer neural network with step function activations. But, the the question of how to learn the weights of a multi-layer neural network based on training examples remained an open problem. The solution, which we describe in the next section, was found in 1976 by Paul Werbos, but did not become widely known until it was rediscovered in 1986 by Rumelhart, Hinton and Williams.
1. Supervised Learning 2. Ockham’s Razor (5.2) 3. Multi-Layer Networks 4.  Gradient Descent (4.3, 6.5.2)
1. Supervised Learning: agent is presented with examples of inputs and their target outputs 2. Reinforcement Learning: agent is not presented with target outputs, but is given a reward signal, which it aims to maximize 3. Unsupervised Learning: agent is only presented with the inputs themselves, and aims to find structure in these inputs
1. we have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specified. 2. the aim is to predict the target value, based on the input attributes. 3. agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set 4. various learning paradigms are available: (1). Neural Network (2). Decision Tree (3). Support Vector Machine, etc.
1. framework (decision tree, neural network, SVM, etc.) 2.  representation (of inputs and outputs) 3. pre-processing / post-processing 4. training method (perceptron learning, backpropagation, etc.) 5. generalization (avoid over-fitting) 6. evaluation (separate training and testing sets)
“The most likely hypothesis is the simplest one consistent with the data.” Since there can be noise in the measurements, in practice need to make a tradeoff between simplicity of the hypothesis and how well it fits the data.
x1 XOR x2 can be written as: (x1 AND x2) NOR (x1 NOR x2) Recall that AND, OR and NOR can be implemented by perceptrons.
Normally, the numbers of input and output units are fixed, but we can choose the number of hidden units.
for this toy problem, there is only a training set; there is no validation or test set, so we don’t worry about overfitting the XOR data cannot be learned with a perceptron, but can be achieved using a 2-layer network with two hidden units
We define an error function E to be (half) the sum over all input patterns of the square of the difference between actual output and desired output If we think of E as height, it defines an error landscape on the weight space. The aim is to find a set of weights for which E is very low.
because of the step function, the landscape will not be smooth but will instead consist almost entirely of flat local regions and “shoulders”, with occasional discontinuous jumps.
Recall that the error function E is (half) the sum over all input patterns of the square of the difference between actual output and desired output The aim is to find a set of weights for which E is very low. If the functions involved are smooth, we can use multi-variable calculus to adjust the weights in such a way as to take us in the steepest downhill direction. Parameter η is called the learning rate. 
This principle can be used to compute the partial derivatives in an efficient and localized manner. Note that the transfer function must be differentiable (usually sigmoid, or tanh).
1. Medical Dignosis 2. Autonomous Driving 3. Game Playing 4. Credit Card Fraud Detection 5. Handwriting Recognition 6. Financial Prediction
1. re-scale inputs and outputs to be in the range 0 to 1 or −1 to 1 2. replace missing values with mean value for that attribute 3. initialize weights to very small random values 4. on-line or batch learning 5. three different ways to prevent overfitting: (1). limit the number of hidden nodes or connections (2). limit the training time, using a validation set (3). weight decay 6. adjust learning rate (and momentum) to suit the particular task
1. Autonomous Land Vehicle In a Neural Network 2.  later version included a sonar range finder (1). 8×32 range finder input retina (2). 29 hidden units (3). 45 output units 3. Supervised Learning, from human actions (Behavioral Cloning) additional “transformed” training items to cover emergency situations 4. drove autonomously from coast to coast
1. Neural networks are biologically inspired 2. Multi-layer neural networks can learn nonlinearly separable functions 3. Backpropagation is effective and widely used
1. Probability (3.1-3.6, 3.9.3, 3.10) 2. Cross Entropy (5.5) 3. Bayes’ Rule (3.11) 4. Weight Decay (5.2.2) 5. Momentum (8.3)
Begin with a set Ω – the sample space (e.g. 6 possible rolls of a die) ω ∈ Ω is a sample point/possible world/atomic event A probability space or probability model is a sample space with an assignment P(ω) for every ω ∈ Ω s.t.
A random variable (r.v.) is a function from sample points to some range (e.g. the Reals or Booleans) For example, Odd(3) = true.
1. cross entropy (1). problem: least squares error function unsuitable for classification, where target = 0 or 1 (2). mathematical theory: maximum likelihood (3). solution: replace with cross entropy error function 2. Weight Decay (1). problem: weights “blow up”, and inhibit further learning (2). mathematical theory: Bayes’ rule (3). solution: add weight decay term to error function 3. Momentum (1). problem: weights oscillate in a “rain gutter” (2). solution: weighted average of gradient over time
For classification tasks, target t is either 0 or 1, so better to use E = −t log(z −(1−t)log(1−z) This can be justified mathematically, and works well in practice – especially when negative examples vastly outweigh positive ones. It also makes the backprop computations simpler.
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. logP(D|h) is called the likelihood. ML Principle: Choose h ∈ H which maximizes the likelihood, i.e. maximizes P(D|h) [or, maximizes logP(D|h)]
The formula for conditional probability can be manipulated to find a relationship when the two variables are swapped: P(a∧b) = P(a|b)P(b) = P(b a)P(a) This is often useful for assessing the probability of an underlying cause after an effect has been observed:
Question: Suppose we have a test for a type of cancer which occurs in 1% of patients. The test has a sensitivity of 98% and a specificity of 97%. If a patient tests positive, what is the probability that they have the cancer Answer: There are two random variables: Cancer (true or false) and Test (positive or negative). The probability is called a prior, because it represents our estimate of the probability before we have done the test (or made some other observation). The sensitivity and specificity are interpreted as follows: P(positive|cancer) = 0.98, and P(negative|¬cancer) = 0.97
Question: You work for a lighting company which manufactures 60% of its light bulbs in Factory A and 40% in Factory B. One percent of the light bulbs from Factory A are defective, while two percent of those from Factory B are defective. If a random light bulb turns out to be defective, what is the probability that it was manufactured in Factory A? Answer: There are two random variables: Factory (A or B) and Defect (Yes or No). In this case, the prior is: P(A) = 0.6, P(B) = 0.4 The conditional probabilities are: P(defect|A) = 0.01, and P(defect|B) = 0.02
H is a class of hypotheses P(D|h) = probability of data D being generated under hypothesis h ∈ H. P(h|D) = probability that h is correct, given that data D were observed. Bayes’ Theorem: P(h) is called the prior.
Assume that small weights are more likely to occur than large weights, i.e. where Z is a normalizing constant. Then the cost function becomes: This can prevent the weights from “saturating” to very high values. Problem: need to determine λ from experience, or empirically.
If landscape is shaped like a “rain gutter”, weights will tend to oscillate without much improvement. Solution: add a momentum factor
Compute matrix of second derivatives (called the Hessian). Approximate the landscape with a quadratic function (paraboloid). Jump to the minimum of this quadratic function.
Use methods from information geometry to find a “natural” re-scaling of the partial derivatives.
1. Geometry of Hidden Unit Activations 2. Limitations of 2-layer networks 3. Alternative transfer functions (6.3) 4. Dropout
1. swap any pair of hidden nodes, overall function will be the same 2. on any hidden node, reverse the sign of all incoming and outgoing weights (assuming symmetric transfer function) 3. hidden nodes with identical input-to hidden weights in theory would never separate; so, they all have to begin with different (small) random weights 4. in practice, all hidden nodes try to do similar job at first, then gradually specialize.
1. for small weights, each layer implements an approximately linear function, so multiple layers also implement an approximately linear function. 2. for large weights, transfer function approximates a step function, so computation becomes digital and learning becomes very slow. 3. with typical weight values, two-layer neural network implements a function which is close to linear, but takes advantage of a limited degree of nonlinearity.
Some functions cannot be learned with a 2-layer sigmoidal network. For example, this Twin Spirals problem cannot be learned with a 2-layer network, but it can be learned using a 3-layer network if we include shortcut connections between non-consecutive layers.
1. Twin Spirals can be learned by 3-layer network with shortcut connections 2. first hidden layer learns linearly separable features 3. second hidden layer learns “convex” features 4. output layer combines these to produce “concave” features 5. training the 3-layer network is delicate 6. learning rate and initial weight values must be very small 7. otherwise, the network will converge to a local optimum 
Training by backpropagation in networks with many layers is difficult. When the weights are small, the differentials become smaller and smaller as we backpropagate through the layers, and end up having no effect. When the weights are large, the activations in the higher layers will saturate to extreme values. As a result, the gradients at those layers will become very small, and will not be propagated to the earlier layers. When the weights have intermediate values, the differentials will sometimes get multiplied many times is places where the transfer function is steep, causing them to blow up to large values.
1. layerwise unsupervised pre-training 2. long short term memory (LSTM) 3. new activations functions LSTM is specifically for recurrent neural networks. We will discuss unsupervised pre-training and LSTM later in the course.
1. Sigmoid and hyperbolic tangent traditionally used for 2-layer networks, but suffer from vanishing gradient problem in deeper networks. 2. Rectified Linear Units (ReLUs) are popular for deep networks, including convolutional networks. Gradients don’t vanish. But, their highly linear nature may cause other problems. 3. Scaled Exponential Linear Units (SELUs) are a recent innovation which seems to work well for very deep networks.
Nodes are randomly chosen to not be used, with some fixed probability (usually, one half). When training is finished and the network is deployed, all nodes are used, but their activations are multiplied by the same probability that was used in the dropout. Thus, the activation received by each unit is the average value of what it would have received during training. Dropout forces the network to achieve redundancy because it must deal with situations where some features are missing. Another way to view dropout is that it implicitly (and efficiently) simulates an ensemble of different architectures.
Ensembling is a method where a number of different classifiers are trained on the same task, and the final class is decided by “voting” among them. In order to benefit from ensembling, we need to have diversity in the different classifiers. For example, we could train three neural networks with different architectures, three Support Vector Machines with different dimensions and kernels, as well as two other classifiers, and ensemble all of them to produce a final result. (Kaggle Competition entries are often done in this way)
Diversity can also be achieved by training on different subsets of data. Suppose we are given N training items. Each time we train a new classifier, we choose N items from the training set with replacement. This means that some items will not be chosen, while others are chosen two or three times. There will be diversity among the resulting classifiers because they have each been trained on a different subset of data. They can be ensembled to produce a more accurate result than a single classifier.
In the case of dropout, the same data are used each time but a different architecture is created by removing the nodes that are dropped. The trick of multiplying the output of each node by the probability of dropout implicitly averages the output over all of these different models.
Suppose we want to classify an image as a bird, sunset, dog, cat, etc. If we can identify features such as feather, eye, or beak which provide useful information in one part of the image, then those features are likely to also be relevant in another part of the image. We can exploit this regularity by using a convolution layer which applies the same weights to different parts of the image.
1. cells in the visual cortex respond to lines at different angles 2. cells in V2 respond to more sophisticated visual features 3. Convolutional Neural Networks are inspired by this neuroanatomy 4. CNN’s can now be simulated with massive parallelism, using GPU’s
1. convolution layers: extract shift-invariant features from the previous layer 2. subsampling or pooling layers: combine the activations of multiple units from the previous layer into one unit 3. fully connected layers: collect spatially diffuse information 4. output layer: choose between classes
There can be multiple steps of convolution followed by pooling, before reaching the fully connected layers. Note how pooling reduces the size of the feature map (usually, by half in each direction).
Consider a classification task with N classes, and assume zj is the output of the unit corresponding to class j. We assume the network’s estimate of the probability of each class j is proportional to exp(zj). Because the probabilites must add up to 1, we need to normalize by dividing by their sum: If the correct class is i, we can treat −logProb(i) as our cost function. The first term pushes up the correct class i, while the second term mainly pushes down the incorrect class j with the highest activation (if j ̸= i).
For example: Assume the original image is J × K , with L channels. We apply an M × N “filter” to these inputs to compute one hidden unit in the convolution layer. In this example J = 6,K = 7,L = 3,M = 3,N = 3. The same weights are applied to the next M × N block of inputs, to compute the next hidden unit in the convolution layer (“weight sharing”). If the original image size is J×K and the filter is size M×N, the convolution layer will be (J+1−M)×(K+1−N)
For example, in the first convolutional layer of LeNet, J = K = 32, M = N = 5. The width of the next layer is J + 1 − M = 32 + 1 − 5 = 28 Question: If there are 6 filters in this layer, compute the number of: weights per neuron? neurons? connections? independent parameters? 
The 5 × 5 window of the first convolution layer extracts from the original 32 × 32 image a 28 × 28 array of features. Subsampling then halves this size to 14 × 14. The second Convolution layer uses another 5 × 5 window to extract a 10 × 10 array of features, which the second subsampling layer reduces to 5 × 5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits ’0’ to ’9’.
Sometimes, we treat the off-edge inputs as zero (or some other value). This is known as “Zero-Padding”. With Zero Padding, the convolution layer is the same size as the original image (or the previous layer).
1. 5 convolutional layers + 3 fully connected layers 2. max pooling with overlapping stride 3. softmax with 1000 classes 4. 2 parallel GPUs which interact only at certain layers
Assume the original image is J × K , with L channels. We again apply an M × N filter, but this time with a “stride” of s > 1. In this example J = 7, K = 9, L = 3, M = 3, N = 3, s = 2. The same formula is used, but j and k are now incremented by s each time. The number of free parameters is 1+L×M×N
j takes on the values 0,s,2s,...,(J−M) k takes on the values 0,s,2s,…,(K−N) The next layer is (1+(J−M)/s) by (1+(K−N)/s)
When combined with zero padding of width P, j takes on the values 0,s,2s,…,(J+2P−M) k takes on the values 0,s,2s,…,(K+2P−N) The next layer is (1+(J+2P−M)/s) by (1+(K+2P−N)/s)
For example, in the first convolutional layer of AlexNet, J = K = 224, P = 2, M = N = 11, s = 4. The width of the next layer is 1+(J+2P−M)/s=1+(224+2×2−11)/4=55
If the previous layer is J × K, and max pooling is applied with width F and stride s, the size of the next layer will be (1+(J−F)/s)×(1+(K−F)/s)
1. Image Datasets and Tasks 2. Convolution in Detail 3. AlexNet 4. Weight Initialization 5. Batch Normalization 6. Residual Networks 7. Dense Networks 8. Style Transfer
1. black and white, resolution 28×28 2. 60,000 images 3. 10 classes (0,1,2,3,4,5,6,7,8,9)
1. color, resolution 32×32 2. 50,000 images 3. 10 classes
1. color, resolution 227×227 2. 1.2 million images 3. 1000 classes
1. image classification 2. object detection 3. object segmentation 4. style transfer 5. generating images 6. generating art 7. image captioning
The 5 × 5 window of the first convolution layer extracts from the original 32 × 32 image a 28 × 28 array of features. Subsampling then halves this size to 14 × 14. The second Convolution layer uses another 5 × 5 window to extract a 10 × 10 array of features, which the second subsampling layer reduces to 5 × 5. These activations then pass through two fully connected layers into the 10 output units corresponding to the digits ’0’ to ’9’.
1. AlexNet, 8 layers (2012) 2. VGG, 19 layers (2014) 3. GoogleNet, 22 layers (2014) 4. ResNets, 152 layers (2015)
1. 5 convolutional layers + 3 fully connected layers 2. max pooling with overlapping stride 3. softmax with 1000 classes 4. 2 parallel GPUs which interact only at certain layers
1. 650K neurons 2. 630M connections 3. 60M parameters 4. more parameters that images → danger of overfitting
1. Rectified Linear Units (ReLUs) 2. overlappingpooling(width=3,stride=2) 3. stochastic gradient descent with momentum and weight decay 4. data augmentation to reduce overfitting 5. 50% dropout in the fully connected layers
1. ten patches of size 224 × 224 are cropped from each of the original 227 × 277 images (using zero padding) 2. the horizontal reflection of each patch is also included. 3. at test time, average the predictions on the 10 patches. 4. also include changes in intensity to RGB channels
1. filters on GPU-1 (upper) are color agnostic 2. filters on GPU-2 (lower) are color specific 3. these resemble Gabor filters
1. > 10 layers: weight initialization and batch nomalization 2. > 30 layers: skip connections 3.  > 100 layers: identity skip connections
1. Example: Toss a coin once, and count the number of Heads Mean μ Variance Standard Deviation σ = 1 (0+1) = 0.5 2 Mean μ Variance Standard Deviation σ =100∗0.5 =50 = 100 ∗ 0.25 = 25 = √Variance = 5 = 1  (0 − 0.5)2 + (1 − 0.5)2)  = 0.25 2 = √Variance = 0.5 2. Example: Toss a coin 100 times, and count the number of Heads 3. Example: Toss a coin 10000 times, and count the number of Heads μ=5000, σ =√2500 =50
Idea: Take any two consecutive stacked layers in a deep network and add a “skip” connection which bipasses these layers and is added to their output. 1. the preceding layers attempt to do the “whole” job, making x as close as possible to the target output of the entire network 2. F(x) is a residual component which corrects the errors from previous layers, or provides additional details which the previous layers were not powerful enough to compute 3. with skip connections, both training and test error drop as you add more layers 4. with more than 100 layers, need to apply ReLU before adding the residual instead of afterwards. This is called an identity skip connection.
Recently, good results have been achieved using networks with densely connected blocks, within which each layer is connected by shortcut connections to all the preceding layers.
1. pretrain CNN on ImageNet (VGG-19) 2. pass input texture through CNN; compute feature map F l for ith filter at spatial location k in layer (depth) l 3. compute the Gram matrix for each pair of features 4. feed (initially random) image into CNN 5. compute L2 distance between Gram matrices of original and new image 6. backprop to get gradient on image pixels 7. update image and go to step 5.
1. “ImageNet Classification with Deep Convolutional Neural Networks”, Krizhevsky et al., 2015. 2. “Understanding the difficulty of training deep feedforward neural networks”, Glorot & Bengio, 2010. 3. “Batch normalization: Accelerating deep network training by reducing internal covariate shift”, Ioffe & Szegedy, ICML 2015. 4. “Deep Residual Learning for Image Recognition”, He et al., 2016. 5. “Densely Connected Convolutional Networks”, Huang et al., 2016. 6. “A Neural Algorithm of Artistic Style”, Gatys et al., 2015.
1. Processing Temporal Sequences 2. Sliding Window 3. Recurrent Network Architectures 4. Hidden Unit Dynamics 5. Long Short Term Memory
There are many tasks which require a sequence of inputs to be processed rather than a single input. 1. speech recognition 2. time series prediction 3. machine translation 4. handwriting recognition
The simplest way to feed temporal input to a neural network is the “sliding window” approach, first used in the NetTalk system (Sejnowski & Rosenberg, 1987).
Given a sequence of 7 characters, predict the phonetic pronunciation of the middle character. For this task, we need to know the characters on both sides. For example, how are the vowels in these words pronounced? pa pat pate paternal mo mod mode modern
1. NETtalk gained a lot of media attention at the time. 2. Hooking it up to a speech synthesizer was very cute. In the early stages of training, it sounded like a babbling baby. When fully trained, it pronounced the words mostly correctly (but sounded somewhat robotic). 3. Later studies on similar tasks have often found that a decision tree could produce equally good or better accuracy. 4. This kind of approach can only learn short term dependencies, not the medium or long term dependencies that are required for some tasks.
1. at each time step, hidden layer activations are copied to “context” layer 2. hidden layer receives connections from input and context layers 3. the inputs are fed one at a time to the network, it uses the context layer to “remember” whatever information is required for it to produce the correct output
1. we can “unroll” are current architecture into an equivalent feedforward architecture, with shared weights 2. applying backpropagation to the unrolled architecture is reffered to as “backpropagation through time” 3. we can backpropagate just one timestep, or a fixed number of timesteps, or all the way back to beginning of the sequence
1. it is sometimes beneficial to add “shortcut” connections directly from input to output 2. connections from output back to hidden have also been explored (sometimes called “Jordan Networks”)
Scan a sequence of characters one at a time, then classify the sequence as Accept or Reject.
1. gated network trained by BPTT 2. emulates exactly the behaviour of Finite State Automaton 3. trained network emulates the behaviour of Finite State Automaton 4. training set must include short, medium and long examples
Scan a sequence of characters one at a time, and try at each step to predict the next character in the sequence. In some cases, the prediction is probabilistic. For the anbn task, the first b is not predictable, but subsequent b’s and the initial a in the next subsequence are predictable.
1. for this task, sequence is accepted if the number of a’s and b’s are equal 2. network counts up by spiralling inwards, down by spiralling outwards
1. Simple Recurrent Networks (SRNs) can learn medium-range dependencies but have difficulty learning long range dependencies 2. Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU) can learn long range dependencies better than SRN
Two excellent Web resources for LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ christianherta.de/lehre/dataScience/machineLearning/neuralNetworks/LSTM.php LSTM – context layer is modulated by three gating mechanisms: forget gate, input gate and output gate. http://colah.github.io/posts/2015-08-Understanding-LSTMs/
SRN – context layer is combined directly with the input to produce the next hidden layer. SRN can learn Reber Grammar, but not Embedded Reber Grammar.
