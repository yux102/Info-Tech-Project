1. origins 2. motivation 3. minimax search 4. resource limits and heuristic evaluation 5. - pruning 6. stochastic games 7. partially observable games 8. continuous, embodied games
1. 1769 Wolfgang von Kempelen (Mechanical Turk) 2. 1846 Charles Babbage & Ada Lovelace (tic-tac-toe) 3. 1952 Alan Turing (Chess algorithm) 4. 1959 Arthur Samuel (Checkers) 5. 1961 Donald Michie (MENACE machine learner)
What shall we do to get rid of Mr. Babbage and his calculating machine? (Prime Minister Robert Peel, 1842)
For the machine is not a thinking being, but simply an automation which acts according to the laws imposed upon it. (Ada Lovelace, 1843)
1. Discrete Games  fully observable, deterministic (chess, checkers, go, othello)  fully observable, stochastic (backgammon, monopoly)  partially observable (bridge, poker, scrabble) 2. Continuous, embodied games  robocup soccer, pool (snooker)
1. Computer considers possible lines of play (Babbage, 1846) 2. Algorithm for perfect play (Zermelo, 1912; Von Neumann, 1944) 3. Finite horizon, approximate evaluation (Zuse, 1945; Wiener, 1948; Shannon, 1950) 4. First chess program (Turing, 1951) 5. Machine learning to improve evaluation accuracy (Samuel, 1952-57) 6. Pruning to allow deeper search (McCarthy, 1956)
1. Unpredictable opponent  solution is a strategy  must respond to every possible opponent reply 2. Time limits  must rely on approximation  tradeoff between speed and accuracy 3. Games have been a key driver of new techniques in CS and AI
Elaborate table-lookup procedures, fast sorting and searching procedures, and a variety of new programming tricks were developed... Samuels 1959 paper contains groundbreaking ideas in these areas: 1. hash tables 2. data compression 3. parameter tuning via machine learning
MAX (X) X X X MIN (O) X X X X X X MAX (X) X O X O X O MIN (O) X O X X O X X O X . . . . . . . . . TERMINAL X O X X O O X O X X O O X X O X O X X OO X Utility 1  0 +1 . . . . . . . . . . . .
Perfect play for deterministic, perfect-information games Idea: choose move to position with highest minimax value = best achievable payoff against best play MAX MIN A 2 3 2 A 3 2 A 1 3 A 11 A 12 A 13 A 21 A 22 A 23 A 31 A 32 A 33 3 12 8 2 4 6 14 5 2
function minimax( node, depth ) if node is a terminal node or depth = 0 return heuristic value of node if we are to play at node let  =  foreach child of node let  = max( , minimax( child, depth-1 )) return  else // opponent is to play at node let  = + foreach child of node let  = min( , minimax( child, depth-1 )) return 
The above formulation of Minimax assumes that all nodes are evaluated with respect to a xed player (e.g. White in Chess). If we instead assume that each node is evaluated with respect to the player whose turn it is to move, we get a simpler formulation known as Negamax.
function negamax( node, depth ) if node is terminal or depth = 0 return heuristic value of node // from perspective of player whose turn it is to move let  =  foreach child of node let  = max( , -negamax( child, depth-1 )) return 
1. Complete? 2. Optimal? 3. Time complexity? 4. Space complexity?
For chess, b  35, m  100 for reasonable games  exact solution completely infeasible Two ways to make the search feasible: 1. dont search to nal position; use heuristic evaluation at the leaves 2. - pruning
1. material  Queen = 9, Rook = 5, Knight = Bishop = 3, Pawn = 1 2. position  some (fractional) score for a particular piece on a particular square 3. interaction  some (fractional) score for one piece attacking another piece, etc. 4. KnightCap used 2000 different features, but evaluation is rapid because very few features are non-zero for any particular board state (e.g. Queen can only be on one of the 64 squares at a time) 5. the value of individual features can be determined by reinforcement learning
Q1: Why would Queen to G5 be a bad move for Black? Q2: How many White replies did you need to consider in answering? Once we have seen one reply scary enough to convince us the move is really bad, we can abandon this move and continue searching elsewhere.
MAX MIN 3 3 2 3 12 8 2 X X
MAX MIN 3 3 2 14 3 12 8 2 X X 14
MAX MIN 3 3 2 14 5 3 12 8 2 X X 14 5
MAX MIN 3 3 3 2 14 5 2 3 12 8 2 X X 14 5 2
function alphabeta( node, depth, ,  ) if node is terminal or depth = 0 { return heuristic value of node } if we are to play at node foreach child of node let  = max( , alphabeta( child, depth-1, ,  )) if    { return  } return  else // opponent is to play at node foreach child of node let  = min( , alphabeta( child, depth-1, ,  )) if    { return  } return 
function minimax( node, depth ) return alphabeta( node, depth, ,  ) function alphabeta( node, depth, ,  ) if node is terminal or depth = 0 return heuristic value of node // from perspective of player whose turn it is to move foreach child of node let  = max( , -alphabeta( child, depth-1, -, - )) if    return  return 
MAX MIN .. .. .. MAX MIN V  is the best value for us found so far, off the current path  is the best value for opponent found so far, off the current path If we nd a move whose value exceeds , pass this new value up the tree. If the current node value exceeds , it is too good to be true, so we prune off the remaining children.
- pruning is guaranteed to give the same result as minimax, but speeds up the computation substantially Good move ordering improves effectiveness of pruning With perfect ordering, time complexity = O(bm/2) To prove that a bad move is bad, we only need to consider one (good) reply. But to prove that a good move is good, we need to consider all replies. This means - can search twice as deep as plain minimax. An increase in search depth from 6 to 12 could change a very weak player into a quite strong one.
Deep Blue defeated human world champion Gary Kasparov in a six-game match in 1997. Traditionally, computers played well in the opening (using a database) and in the endgame (by deep search) but humans could beat them in the middle game by opening up the board to increase the branching factor. Kasparov tried this, but because of its speed Deep Blue remained strong. Some experts believe Kasparov should have been able to defeat Deep Blue in 1997 if he hadnt lost his nerve. However, chess programs stronger than Deep Blue are now running on standard PCs and could denitely defeat the strongest humans. Modern chess programs rely on quiescent search, transposition tables and pruning heuristics.
Chinook failed to defeat human world champion Marion Tinsley prior to his death in 1994, but has beaten all subsequent human champions. Chinook used an endgame database dening perfect play for all positions involving 8 or fewer pieces on the board  a total of 443,748,401,247 positions. This database has since been expanded to include all positions with 10 or fewer pieces (38 trillion positions). In 2007, Jonathan Shaeffer released a new version of Chinook and published a proof that it will never lose. His proof method lls out the game tree incrementally, ignoring branches which are likely to be pruned. After many months of computation, it eventually converges to a skeleton of the real (pruned) tree which is comprehensive enough to complete the proof.
The branching factor for Go is greater than 300, and static board evaluation is difcult. Traditional Go programs broke the board into regions and used pattern knowledge to explore each region. Since 2006, new Monte Carlo players have been developed using UCB search. A tree is built up stochastically. After a small number of moves, the rest of the game is played out randomly, using fast pattern matching to give preference to urgent moves. In March 2016, AlphaGo defeated the human Go champion Lee Sedol in a 4-1 match. AlphaGo uses MCTS, with deep learning neural networks for move selection and board evaluation. The networks are trained initially on a database of thousands of human championship Go games, and then rened with millions of games of self-play.
0 1 2 3 4 5 6 7 8 9 10 11 12 25 24 23 22 21 20 19 18 17 16 15 14 13
In stochastic games, chance introduced by dice, card-shufing, etc. Expectimax is an adaptation of Minimax which also handles chance nodes. ... ... if node is a chance node return average of values of successor nodes Adaptations of - pruning are possible, provided the evaluation is bounded.
MAX CHANCE 3 1 0.5 0.5 0.5 0.5 MIN 2 4 0 2 2 4 7 4 6 0 5 2
MAX MIN 1 2 1 20 1 2 2 4 1 20 20 400 Move choice is preserved under any monotonic transformation of EVAL. Only the order matters: payoff in deterministic games acts as an ordinal utility function.
MAX DICE 2.1 .9 .1 .9 1.3 .1 21 40.9 .9 .1 .9 .1 MIN 2 3 1 4 20 30 1 400 2 2 3 3 1 1 4 4 20 20 30 30 1 1 400 400 Move choice only preserved by positive linear transformation of EVAL Hence EVAL should be proportional to the expected payoff.
Card games are partially observable, because (some of) the opponents cards are unknown. This makes the problem very difcult, because some information is known to one player but not to another. Typically we can calculate a probability for each possible deal. Idea: compute the minimax value of each action in each deal, then choose the action with highest expected value over all deals. GIB, a strong and well-known bridge program, approximates this idea by 1) generating 100 deals consistent with bidding information 2) picking the action that wins most tricks on average
Currently best solution uses A*Search, after reverse engineering the world model.
Combines path planning, low-level control, reasoning under uncertainty and (for ghosts) multi-agent coordination.
Low level technical issues 1. undistortion of overhead camera image 2. ball appears egg-shaped, need to nd centre accurately High level strategy 3. easy to sink current ball 4. more complicated to set up for the next ball 5. competition using physical simulator
Machine Educable Noughts And Crosses Engine Donald Michie, 1961
MAX (X) X X X MIN (O) X X X X X X MAX (X) X O X O X O MIN (O) X O X X O X X O X . . . . . . . . . TERMINAL X O X X O O X O X X O O X X O X O X X OO X Utility 1  0 +1 . . . . . . . . . . . .
1. games are fun to work on! 2. games continue to be a driver of new technology 3. tradeoff between speed and accuracy 4. probabilistic reasoning 5. force us to build whole systems  chain is as strong as its weakest link
Tom Standage, 2002. The Mechanical Turk, Penguin Books. Arthur Samuel, 1959. Some studies in machine learning using the game of checkers, IBM Journal on Research and Development, pages 210-229. Chinook: www.cs.ualberta.ca/ chinook Robocup: www.robocup.org [look for Innite Mario and Deep Green on youtube]
