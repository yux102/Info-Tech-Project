1. Neurons  Biological and Articial 2. Perceptron Learning 3. Linear Separability 4. Multi-Layer Networks
The brain is made up of neurons (nerve cells) which have 1. a cell body (soma) 2. dendrites (inputs) 3. an axon (outputs) 4. synapses (connections between cells) Synapses can be exitatory or inhibitory and may change over time. When the inputs reach some threshhold an action potential (electrical pulse) is sent along the axon to the outputs.
1. human brain has 100 billion neurons with an average of 10, 000 synapses each 2. latency is about 3-6 milliseconds 3. therefore, at most a few hundred steps in any mental computation, but massively parallel
(Articial) Neural Networks are made up of nodes which have 1. inputs edges, each with some weight 2. outputs edges (with weights) 3. an activation level (a function of the inputs) Weights can be positive or negative and may change over time (learning). The input function is the weighted sum of the activation levels of inputs. The activation level is a non-linear transfer function g of this input: activationi = g(si) = g( j wi jx j) Some nodes are inputs (sensing), some are outputs (action)
w1 s  g  g(s)  x1 x2   w2   w0=-th   s = w1x1 + w2x2th x1, x2 are inputs w1, w2 are synaptic weights 1 = w1x1 + w2x2 + w0 th is a threshold w0 is a bias weight g is transfer function
Originally, a (discontinuous) step function was used for the transfer function: g(s) = n 1, 0, if if s  0 s < 0 (Later, other transfer functions were introduced, which are continuous and smooth)
Q: what kind of functions can a perceptron compute? A: linearly separable functions Examples include: AND OR NOR w1 = w2 = 1.0, w0 = 1.5 w1 = w2 = 1.0, w0 = 0.5 w1 = w2 = 1.0, w0 = 0.5 Q: How can we train it to learn a new function?
Adjust the weights as each input is presented. recall: s = w1x1 + w2x2 + w0 if g(s) = 0 but should be 1, if g(s) = 1 but should be 0, wk  wk +  xk w0  w0 +  wk  wk   xk w0  w0   so s  s +  (1 +  x2 k ) so s  s   (1 +  x2 k ) k k otherwise, weights are unchanged. ( > 0 is called the learning rate) Theorem: This will eventually learn to classify the data correctly, as long as they are linearly separable.
x1 x2 w1   w2   (+/)  w1 x1 + w2 x2 + w0 > 0 learning rate  = 0.1 begin with random weights   w0   1 w1 = 0.2 w2 = 0.0 w0 = 0.1
x 2 0.2 x1 + 0.0 x2  0.1 > 0 (1,1) w1  w1   x1 = 0.1 w2  w2   x2 = 0.1 w0  w0   = 0.2 x 1
x 2 0.1 x1  0.1 x2  0.2 > 0 w1  w1 +  x1 w2  w2 +  x2 = = 0.3 0.0 w0  w0 +  = 0.1 (2,1) x 1
x 2 (1.5,0.5) 0.3 x1 + 0.0 x2  0.1 > 0 (2,2) 3rd point correctly classied, so no change 4th point: w1  w1   x1 = 0.1 w2  w2   x2 = 0.2 w0  w0   = 0.2 x 1 0.1 x1  0.2 x2  0.2 > 0
x 2 eventually, all the data will be correctly classied (provided it is linearly separable) x 1
Problem: many useful functions are not linearly separable (e.g. XOR) I 1 1 0 0 I 1 1 0 0 1 I 2 I 1 1 0 0 ? 1 I 2 1 I 2 (a) I 1 and  I 2 (b) I 1 or I 2 (c) I 1 xor I 2 Possible solution: x1 XOR x2 can be written as: (x1 AND x2) NOR (x1 NOR x2) Recall that AND, OR and NOR can be implemented by perceptrons.
XOR NOR +0.5 1 1 +1 1 AND NOR 1.5 +1 1 +0.5 Problem: How can we train it to learn a new function? (credit assignment)
