1. 1959 Checkers (Arthur Samuel) 2. 1961 MENACE tic-tac-toe (Donald Michie) 3. 1989 TD-Gammon (Gerald Tesauro) 4. 1997 TD-leaf (Baxter et al.) 5. 2006 MoGo using MCTS (Gelly & Wang) 6. 2009 TreeStrap (Veness et al.) 7. 2016 AlphaGo 8. 2018 Alpha Zero c(cid:13)Alan Blair, 2015-18
Machine Educable Noughts And Crosses Engine Donald Michie, 1961 c(cid:13)Alan Blair, 2015-18
MAX (X) X X X MIN (O) X X X X X X MAX (X) X O X O X O MIN (O) X O X X O X X O X . . . . . . . . . TERMINAL X O X X O O X O X X O O X X O X O X X OO X Utility 1  0 +1 . . . . . . . . . . . . c(cid:13)Alan Blair, 2015-18
This BOXES algorithm was later adapted to learn more general tasks such as Pole Balancing, and helped lay the foundation for the modern eld of Reinforcement Learning. 1. BOXES: An Experiment in Adaptive Control, D.Michie and R.Chambers, Machine Intelligence, Oliver and Boyd, Edinburgh, UK, (1968). c(cid:13)Alan Blair, 2015-18
Articial Intelligence in general, and Machine Learning in particular, came under heavy criticism in the early 1970s. Donald Michies Reinforcement Learning research was deliberately excluded from the 1973 Lighthill report because Lighthill wanted to focus attention on other areas which could most easily be criticised. The eld became largely dormant, until it was revived in the late 1980s, largely through the work of Richard Sutton. Gerald Tesauro applied Suttons TD-Learning algorithm to the game of Backgammon in 1989. c(cid:13)Alan Blair, 2015-18
Suppose we want a write a computer program to play a game like Backgammon, Chess, Checkers or Go. This can be done using a tree search algorithm (expectimax, MCTS, or minimax with alpha-beta pruning). But we need: (a) an appropriate way of encoding any board position as a set of numbers, and (b) a way to train a neural network or other learning system to compute a board evaluation, based on those numbers c(cid:13)Alan Blair, 2015-18
0 1 2 3 4 5 6 7 8 9 10 11 12 25 24 23 22 21 20 19 18 17 16 15 14 13 c(cid:13)Alan Blair, 2015-18
Board encoding Two layer neural network 1. 4 units  2 players  24 points 2. 196 input units 3. 2 units for the bar 4. 20 hidden units 5. 2 units for off the board 6. 1 output unit The input s is the encoded board position (state), the output V(s) is the value of this position (probability of winning). At each move, roll the dice, nd all possible next board positions, convert them to the appropriate input format, feed them to the network, and choose the one which produces the largest output. c(cid:13)Alan Blair, 2015-18
w  w + (T  V ) V w V = actual output T = target value w = weight  = learning rate Q: How do we choose the target value T ? In other words, how do we know what the value of the current position should have been? or, how do we nd a better estimate for the value of the current position? c(cid:13)Alan Blair, 2015-18
1. Behavioral Cloning (Supervised Learning)  learn moves from human games (Expert Preferences) 2. Temporal Difference Learning  use subsequent positions to rene evaluation of current position  general method, does not rely on knowing the world model (rules of the game) 3. methods which combine learning with tree search (must know the world model)  TD-Root (Samuel, 1959)  TD-Leaf (Baxter et al., 1998)  TreeStrap (Veness et al., 2009) c(cid:13)Alan Blair, 2015-18
We have a sequences of positions in the game, each with its own (estimated) value: (current estimate) Vk  Vk+1  . . .  Vm  Vm+1 (nal result) TD(0): Use the value of the next state (Vk+1) as the training value for the current state (Vk). TD(): use Tk as the training value for Vk, where Tk = (1  ) m  t=k+1 t1kVt + mkVm+1 Tk is a weighted average of future estimates,  = discount factor (0   < 1). c(cid:13)Alan Blair, 2015-18
1. Tesauro trained two networks:  EP-network was trained on Expert Preferences (Supervised)  TD-network was trained by self play (TD-Learning) 2. TD-network outperformed the EP-network. 3. With modications such as 3-step lookahead (expectimax) and additional hand-crafted input features, TD-Gammon became the best Backgammon player in the world (Tesauro, 1995). c(cid:13)Alan Blair, 2015-18
1. Random dice rolls in Backgammon force self-play to explore a much larger part of the search space than in a deterministic game. 2. Humans are good at reasoning about a small set of probabilistic outcomes. But, playing Backgammon well requires aggregating a large set of possibilities, each with a small likelihood, and balancing them against each other. Neural Networks might be better at this than humans. 3. For deterministic games like Chess, direct TD-Learning performs poorly. Methods which combine learning with tree search are more effective. c(cid:13)Alan Blair, 2015-18
Move selection is by alpha-beta search, using some function V(s) to evaluate the leaves. c(cid:13)Alan Blair, 2015-18
1. Material weights  For example, Queen = 9, Rook = 5, Knight = Bishop = 3, Pawn = 1 2. Piece-Square weights  some (fractional) score for a particular piece on a particular square 3. Attack/Defend weights  some (fractional) score for one piece attacking or defending another piece. 4. Other features, such as Pawn structure, Mobility, etc. 5. There are no hidden nodes. V(s) is a linear combination of input features, composed with a sigmoid function, to produce a value between 0 and 1 (probability of winning). c(cid:13)Alan Blair, 2015-18
() TD 1. TD-Learning can be applied even if we do not know the world model. But, in this case we do know the world model (rules of the game) 2. Can we make use of the valuable information in the search tree? c(cid:13)Alan Blair, 2015-18
TDRoot 1. TD() (Sutton 1988, Tesauro 1992) 2. TD-Root (Samuel 1959) c(cid:13)Alan Blair, 2015-18
TDLeaf () 1. TD() (Sutton 1988, Tesauro 1992) 2. TD-Root (Samuel 1959) 3. TD-Leaf() (Baxter et al. 1998) c(cid:13)Alan Blair, 2015-18
TreeStrap 1. TD() (Sutton 1988, Tesauro 1992) 2. TD-Root (Samuel 1959) 3. TD-Leaf() (Baxter et al. 1998) 4. TreeStrap (Veness et al. 2009) c(cid:13)Alan Blair, 2015-18
1. all non-leaf positions are updated (including moves not selected) 2. when alpha-beta causes a cutoff, we can still train towards the upper or lower bound c(cid:13)Alan Blair, 2015-18
1. showed for the rst time that a Chess player could be trained to Master level entirely by self-play, from random initial weights 2. learning sometimes became unstable  learning rate had to be carefully chosen  had to put a limit on the size of individual weight updates  we have since found that scaling the learning rate by depth of the node makes learning more stable c(cid:13)Alan Blair, 2015-18
Chess Duchess material: check/checkmate: 6 2 9 8 2  4 = attack/defend: 6262 = 144 9494 = 1296 piece-square: 6462 = 768 11794 = 4212 total: branching factor depth of search 920 24 14 5525 50 5-9 c(cid:13)Alan Blair, 2015-18
Chess Duchess iterative deepening Yes killer move heuristic Yes Yes Yes best reply heuristic history heuristic Yes Yes (Maybe) (Maybe) hash table Yes (Current Work) quiescent search Yes No c(cid:13)Alan Blair, 2015-18
P1 D P2 D K F Q R W N B P K F Q R W N B P K F Q R W N B P3 D P4 D P K F Q R W P N B P1 P2 P3 P4 KQ F R DB WN P KQ F R DB WN P KQ F R DB WN P KQ F R DB WN P c(cid:13)Alan Blair, 2015-18
P3 P2 P3 P4 P3 King P1 P4 P2 P3 Queen P4 P1 P4 P2 Fortress P1 P2 Rook P1 c(cid:13)Alan Blair, 2015-18
P3 P2 P3 P4 P3 King P1 P4 P2 P3 Queen P4 P1 P4 P2 Fortress P1 P2 Rook P1 c(cid:13)Alan Blair, 2015-18
1. Games can be learned from human expert preferences, or from self-play (or a combination) 2. TD-Learning is a general method, which does not rely on knowing the world model 3. TreeStrap is more powerful, because it also renes the value of moves which were not chosen; but it relies on knowing the world model 4. Monte Carlo tree search good for games with large branching factor 5. Deep Learning for Go, Atari Games 6. Starcraft? c(cid:13)Alan Blair, 2015-18
