1. Constraint Satisfaction Problems 2. CSP examples 3. backtracking search and heuristics 4. forward checking and arc consistency 5. local search  hill climbing  simulated annealing c(cid:13)Alan Blair, 2013-9
Constraint Satisfaction Problems are dened by a set of variables Xi, each with a domain Di of possible values, and a set of constraints C. The aim is to nd an assignment of the variables Xi from the domains Di in such a way that none of the constraints C are violated. c(cid:13)Alan Blair, 2013-9
Western Australia Northern Territory South Australia Queensland New South Wales Victoria Variables WA, NT, Q, NSW, V, SA, T Domains Di = {red, green, blue} Constraints: adjacent regions must have different colors e.g. WA6= NT, etc. Tasmania c(cid:13)Alan Blair, 2013-9
Solution is an assignment that satises all the constraints, e.g. Western Australia Northern Territory South Australia Queensland New South Wales Victoria Tasmania {WA=red, NT=green, Q=red, NSW=green, V=red, SA=blue, T=green} c(cid:13)Alan Blair, 2013-9
Put n queens on an n-by-n chess board so that no two queens are attacking each other. c(cid:13)Alan Blair, 2013-9
Assume one queen in each column. Which row does each one go in? Variables: Q1, Q2, Q3, Q4 Domains: Di = {1, 2, 3, 4} Constraints: Qi 6= Q j (cannot be in same row) |Qi  Q j| 6= |i  j| (or same diagonal) c(cid:13)Alan Blair, 2013-9
S E N D + M O R E M O N E Y Variables: D E M N O R S Y Domains: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} Constraints: M 6= 0, S 6= 0 (unary constraints) Y = D + E or Y = D + E  10, etc. D 6= E, D 6= M, D 6= N, etc. c(cid:13)Alan Blair, 2013-9
We can add hidden variables to simplify the constraints. OWT + OWT F O U R F T U W R O Variables: F T U W R O X1X2X3 Domains: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} X3 X2 X1 Constraints: AllDifferent(F,T,U,W,R,O) O + O = R + 10X1, etc. c(cid:13)Alan Blair, 2013-9
1. Assignment problems (e.g. who teaches what class) 2. Timetabling problems (e.g. which class is offered when and where?) 3. Hardware conguration 4. Transport scheduling 5. Factory scheduling c(cid:13)Alan Blair, 2013-9
1. Unary constraints involve a single variable  M 6= 0 2. Binary constraints involve pairs of variables  SA 6= WA 3. Higher-order constraints involve 3 or more variables  Y = D + E or Y = D + E  10 4. Inequality constraints on Continuous variables  EndJob1 + 5  StartJob3 5. Soft constraints (Preferences)  11am lecture is better than 8am lecture! c(cid:13)Alan Blair, 2013-9
CSPs can be solved by assigning values to variables one by one, in different combinations. Whenever a constraint is violated, we go back to the most recently assigned variable and assign it a new value. This can be achieved by a Depth First Search on a special kind of state space, where states are dened by the values assigned so far: 1. Initial state: the empty assignment. 2. Successor function: assign a value to an unassigned variable that does not conict with previously assigned values of other variables. (If no legal values remain, the successor function fails.) 3. Goal test: all variables have been assigned a value, and no constraints have been violated. c(cid:13)Alan Blair, 2013-9
c(cid:13)Alan Blair, 2013-9
c(cid:13)Alan Blair, 2013-9
Important difference between Path Search Problems and CSPs: 1. Constraint Satisfaction Problems (e.g. n-Queens)  difcult part is knowing the nal state  how to get there is easy 2. Path Search Problems (e.g. Rubiks Cube)  knowing the nal state is easy  difcult part is how to get there c(cid:13)Alan Blair, 2013-9
The search space for this Depth First Search has certain very specic properties: 1. if there are n variables, every solution will occur at exactly depth n. 2. variable assignments are commutative [WA = red then NT = green] same as [NT = green then WA = red] Backtracking search can solve n-Queens for n  25 c(cid:13)Alan Blair, 2013-9
General-purpose heuristics can give huge gains in speed: 1. which variable should be assigned next? 2. in what order should its values be tried? 3. can we detect inevitable failure early? c(cid:13)Alan Blair, 2013-9
Minimum Remaining Values (MRV): Choose the variable with the fewest legal values. c(cid:13)Alan Blair, 2013-9
Tie-breaker among MRV variables Degree heuristic: Choose the variable with the most constraints on remaining variables. c(cid:13)Alan Blair, 2013-9
Given a variable, choose the least constraining value: the one that rules out the fewest values in the remaining variables (More generally, 3 allowed values would be better than 2, etc.) Combining these heuristics makes 1000 queens feasible. c(cid:13)Alan Blair, 2013-9
Idea: Keep track of remaining legal values for unassigned variables WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
Idea: Keep track of remaining legal values for unassigned variables Terminate search when any variable has no legal values WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
Idea: Keep track of remaining legal values for unassigned variables Terminate search when any variable has no legal values WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
Idea: Keep track of remaining legal values for unassigned variables Terminate search when any variable has no legal values WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
Forward checking propagates information from assigned to unassigned variables, but doesnt provide early detection for all failures: WA NT Q NSW V SA T NT and SA cannot both be blue! Constraint propagation repeatedly enforces constraints locally. c(cid:13)Alan Blair, 2013-9
Simplest form of constraint propagation makes each arc consistent X  Y is consistent if for every value x of X there is some allowed y WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
Simplest form of propagation makes each arc consistent X  Y is consistent if for every value x of X there is some allowed y WA NT Q NSW V SA T c(cid:13)Alan Blair, 2013-9
X  Y is consistent if for every value x of X there is some allowed y WA NT Q NSW V SA T If X loses a value, neighbors of X need to be rechecked. c(cid:13)Alan Blair, 2013-9
X  Y is consistent if for every value x of X there is some allowed y WA NT Q NSW V SA T Arc consistency detects failure earlier than forward checking. For some problems, it can speed up the search enormously. For others, it may slow the search due to computational overheads. c(cid:13)Alan Blair, 2013-9
There is another class of algorithms for solving CSPs, called Iterative Improvement or Local Search. These algorithms assign all variables randomly in the beginning (thus violating several constraints), and then change one variable at a time, trying to reduce the number of violations at each step. h = 5 h = 2 h = 0 c(cid:13)Alan Blair, 2013-9
2 2 1 2 3 1 2 3 3 2 3 2 3 0 1. Variable selection: randomly select any conicted variable 2. Value selection by min-conicts heuristic  choose value that violates the fewest constraints c(cid:13)Alan Blair, 2013-9
Sometimes, have to go sideways or even backwards in order to make progress towards the actual solution. c(cid:13)Alan Blair, 2013-9
When we are minimizing violated constraints, it makes sense to think of starting at the top of a ridge and climbing down into the valleys. c(cid:13)Alan Blair, 2013-9
1. stochastic hill climbing based on difference between evaluation of previous state (h0) and new state (h1). 2. if h1 < h0, denitely make the change 3. otherwise, make the change with probability e(h1h0)/T where T is a temperature parameter. 4. reduces to ordinary hill climbing when T = 0 5. becomes totally random search as T   6. sometimes, we gradually decrease the value of T during the search c(cid:13)Alan Blair, 2013-9
Given random initial state, hill climbing by min-conicts with random restarts can solve n-queens in almost constant time for arbitrary n with high probability (e.g., n = 10,000,000). In general, randomly-generated CSPs tend to be easy if there are very few or very many constraints. They become extra hard in a narrow range of the ratio R = number of constraints number of variables CPU time critical    ratio R c(cid:13)Alan Blair, 2013-9
1. Much interest in CSPs for real-world applications 2. Backtracking = depth-rst search with one variable assigned per node 3. Variable and Value ordering heuristics help signicantly 4. Forward Checking helps by detecting inevitable failure early 5. Hill Climbing by min-conicts often effective in practice 6. Simulated Annealing can help to escape from local optima 7. Which method(s) are best? It varies from one task to another! c(cid:13)Alan Blair, 2013-9
