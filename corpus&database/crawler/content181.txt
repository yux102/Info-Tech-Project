Agent World Model Planning Bayesian Learning Inference Learning Perception Action Statistical Learning Reinforcement Learning Environment c(cid:13)AIMA, 2003, Blair, 2013-19
1. Supervised Learning  agent is presented with examples of inputs and their target outputs 2. Reinforcement Learning  agent is not presented with target outputs, but is given a reward signal, which it aims to maximize 3. Unsupervised Learning  agent is only presented with the inputs themselves, and aims to nd structure in these inputs c(cid:13)AIMA, 2003, Blair, 2013-19
1. we have a training set and a test set, each consisting of a set of items; for each item, a number of input attributes and a target value are specied. 2. the aim is to predict the target value, based on the input attributes. 3. agent is presented with the input and target output for each item in the training set; it must then predict the output for each item in the test set 4. various learning paradigms are available:  Decision Tree  Neural Network  Support Vector Machine, etc. c(cid:13)AIMA, 2003, Blair, 2013-19
1. framework (decision tree, neural network, SVM, etc.) 2. representation (of inputs and outputs) 3. pre-processing / post-processing 4. training method (perceptron learning, backpropagation, etc.) 5. generalization (avoid over-tting) 6. evaluation (separate training and testing sets) c(cid:13)AIMA, 2003, Blair, 2013-19
Which curve gives the best t to these data? f(x) x c(cid:13)AIMA, 2003, Blair, 2013-19
Which curve gives the best t to these data? f(x) straight line? x c(cid:13)AIMA, 2003, Blair, 2013-19
Which curve gives the best t to these data? f(x) parabola? x c(cid:13)AIMA, 2003, Blair, 2013-19
Which curve gives the best t to these data? f(x) 4th order polynomial? c(cid:13)AIMA, 2003, Blair, 2013-19 x
Which curve gives the best t to these data? f(x) Something else? c(cid:13)AIMA, 2003, Blair, 2013-19 x
The most likely hypothesis is the simplest one consistent with the data. o o o o o o x o x x x x o x o o o x x x x x o x x o x o o o o o o o o x o x x x x o x o o o x x x x x o x x o x o o o o o o o o x o x x x x o x o o o x x x x x o x x o x o o inadequate good compromise over-tting Since there can be noise in the measurements, in practice need to make a tradeoff between simplicity of the hypothesis and how well it ts the data. c(cid:13)AIMA, 2003, Blair, 2013-19
Predicted Buchanan Votes by County [faculty.washington.edu/mtbrett] c(cid:13)AIMA, 2003, Blair, 2013-19
Alt Bar F/S Hun Pat Price Rain Res Type Est Wait? T T F T T F F F F T F T X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X11 X12 F F T F F T T F T T F T F F F T T F F F T T F T T T F T F T F T F T F T Some $$$ Full Some Full Full Some None Some Full Full None Full $ $ $ $$$ $$ $ $$ $ $$$ $ $ F F F F F T T T T F F F T F F F T T F T F T F F French 010 Thai 3060 Burger 010 Thai 1030 French Italian Burger Thai Burger >60 010 010 010 >60 Italian 1030 Thai 010 Burger 3060 c(cid:13)AIMA, 2003, Blair, 2013-19 T F T T F T F T F F F T
Patrons? None Some Full F T WaitEstimate? >60 3060 1030 F Alternate? No  Yes Hungry? No  Yes 010 T Reservation? Fri/Sat? T No  Yes No  Yes Alternate? No  Yes Bar? T F T T No F  Yes T Raining? No  Yes F T c(cid:13)AIMA, 2003, Blair, 2013-19
1. Provided the training data are not inconsistent, we can split the attributes in any order and still produce a tree that correctly classies all examples in the training set. 2. However, we really want a tree which is likely to generalize to correctly classify the (unseen) examples in the test set. 3. In view of Ochkams Razor, we prefer a simpler hypothesis, i.e. a smaller tree. 4. But how can we choose attributes in order to produce a small tree? c(cid:13)AIMA, 2003, Blair, 2013-19
Patrons? Type? None Some Full French Italian Thai Burger Patrons is a more informative attribute than Type, because it splits the examples more nearly into sets that are all positive or all negative. This notion of informativeness can be quantied using the mathematical concept of entropy. A parsimonious tree can be built by minimizing the entropy at each step. c(cid:13)AIMA, 2003, Blair, 2013-19
Entropy is a measure of how much information we gain when the target attribute is revealed to us. In other words, it is not a measure of how much we know, but of how much we dont know. If the prior probabilities of the n target attribute values are p1, . . . , pn then the entropy is H(hp1, . . . , pni) = n  i=1 pi log2 pi c(cid:13)AIMA, 2003, Blair, 2013-19
Entropy is the number of bits per symbol achieved by a (block) Huffman Coding scheme. Example 1: H(h0.5, 0.5i) = 1 bit. Suppose we want to encode, in zeroes and ones, a long message composed of the two letters A and B, which occur with equal frequency. This can be done efciently by assigning A=0, B=1. In other words, one bit is needed to encode each letter. c(cid:13)AIMA, 2003, Blair, 2013-19
Example 2: H(h0.5, 0.25, 0.25i) = 1.5 bits. Suppose we need to encode a message consisting of the letters A, B and C, and that B and C occur equally often but A occurs twice as often as the other two letters. In this case, the most efcient code would be A=0, B=10, C=11. The average number of bits needed to encode each letter is 1.5 . If the letters occur in some other proportion, we would need to block them together in order to encode them efciently. But, the average number of bits required by the most efcient coding scheme is given by H(hp1, . . . , pni) = n  i=1 pi log2 pi c(cid:13)AIMA, 2003, Blair, 2013-19
Suppose we have p positive and n negative examples at a node.  H(hp/(p + n), n/(p + n)i) bits needed to classify a new example. e.g. for 12 restaurant examples, p = n = 6 so we need 1 bit. An attribute splits the examples E into subsets Ei, each of which (we hope) needs less information to complete the classication. Let Ei have pi positive and ni negative examples  H(hpi/(pi + ni), ni/(pi + ni)i) bits needed to classify a new example  expected number of bits per example over all branches is pi + ni p + n  i H(h pi pi + ni , ni pi + ni i) For Patrons, this is 0.459 bits, for Type this is (still) 1 bit. c(cid:13)AIMA, 2003, Blair, 2013-19
Patrons? Type? None Some Full French Italian Thai Burger For Patrons, Entropy = 1 6 (0) + = 0 + 0 + For Type, Entropy = 1 6 (1) + 1 3 1 2 1 6 (0) + 1 2 (cid:2) 1 3 log( 1 3 )  2 3 log( 2 3 )(cid:3) (cid:2) 1 3 (1.585) + (1) + 1 3 (1) + 2 3 1 3 (0.585)(cid:3) = 0.459 (1) = 1 c(cid:13)AIMA, 2003, Blair, 2013-19
Patrons? None Some Full F T Hungry?  Yes Type? No F French Italian Thai Burger T F Fri/Sat? T No F  Yes T c(cid:13)AIMA, 2003, Blair, 2013-19
According to Ockhams Razor, we may wish to prune off branches that do not provide much benet in classifying the items. When a node becomes a leaf, all items will be assigned to the majority class at that node. We can estimate the error rate on the (unseen) test items using the Laplace error: E = 1  n + 1 N + k N = total number of (training) items at the node n = number of (training) items in the majority class k = number of classes If the average Laplace error of the children exceeds that of the parent node, we prune off the children. c(cid:13)AIMA, 2003, Blair, 2013-19
Should the children of this node be pruned or not? Left child has class frequencies [7,3] E = 1  n + 1 N + k = 1  7 + 1 10 + 2 = 0.333 Right child has E = 0.429 Parent node has E = 0.412 Average for Left and Right child is E = 10 15 (0.333) + 5 15 (0.429) = 0.365 [9,6] [7,3] [2,3] Since 0.365 < 0.412, children should NOT be pruned. c(cid:13)AIMA, 2003, Blair, 2013-19
Should the children of this node be pruned or not? Left child has class frequencies [3,2] E = 1  n + 1 N + k = 1  3 + 1 5 + 2 = 0.429 Right child has E = 0.333 Parent node has E = 0.375 Average for Left and Right child is E = 5 6 (0.429) + 1 6 (0.333) = 0.413 [4,2] [3,2] [1,0] Since 0.413 > 0.375, children should be pruned. c(cid:13)AIMA, 2003, Blair, 2013-19
Should the children of this node be pruned or not? Left and Middle child have class frequencies [15,1] E = 1  n + 1 N + k = 1  15 + 1 16 + 2 = 0.111 Right child has E = 0.333 Parent node has E = 4 Average for Left, Middle and Right child is 35 = 0.114 [30,3] [15,1] [15,1] [0,1] E = 16 33 (0.111)+ 16 33 (0.111)+ 1 33 (0.333) = 0.118 Since 0.118 > 0.114, children should be pruned. c(cid:13)AIMA, 2003, Blair, 2013-19
1. Supervised Learning  training set and test set  try to predict target value, based on input attributes 2. Ockhams Razor  tradeoff between simplicity and accuracy 3. Decision Trees  improve generalisation by building a smaller tree (using entropy)  prune nodes based on Laplace error c(cid:13)AIMA, 2003, Blair, 2013-19
